{
  "lastUpdated": "2025-09-30T06:15:36.701Z",
  "category": "generative-ai",
  "totalItems": 52,
  "items": [
    {
      "id": "aws-news-c959ef8c3720",
      "title": "Building health care agents using Amazon Bedrock AgentCore",
      "description": "In this solution, we demonstrate how the user (a parent) can interact with a Strands or LangGraph agent in conversational style and get information about the immunization history and schedule of their child, inquire about the available slots, and book appointments. With some changes, AI agents can be made event-driven so that they can automatically send reminders, book appointments, and so on.",
      "link": "https://aws.amazon.com/blogs/machine-learning/building-health-care-agents-using-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-26T16:03:41.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore"
      ]
    },
    {
      "id": "aws-news-72cd9c93cdeb",
      "title": "Build multi-agent site reliability engineering assistants with Amazon Bedrock AgentCore",
      "description": "In this post, we demonstrate how to build a multi-agent SRE assistant using Amazon Bedrock AgentCore, LangGraph, and the Model Context Protocol (MCP). This system deploys specialized AI agents that collaborate to provide the deep, contextual intelligence that modern SRE teams need for effective incident response and infrastructure management.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-multi-agent-site-reliability-engineering-assistants-with-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-26T15:58:34.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore"
      ]
    },
    {
      "id": "aws-news-4419894b788a",
      "title": "Local file redirection is now available on Amazon AppStream 2.0 multi-session fleets",
      "description": "Amazon AppStream 2.0 is enhancing the end-user experience by introducing support for local files redirection on multi-session fleets. While this feature is already available on single-session fleets, this launch extends it to multi-session fleets, helping administrators to leverage the cost benefits of the multi-session model while providing an enhanced end-user experience.\n  Local file redirection on AppStream helps deliver benefits by enabling seamless access to local files directly from streaming applications, enhancing user productivity and experience. This feature reduces the need for manual file uploads and downloads, providing a natural, desktop-like experience with intuitive drag-and-drop functionality. Users can more efficiently manage their workflows while helping to maintain security through controlled access to local resources and secure file handling between environments.\n  This feature is available at no additional cost in all the AWS Regions where Amazon AppStream 2.0 is available. AppStream 2.0 offers pay-as-you go pricing. To get started with AppStream 2.0, see Getting Started with Amazon AppStream 2.0.\n  To enable this feature for your users, you must use an AppStream 2.0 image that uses latest AppStream 2.0 agent or has been updated using Managed AppStream 2.0 image updates released on or after September 05, 2025.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/local-file-redirection-amazon-appstream-multi-session-fleets/",
      "pubDate": "2025-09-26T15:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "now-available",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-13a535dc87a5",
      "title": "Amazon EBS increases the maximum size and provisioned performance of General Purpose (gp3) volumes",
      "description": "Amazon Elastic Block Store (Amazon EBS) now supports higher volume-level limits for its General Purpose (gp3) volumes. With this update, gp3 volumes can scale up to 64 TiB in size (4X the previous 16 TiB limit), up to 80,000 IOPS (5X the previous 16,000 IOPS limit), and up to 2,000 MiB/s throughput (2X the previous 1,000 MiB/s limit).\n  These expanded limits help reduce operational complexity for storage-intensive workloads by enabling gp3 volumes with larger capacity and higher performance. You can consolidate multiple striped volumes into a single gp3 volume, streamline architectures, and lower management overhead. The increased limits particularly benefit customers running containerized workloads with limited support for striping multiple volumes, applications that rely on single-volume architectures, and growing workloads approaching current gp3 limits. The pricing model remains unchanged: you pay for storage plus any additional IOPS and throughput provisioned beyond the baseline performance.\n  The new gp3 limits are available in all AWS Commercial Regions and AWS GovCloud (US) Regions where gp3 volumes are available. To get started and learn more, please visit the Amazon EBS user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ebs-size-provisioned-performance-gp3-volumes/",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-82a70dbffaef",
      "title": "AWS Compute Optimizer now supports 99 new Amazon EC2 instance types",
      "description": "AWS Compute Optimizer now supports 99 additional Amazon Elastic Compute Cloud (Amazon EC2) instance types. These enhancements help you identify additional savings opportunities across your EC2 instances without specialized knowledge or manual analysis.\n  Compute Optimizer has expanded support to include the latest generation Compute Optimized (C8gn, C8gd), General Purpose (M8i, M8i-flex, M8gd), Memory Optimized (R8i, R8i-flex, R8gd), and Storage Optimized (I8ge) instance types. This expansion enables Compute Optimizer to help you take advantage of the price-to-performance improvements offered by the newest instance types.\n  This new feature is available in all AWS Regions where Compute Optimizer is available except the AWS GovCloud (US) and the China Regions. For more information about Compute Optimizer, visit our product page and documentation. You can start using Compute Optimizer through the AWS Management Console, AWS CLI, or AWS SDK.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-compute-optimizer-99-new-amazon-ec2-instance-types/",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "ec2",
        "new-feature",
        "improvement",
        "enhancement",
        "support",
        "expansion"
      ]
    },
    {
      "id": "aws-news-c38dfafac29a",
      "title": "DoWhile loops now supported in Amazon Bedrock Flows",
      "description": "Today, we are excited to announce support for DoWhile loops in Amazon Bedrock Flows. With this powerful new capability, you can create iterative, condition-based workflows directly within your Amazon Bedrock flows, using Prompt nodes, AWS Lambda functions, Amazon Bedrock Agents, Amazon Bedrock Flows inline code, Amazon Bedrock Knowledge Bases, Amazon Simple Storage Service (Amazon S3), […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/dowhile-loops-now-supported-in-amazon-bedrock-flows/",
      "pubDate": "2025-09-25T20:25:08.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "lambda",
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "lambda",
        "s3",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-1510ce03e38b",
      "title": "How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock",
      "description": "In this post, we explore how we built a multi-agent conversational AI system using Amazon Bedrock that delivers knowledge-grounded property investment advice. We explore the agent architecture, model selection strategy, and comprehensive continuous evaluation system that facilitates quality conversations while facilitating rapid iteration and improvement.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-prophero-built-an-intelligent-property-investment-advisor-with-continuous-evaluation-using-amazon-bedrock/",
      "pubDate": "2025-09-25T19:25:23.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "improvement"
      ]
    },
    {
      "id": "aws-news-daeb8d6f30a5",
      "title": "Accelerate benefits claims processing with Amazon Bedrock Data Automation",
      "description": "In the benefits administration industry, claims processing is a vital operational pillar that makes sure employees and beneficiaries receive timely benefits, such as health, dental, or disability payments, while controlling costs and adhering to regulations like HIPAA and ERISA. In this post, we examine the typical benefit claims processing workflow and identify where generative AI-powered automation can deliver the greatest impact.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-benefits-claims-processing-with-amazon-bedrock-data-automation/",
      "pubDate": "2025-09-25T19:20:16.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-7a5d69e1edad",
      "title": "Amazon EC2 I7i instances now available in AWS Europe (Milan) and AWS US West (N. California)",
      "description": "AWS is announcing the availability of high performance Storage optimized Amazon EC2 I7i instances in AWS Europe (Milan) and US West (N. California) regions. Powered by 5th Gen Intel Xeon Processors with an all-core turbo frequency of 3.2 GHz, these new instances deliver up to 23% better compute performance and more than 10% better price performance over previous generation I4i instances. Powered by 3rd generation AWS Nitro SSDs, I7i instances offer up to 45TB of NVMe storage with up to 50% better real-time storage performance, up to 50% lower storage I/O latency, and up to 60% lower storage I/O latency variability compared to I4i instances.\n  I7i instances offer compute and storage performance for x86-based storage optimized instances in Amazon EC2 ideal for I/O intensive and latency-sensitive workloads that demand very high random IOPS performance with real-time latency to access the small to medium size datasets. Additionally, torn write prevention feature support up to 16KB block sizes, enabling customers to eliminate database performance bottlenecks.\n  I7i instances are available in eleven sizes - nine virtual sizes up to 48xlarge and two bare metal sizes - delivering up to 100Gbps of network bandwidth and 60Gbps of Amazon Elastic Block Store (EBS) bandwidth. To learn more, visit the I7i instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-i7i-instances-available-in-milan-california/",
      "pubDate": "2025-09-25T17:01:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-b049add8cd1b",
      "title": "Amazon EC2 Allowed AMIs setting adds new parameters for enhanced AMI governance",
      "description": "Allowed AMIs, the Amazon EC2 account-wide setting that enables you to limit the discovery and use of Amazon Machine Images (AMIs) within your Amazon Web Services accounts, adds support for four new parameters — marketplace codes, deprecation time, creation date and AMI names.\n  Previously, you could specify accounts or owner aliases that you trust in your Allowed AMIs setting. Starting today, you can use the four new parameters to define additional criteria to further reduce risk of inadvertently launching instances with non-compliant or unauthorized AMIs. Marketplace codes can be provided to limit the use of Marketplace AMIs, the deprecation time and creation date parameters can be used to limit the use of outdated AMIs, and AMI name parameter can be used to restrict usage to AMIs with specific naming pattern. You can also leverage Declarative Policies to configure these parameters to perform AMI governance across your organization.\n  These additional parameters are now supported in all AWS regions including AWS China (Beijing) Region, operated by Sinnet, and AWS China (Ningxia) Region, operated by NWCD, and AWS GovCloud (US). To learn more, please visit the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-allowed-amis-setting-parameters-ami-governance/",
      "pubDate": "2025-09-25T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "launch",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-6a85e20f60f0",
      "title": "PostgreSQL 18.0 is now available in Amazon RDS Database Preview Environment",
      "description": "Amazon RDS for PostgreSQL 18.0 is now available in the Amazon RDS Database Preview Environment, allowing you to evaluate the latest PostgreSQL features while leveraging the benefits of a fully managed database service. This preview environment provides you a sandbox where you can test applications and explore new PostgreSQL 18.0 capabilities before they become generally available.\n \nPostgreSQL 18.0 includes \"skip scan\" support for multicolumn B-tree indexes and improves WHERE clause handling for OR and IN conditions. It introduces parallel Generalized Inverted Index (GIN) builds and updates join operations. It now supports Universally Unique Identifiers Version 7 (UUIDv7), which combines timestamp-based ordering with traditional UUID uniqueness to boost performance in high-throughput distributed systems. Observability improvements show buffer usage counts and index lookups during query execution, along with per-connection I/O utilization metric. Please refer to the RDS PostgreSQL release documentation for more details.\n \nAmazon RDS Database Preview Environment database instances are retained for a maximum period of 60 days and are automatically deleted after the retention period. Amazon RDS database snapshots that are created in the preview environment can only be used to create or restore database instances within the preview environment. You can use the PostgreSQL dump and load functionality to import or export your databases from the preview environment.\n \nAmazon RDS Database Preview Environment database instances are priced as per the pricing in the US East (Ohio) Region.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/postgresql-180-amazon-rds-database-preview-environment/",
      "pubDate": "2025-09-25T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "rds",
        "preview",
        "generally-available",
        "now-available",
        "update",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-36218ab4636b",
      "title": "AWS announces unlimited network burst duration on EC2 I8g and I7i instances",
      "description": "Today, AWS eliminated the networking bandwidth burst duration limitations for Amazon EC2 I7i and I8g instances on sizes larger than 4xlarge. This update doubles the Network Bandwidth available at all times for i7i and i8g instances on sizes larger than 4xlarge. Previously, these instance sizes had a baseline bandwidth and used a network I/O credit mechanism to burst beyond their baseline bandwidth on a best effort basis. Today these instance sizes can sustain their maximum performance indefinitely. With this improvement, customers running memory and network intensive workloads on larger instance sizes can now consistently maintain their maximum network bandwidth without interruption, delivering more predictable performance for applications that require sustained high-throughput network connectivity. This change applies only to instance sizes larger than 4xlarge, while smaller instances will continue to operate with their existing baseline and burst bandwidth configurations.\n \nAmazon EC2 I7i and I8g instances are designed for I/O intensive workloads that require rapid data access and real-time latency from storage. These instances excel at handling transactional, real-time, distributed databases, including MySQL, PostgreSQL, Hbase and NoSQL solutions like Aerospike, MongoDB, ClickHouse, and Apache Druid. They're also optimized for real-time analytics platforms such as Apache Spark, data lakehouse, and AI LLM pre-processing for training. These instances have up to 1.5 TiB of memory, and 45 TB local instance storage. They deliver up to 100 Gbps of network performance bandwidth, and 60 Gbps of dedicated bandwidth for Amazon Elastic Block Store (EBS).\n \nTo learn more, see Amazon EC2 I7i and I8g instances. To get started, see AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-announces-unlimited-network-burst-duration-i8g-i7i",
      "pubDate": "2025-09-24T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "update",
        "improvement"
      ]
    },
    {
      "id": "aws-news-f76fdca33bd0",
      "title": "Running deep research AI agents on Amazon Bedrock AgentCore",
      "description": "AI agents are evolving beyond basic single-task helpers into more powerful systems that can plan, critique, and collaborate with other agents to solve complex problems. Deep Agents—a recently introduced framework built on LangGraph—bring these capabilities to life, enabling multi-agent workflows that mirror real-world team dynamics. The challenge, however, is not just building such agents but […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/running-deep-research-ai-agents-on-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-23T20:35:23.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex"
      ]
    },
    {
      "id": "aws-news-86dbb31f2325",
      "title": "Amazon DataZone is now available in 3 additional commercial regions",
      "description": "Amazon DataZone is now available in AWS Asia Pacific (Hong Kong), Asia Pacific (Malaysia) and Europe (Zurich) Regions.\n  Amazon DataZone is a fully managed data management service to catalog, discover, analyze, share, and govern data between data producers and consumers in your organization. With Amazon DataZone, data producers populate the business data catalog with structured data assets from AWS Glue Data Catalog and Amazon Redshift tables. Data consumers search and subscribe to data assets in the data catalog and share with other collaborators working on the same business use case. Consumers can analyze their subscribed data assets with tools—such as Amazon Redshift or Amazon Athena query editors—that are directly accessed from the Amazon DataZone portal. The integrated publishing and subscription workflow provides access to auditing capabilities across projects.\n  For more information on AWS Regions where Amazon DataZone is available in preview, see supported regions.\n \nAdditionally, Amazon DataZone powers governance in the next generation of Amazon SageMaker, which simplifies the discovery, governance, and collaboration of data and AI across your lakehouse, AI models, and GenAI applications. With Amazon SageMaker Catalog (built on Amazon DataZone), users can securely discover and access approved data and models using semantic search with generative AI–created metadata, or they could just ask Amazon Q Developer using natural language to find their data. For more information on AWS Regions where the next generation of SageMaker is available, see supported regions. To learn more about the next generation of SageMaker, visit the product webpage.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-datazone-additional-regions/",
      "pubDate": "2025-09-23T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q",
        "q developer",
        "sagemaker",
        "redshift",
        "glue",
        "athena"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer",
        "sagemaker",
        "redshift",
        "glue",
        "athena",
        "preview",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-56f37bafb064",
      "title": "Amazon EC2 R8gb instances are now generally available",
      "description": "Today, AWS announces the general availability of the new Amazon Elastic Block Storage (Amazon EBS) optimized Amazon Elastic Compute Cloud (Amazon EC2) R8gb instances. These instances are powered by AWS Graviton4 processors to deliver up to 30% better compute performance than AWS Graviton3 processors. At up to 150 Gbps of EBS bandwidth, these instances offer higher EBS performance compared to same-sized equivalent Graviton4-based instances. Take advantage of the higher block storage performance offered by these new EBS optimized EC2 instances to scale the performance and throughput of workloads such as high performance databases and NoSQL databases, while optimizing the cost of running your workloads.\n \nFor increased scalability, these instances offer instance sizes up to 24xlarge, including one metal size, up to 768 GiB of memory, up to 150 Gbps of EBS bandwidth, up to 200 Gbps of networking bandwidth. These instances support Elastic Fabric Adapter (EFA) networking on the 16xlarge, 24xlarge, and metal-24xl sizes, which enables lower latency and improved cluster performance for workloads deployed on tightly coupled clusters.\n \nThe new R8gb instances are available in US East (N. Virginia) and US West (Oregon) regions. Metal sizes are only available in US East (N. Virginia) region.\n \nTo learn more, see Amazon R8gb Instances. To begin your Graviton journey, visit the Level up your compute with AWS Graviton page. To get started, see AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-r8gb-instances/",
      "pubDate": "2025-09-23T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2",
        "graviton"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "graviton",
        "generally-available",
        "support"
      ]
    },
    {
      "id": "aws-news-77e33a4e16c4",
      "title": "Amazon Connect flow designer now supports analytics mode",
      "description": "Amazon Connect now offers new enhanced analytics in the drag-and-drop flow designer that help you make data-driven decisions when building and optimizing your flows. Amazon Connect flows allow you to create end-to-end self-service and automated customer experiences such as interactive voice response (IVR), step-by-step guides, and back office processes and tasks. With this launch, you can now view aggregate metrics on how customers move through each step in the flow including where they run into errors or abandon the experience. For example, you can see how many conversational AI interactions result in transfers to agent queues or when customers end up in the wrong queue because an error in the flow configuration. These new capabilities help you identify behavioral patterns and evaluate root causes, allowing you to deliver better outcomes for customers.\n  This new capability is included with Amazon Connect (with unlimited AI) pricing. To learn more about this feature, see the Amazon Connect Administrator Guide. This feature is available in all AWS regions that offers Amazon Connect. To learn more about Amazon Connect, the AWS cloud-based contact center, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-connect-flow-designer-analytics-mode/",
      "pubDate": "2025-09-22T16:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-d5a772368a74",
      "title": "Move your AI agents from proof of concept to production with Amazon Bedrock AgentCore",
      "description": "This post explores how Amazon Bedrock AgentCore helps you transition your agentic applications from experimental proof of concept to production-ready systems. We follow the journey of a customer support agent that evolves from a simple local prototype to a comprehensive, enterprise-grade solution capable of handling multiple concurrent users while maintaining security and performance standards.",
      "link": "https://aws.amazon.com/blogs/machine-learning/move-your-ai-agents-from-proof-of-concept-to-production-with-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-19T16:09:26.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "rds",
        "experimental",
        "support"
      ]
    },
    {
      "id": "aws-news-d073fb3f6dab",
      "title": "Announcing AWS Neuron SDK 2.26.0",
      "description": "Today, AWS announces the general availability of Neuron SDK 2.26.0, delivering improvements for deep learning workloads on AWS Inferentia and Trainium-based instances. This release introduces support for PyTorch 2.8 and JAX 0.6.2, along with enhanced inference capabilities on Trainium2 (Trn2) instances. These updates enable developers to leverage the latest frameworks while benefiting from improved model deployment flexibility and performance optimizations.\n  With Neuron SDK 2.26.0, customers can now deploy FLUX.1-dev image generation model, along with Llama 4 Scout and Maverick variants (beta) on Trn2 instances. The release introduces expert parallelism support (beta) for efficient distribution of Mixture-of-Experts (MoE) models across multiple NeuronCores, and adds new capabilities through new Neuron Kernel Interface (NKI) APIs. The updated Neuron Profiler provides improved capabilities, including system profile grouping for distributed workloads.\n  The new SDK version is available in all AWS Regions supporting Inferentia and Trainium instances, offering enhanced performance and monitoring capabilities for machine learning workloads.\n  To learn more and for a full list of new features and enhancements, see:\n  \n \n \nAWS Neuron 2.26.0 release notes\n \n \nTrn2 Instances\n \n \nTrn1 Instances\n \n \nInf2 Instances",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-neuron-2-26-announce/",
      "pubDate": "2025-09-19T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "trainium",
        "inferentia",
        "neuron"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "trainium",
        "inferentia",
        "neuron",
        "beta",
        "new-feature",
        "update",
        "improvement",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-897537d328af",
      "title": "Amazon Q Developer CLI announces support for remote MCP servers",
      "description": "Amazon Q Developer CLI announces support for remote MCP servers. Remote MCP servers improve scalability and security of the tools you use within your development tasks. Not only does it reduce the use of compute resources by moving to a centralized server, it also helps you better manage access and security. You can now integrate with MCP servers such as Atlassian, and GitHub that support HTTP and support OAuth based authentication.\n  To configure a remote MCP server, specify the transport type as HTTP, the URL where users will get authentication credentials, and any optional headers to include when making the request. You can configure remote MCP servers in your custom agent configuration or in mcp.json. When a CLI session is initiated, you will see the list of MCP servers to load and can query the list for the authentication URL. Once you successfully complete the authentication steps, Q Developer CLI will query the tools available from the MCP server and make it available to the agent.\n  Remote MCP servers are available in Amazon Q Developer CLI and Amazon Q Developer IDE plugins. For more information, check out the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-q-developer-remote-mcp-servers/",
      "pubDate": "2025-09-18T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer",
        "support"
      ]
    },
    {
      "id": "aws-news-804168525168",
      "title": "Monitor Amazon Bedrock batch inference using Amazon CloudWatch metrics",
      "description": "In this post, we explore how to monitor and manage Amazon Bedrock batch inference jobs using Amazon CloudWatch metrics, alarms, and dashboards to optimize performance, cost, and operational efficiency.",
      "link": "https://aws.amazon.com/blogs/machine-learning/monitor-amazon-bedrock-batch-inference-using-amazon-cloudwatch-metrics/",
      "pubDate": "2025-09-18T15:33:07.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "rds",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "rds",
        "cloudwatch"
      ]
    },
    {
      "id": "aws-news-966a8a4d6e6a",
      "title": "OpenAI open weight models expand to new regions on Amazon Bedrock",
      "description": "Today, AWS announces the expansion of OpenAI open weight models on Amazon Bedrock to eight new regions. This expansion brings these powerful AI models closer to customers in various parts of the world, enabling lower latency and improved performance for a wide range of AI-powered applications.\n  With this expansion, the OpenAI open weight models are now available in the following AWS Regions: US East (N. Virginia), Asia Pacific (Tokyo), Europe (Stockholm), Asia Pacific (Mumbai), Europe (Ireland), South America (São Paulo), Europe (London), and Europe (Milan), in addition to the previously supported region of US West (Oregon). This broader availability allows more customers to leverage these state-of-the-art AI models while keeping their data within their preferred geographic locations, helping to address data residency requirements and reduce network latency.\n  To learn more about OpenAI open weight models on Amazon Bedrock and how to get started, visit the Amazon Bedrock console or check out our documentation. For more information about the initial release of these models on Amazon Bedrock, refer to our previous blog post.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/open-ai-open-weight-models-new-regions-amazon-bedrock",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "now-available",
        "support",
        "new-region",
        "expansion"
      ]
    },
    {
      "id": "aws-news-61df979b79c6",
      "title": "Tailor Amazon SageMaker Unified Studio project environments to your needs using custom blueprints",
      "description": "Amazon SageMaker Unified Studio is a single data and AI development environment that brings together data preparation, analytics, machine learning (ML), and generative AI development in one place. By unifying these workflows, it saves teams from managing multiple tools and makes it straightforward for data scientists, analysts, and developers to build, train, and deploy ML […]",
      "link": "https://aws.amazon.com/blogs/big-data/tailor-amazon-sagemaker-unified-studio-project-environments-to-your-needs-using-custom-blueprints/",
      "pubDate": "2025-09-17T22:49:58.000Z",
      "source": "bigDataBlog",
      "services": [
        "sagemaker",
        "unified studio"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "unified studio"
      ]
    },
    {
      "id": "aws-news-4cc4978a07d1",
      "title": "Supercharge your organization’s productivity with the Amazon Q Business browser extension",
      "description": "In this post, we showed how to use the Amazon Q Business browser extension to give your team seamless access to AI-driven insights and assistance. The browser extension is now available in US East (N. Virginia) and US West (Oregon) AWS Regions for Mozilla, Google Chrome, and Microsoft Edge as part of the Lite Subscription.",
      "link": "https://aws.amazon.com/blogs/machine-learning/supercharge-your-organizations-productivity-with-the-amazon-q-business-browser-extension/",
      "pubDate": "2025-09-17T19:37:32.000Z",
      "source": "mlBlog",
      "services": [
        "amazon q",
        "q business"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q business",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-914f37d8d608",
      "title": "Build Agentic Workflows with OpenAI GPT OSS on Amazon SageMaker AI and Amazon Bedrock AgentCore",
      "description": "In this post, we show how to deploy gpt-oss-20b model to SageMaker managed endpoints and demonstrate a practical stock analyzer agent assistant example with LangGraph, a powerful graph-based framework that handles state management, coordinated workflows, and persistent memory systems.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-agentic-workflows-with-openai-gpt-oss-on-amazon-sagemaker-ai-and-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-17T19:31:44.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "sagemaker"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-f7a834be9ed0",
      "title": "Amazon EC2 I8ge instances now available in AWS Europe (Frankfurt)",
      "description": "Starting today, Amazon Elastic Compute Cloud (Amazon EC2) storage optimized I8ge instances are available in AWS Europe (Frankfurt) region. I8ge instances are powered by AWS Graviton4 processors to deliver up to 60% better compute performance compared to previous generation Graviton2-based storage optimized Amazon EC2 instances. I8ge instances use the latest third generation AWS Nitro SSDs, local NVMe storage that deliver up to 55% better real-time storage performance per TB while offering up to 60% lower storage I/O latency and up to 75% lower storage I/O latency variability compared to previous generation Im4gn instances. At 120 TB, I8ge instances have the highest storage density among AWS Graviton-based storage optimized Amazon EC2 instances. These instances are built on the AWS Nitro System, which oﬄoads CPU virtualization, storage, and networking functions to dedicated hardware and software enhancing the performance and security for your workloads.\n \nI8ge instances offer instance sizes up to 48xlarge including two metal sizes, 1,536 GiB of memory, and 120 TB instance storage. At 300 Gbps, these instances have the highest networking bandwidth among storage optimized Amazon EC2 instances. They are ideal for real-time applications that require much larger storage density such as relational databases, non-relational databases, streaming databases, search queries and data analytics.\n \nTo learn more, see Amazon EC2 I8ge instances. To begin your Graviton journey, visit the Level up your compute with AWS Graviton page. To get started, see AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-i8ge-instances-in-europe-frankfurt",
      "pubDate": "2025-09-17T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2",
        "graviton"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "graviton",
        "now-available"
      ]
    },
    {
      "id": "aws-news-949eb2c3ce5a",
      "title": "AWS Parallel Computing Service (PCS) now supports Amazon EC2 Capacity Blocks for ML",
      "description": "AWS Parallel Computing Service (PCS) now supports Amazon EC2 Capacity Blocks for ML. You can now use Amazon EC2 instances reserved using EC2 Capacity Blocks natively in PCS clusters.\n  Native support for EC2 Capacity Blocks in PCS simplifies capacity planning for cutting-edge GPU-based workloads in Slurm clusters, helping to ensure that GPU capacity is available when and where it’s needed. EC2 Capacity Blocks can be associated with PCS compute node groups via an EC2 Launch Template.\n  PCS is a managed service that makes it easier for you to run and scale your high performance computing (HPC) workloads and build scientific and engineering models on AWS using Slurm. You can use PCS to build complete, elastic environments that integrate compute, storage, networking, and visualization tools. PCS simplifies cluster operations with managed updates and built-in observability features, helping to remove the burden of maintenance. You can work in a familiar environment, focusing on your research and innovation instead of worrying about infrastructure.\n  PCS now supports EC2 Capacity Blocks in all AWS Regions where both services are available. Read more about PCS support for EC2 Capacity Blocks in the PCS User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-parallel-computing-service-ec2-capacity-blocks-ml/",
      "pubDate": "2025-09-17T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "ec2",
        "launch",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-bb092a4052ee",
      "title": "Get started with Amazon OpenSearch Service: T-shirt size your domain for log analytics",
      "description": "When you’re spinning up your Amazon OpenSearch Service domain, you need to figure out the storage, instance types, and instance count; decide the sharding strategies and whether to use a cluster manager; and enable zone awareness. Generally, we consider storage as a guideline for determining instance count, but not other parameters. In this post, we […]",
      "link": "https://aws.amazon.com/blogs/big-data/get-started-with-amazon-opensearch-service-t-shirt-size-your-domain-for-log-analytics/",
      "pubDate": "2025-09-16T20:26:45.000Z",
      "source": "bigDataBlog",
      "services": [
        "opensearch",
        "opensearch service"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "opensearch",
        "opensearch service"
      ]
    },
    {
      "id": "aws-news-e5fcb3da4695",
      "title": "Amazon SageMaker introduces Amazon S3 based shared storage for enhanced project collaboration",
      "description": "AWS recently announced that Amazon SageMaker now offers Amazon Simple Storage Service (Amazon S3) based shared storage as the default project file storage option for new Amazon SageMaker Unified Studio projects. This feature addresses the deprecation of AWS CodeCommit while providing teams with a straightforward and consistent way to collaborate on project files across the […]",
      "link": "https://aws.amazon.com/blogs/big-data/amazon-sagemaker-introduces-amazon-s3-based-shared-storage-for-enhanced-project-collaboration/",
      "pubDate": "2025-09-16T20:23:44.000Z",
      "source": "bigDataBlog",
      "services": [
        "sagemaker",
        "unified studio",
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "unified studio",
        "s3"
      ]
    },
    {
      "id": "aws-news-284b40fb9a60",
      "title": "Amazon EC2 I7i instances now available in South America (São Paulo), Canada West (Calgary) regions",
      "description": "Amazon Web Services (AWS) announces the availability of high performance Storage Optimized Amazon EC2 I7i instances in the AWS South America (São Paulo), Canada West (Calgary) regions. Powered by 5th generation Intel Xeon Scalable processors with an all-core turbo frequency of 3.2 GHz, these new instances deliver up to 23% better compute performance and more than 10% better price performance over previous generation I4i instances. Powered by 3rd generation AWS Nitro SSDs, I7i instances offer up to 45TB of NVMe storage with up to 50% better real-time storage performance, up to 50% lower storage I/O latency, and up to 60% lower storage I/O latency variability compared to I4i instances.\n  I7i instances offer the best compute and storage performance for x86-based storage optimized instances in Amazon EC2, ideal for I/O intensive and latency-sensitive workloads that demand very high random IOPS performance with real-time latency to access the small to medium size datasets (multi-TBs). Additionally, torn write prevention feature support up to 16KB block sizes, enabling customers to eliminate database performance bottlenecks.\n  I7i instances are available in eleven sizes - nine virtual sizes up to 48xlarge and two bare metal sizes - delivering up to 100Gbps of network bandwidth and 60Gbps of Amazon Elastic Block Store (EBS) bandwidth.\n To learn more, visit the I7i instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-i7i-instances-sao-paulo-calgary-regions/",
      "pubDate": "2025-09-16T17:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-f789b4eea2b5",
      "title": "Amazon Lex provides generative AI based enhanced natural language understanding in eight new languages",
      "description": "Amazon Lex now allows you to leverage large language models (LLMs) to improve the natural language understanding of your deterministic conversational AI bots in eight new languages: Chinese, Japanese, Korean, Portuguese, Catalan, French, Italian, and German. With this capability, your voice- and chat-bots can better handle complex utterances, maintain accuracy despite spelling errors, and extract key information from verbose inputs to fulfill the customer’s request. For example, a customer could say ‘Hi I want to book a flight for my wife, my two kids and myself’, and the LLM will properly identify to book flight tickets for four people.\n \nThis feature is available in 10 commercial AWS Regions where Amazon Connect is available: Europe (Ireland), Europe (Frankfurt), US East (N. Virginia), Asia Pacific (Seoul), Europe (London), Asia Pacific (Tokyo), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central). To learn more about this feature, visit Amazon Lex documentation or to learn how Amazon Connect and Amazon Lex deliver cloud-based conversational AI experiences for contact centers, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-lex-generative-ai-natural-language-eight-languages/",
      "pubDate": "2025-09-16T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "ga"
      ]
    },
    {
      "id": "aws-news-26d7a1c18ba9",
      "title": "New fault action in AWS FIS to inject I/O latency on Amazon EBS volumes",
      "description": "Today, Amazon EBS announced a new latency injection action in AWS Fault Injection Service (FIS), a fully managed service for running fault injection experiments. You can now use this action to inject I/O latency on your volumes as part of a controlled testing experiment to understand how your mission-critical applications respond to storage faults. With the new fault action, you can test your architecture against elevated storage latency, allowing you to observe application behavior and fine-tune your monitoring and recovery processes to ensure high availability.\n  EBS volumes are designed to meet the needs of highly available, latency-sensitive applications such as Oracle, SAP HANA, and Microsoft SQL Server. The latency injection action simulates degraded I/O performance on your volume to replicate the real-world signals, such as Amazon CloudWatch alarms and operating system timeouts, that occur during storage performance issues. Using this action, you can build confidence that your application can withstand and quickly recover from disruptions that cause high I/O latency on your EBS volume. To get started, you can directly use the pre-defined latency injection experiment templates available in the EBS and FIS consoles. Alternatively, you can customize these experiment templates or create your own experiment templates to meet your application-specific testing needs. You can integrate these latency injection experiments into your existing chaos engineering tests, continuous integration, and release testing, as well as combine multiple FIS actions in one experiment.\n  This new action is available in all AWS Regions where AWS FIS is available. To learn more, visit the EBS FIS actions user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-fis-action-inject-io-latency-on-ebs/",
      "pubDate": "2025-09-16T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "cloudwatch",
        "ga",
        "integration"
      ]
    },
    {
      "id": "aws-news-15d87cb93331",
      "title": "Streamline access to ISO-rating content changes with Verisk rating insights and Amazon Bedrock",
      "description": "In this post, we dive into how Verisk Rating Insights, powered by Amazon Bedrock, large language models (LLM), and Retrieval Augmented Generation (RAG), is transforming the way customers interact with and access ISO ERC changes.",
      "link": "https://aws.amazon.com/blogs/machine-learning/streamline-access-to-iso-rating-content-changes-with-verisk-rating-insights-and-amazon-bedrock/",
      "pubDate": "2025-09-16T16:43:42.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-431447666038",
      "title": "Unified multimodal access layer for Quora’s Poe using Amazon Bedrock",
      "description": "In this post, we explore how the AWS Generative AI Innovation Center and Quora collaborated to build a unified wrapper API framework that dramatically accelerates the deployment of Amazon Bedrock FMs on Quora’s Poe system. We detail the technical architecture that bridges Poe’s event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, demonstrate how a template-based configuration system reduced deployment time from days to 15 minutes, and share implementation patterns for protocol translation, error handling, and multi-modal capabilities.",
      "link": "https://aws.amazon.com/blogs/machine-learning/unified-multimodal-access-layer-for-quoras-poe-using-amazon-bedrock/",
      "pubDate": "2025-09-16T16:40:11.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "nova"
      ]
    },
    {
      "id": "aws-news-7f6d11754731",
      "title": "Amazon EC2 supports detailed performance stats on all NVMe local volumes",
      "description": "Today, Amazon announced the availability of detailed performance statistics for Amazon EC2 instance store NVMe volumes. This new capability delivers real-time visibility into the performance of your AWS Nitro System-based EC2 instance store NVMe volumes, making it easier to monitor storage health and quickly resolve application performance issues.\n \nWith EC2 detailed performance statistics, you can access 11 comprehensive metrics at one second granularity to monitor input/output (I/O) statistics of your locally attached NVMe volumes, including queue length measurements, IOPS, throughput, and detailed I/O latency histograms. These metrics are similar to the detailed performance statistics available for EBS volumes, providing a consistent monitoring experience across both storage types. The granular visibility provided by these metrics helps you identify specific workloads affected by performance variations, and optimize your application's IO patterns for maximum efficiency. Additionally, the metrics include latency histograms broken down by IO size, providing even more detailed insights into performance patterns.\n  Detailed performance statistics for EC2 instance store NVMe volumes are available by default for all Nitro-based EC2 instances with locally attached NVMe volumes across all AWS Commercial and China Regions, at no additional charge.\n \nTo learn more about the EC2 instance store NVMe detailed performance statistics and how to access them, please visit the documentation here.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-detailed-performance-stats-nvme-local-volumes/",
      "pubDate": "2025-09-16T16:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-caf42165b637",
      "title": "AWS Storage Gateway now supports IPv6",
      "description": "AWS Storage Gateway announces Internet Protocol version 6 (IPv6) support for AWS Storage Gateway endpoints, APIs, and gateway appliance interfaces. This enhancement enables both IPv6 and IPv4 access to our new dual-stack endpoints. The existing AWS Storage Gateway endpoints supporting IPv4 only will remain available for backwards compatibility.\n  AWS Storage Gateway provides on-premises access to data stored in AWS storage. With this launch, customers can standardize their applications and workflows for managing their AWS Storage Gateway resources on IPv6 while maintaining backward compatibility with IPv4 clients. By using the new dual-stack capabilities in the Storage Gateway appliances, service endpoints, and APIs, customers can transition from IPv4 to IPv6 gradually without needed to switch all their networking at once.\n  AWS Storage Gateway support for IPv6 is available in all AWS Regions where the service is offered. To learn more, visit the AWS Storage Gateway user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-storage-gateway-ipv6",
      "pubDate": "2025-09-16T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "rds",
        "launch",
        "ga",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-5429585a0e6f",
      "title": "AWS Transfer Family is now available in AWS Asia Pacific (Taipei) region",
      "description": "Customers in AWS Asia Pacific (Taipei) Region can now use AWS Transfer Family for file transfers over Secure File Transfer Protocol (SFTP), File Transfer Protocol (FTP), FTP over SSL (FTPS) and Applicability Statement 2 (AS2).\n  AWS Transfer Family provides fully managed file transfers for Amazon Simple Storage Service (Amazon S3) and Amazon Elastic File System (Amazon EFS) over SFTP, FTP, FTPS and AS2 protocols. In addition to file transfers, Transfer Family enables common file processing and event-driven automation for managed file transfer (MFT) workflows, helping customers to modernize and migrate their business-to-business file transfers to AWS.\n  To learn more about AWS Transfer Family, visit our product page and user-guide. See the AWS Region Table for complete regional availability information.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-transfer-family-asia-pacific-taipei-region/",
      "pubDate": "2025-09-16T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "now-available"
      ]
    },
    {
      "id": "aws-news-98a88cfc16cc",
      "title": "Amazon OpenSearch Service announces Derived Source for storage optimization",
      "description": "Amazon OpenSearch Service introduces support for Derived Source, a new feature that can help reduce the amount of storage required for your OpenSearch Service domains. With derived source support, you can skip storing source fields and dynamically derive them when required. \n  OpenSearch stores each ingested document in the _source field and also indexes individual fields for search. The _source field can consume significant storage space. To reduce storage use, you can configure OpenSearch to skip storing the _source field and instead reconstruct it dynamically when needed, for example, during search, get, mget, reindex, or update operations.\n  Derived Source is available in all regions where OpenSearch 3.1 is supported. The feature is opt-in and can be enabled at index creation using composite index settings.\n  Please refer to the AWS Regional Services List for more information about Amazon OpenSearch Service availability. To learn more about Derived Source, see the OpenSearch documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-opensearch-derived-source/",
      "pubDate": "2025-09-16T04:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "opensearch",
        "opensearch service"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "opensearch",
        "opensearch service",
        "new-feature",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-4bc44e9f4d66",
      "title": "Amazon S3 Batch Operations now supports managing buckets or prefixes in a single step in AWS Management Console",
      "description": "Amazon S3 Batch Operations now supports managing objects within an S3 bucket, prefix, suffix, or more, in a single step in AWS Management Console. When creating an S3 Batch Operation, customers can specify the objects on which to perform the operation. With this feature, you have the option to instead specify an entire bucket, prefix, suffix, creation date, or storage class. Amazon S3 Batch Operations will then quickly apply the operation to all the matching objects and notify you when the job completes.\n \nS3 Batch Operations lets you easily perform one-time or recurring batch workloads such as copying objects between staging and production buckets, restoring archived backups from S3 Glacier storage classes, or computing objects checksum to verify the content of stored datasets, at any scale. After starting your job, S3 Batch Operations automatically processes all of the objects that match your filtering criteria. You will receive a detailed completion report with the status of each object once the job completes.\n  This feature of S3 Batch Operations is available in all AWS Regions. You can get started through AWS Management Console, AWS Command Line Interface (CLI), or the AWS Software Development Kit (SDK) client. For pricing information, please visit the Management & Insights tab of the Amazon S3 pricing page. To learn more about S3 Batch Operations, visit the S3 User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-s3-batch-operations-managing-buckets-console",
      "pubDate": "2025-09-15T21:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "support"
      ]
    },
    {
      "id": "aws-news-3e8ef7c7d01f",
      "title": "Automate and orchestrate Amazon EMR jobs using AWS Step Functions and Amazon EventBridge",
      "description": "In this post, we discuss how to build a fully automated, scheduled Spark processing pipeline using Amazon EMR on EC2, orchestrated with Step Functions and triggered by EventBridge. We walk through how to deploy this solution using AWS CloudFormation, processes COVID-19 public dataset data in Amazon Simple Storage Service (Amazon S3), and store the aggregated results in Amazon S3.",
      "link": "https://aws.amazon.com/blogs/big-data/automate-and-orchestrate-amazon-emr-jobs-using-aws-step-functions-and-amazon-eventbridge/",
      "pubDate": "2025-09-15T17:10:24.000Z",
      "source": "bigDataBlog",
      "services": [
        "s3",
        "ec2",
        "emr",
        "cloudformation",
        "eventbridge",
        "step functions"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "ec2",
        "emr",
        "cloudformation",
        "eventbridge",
        "step functions",
        "ga"
      ]
    },
    {
      "id": "aws-news-591c4b0afb89",
      "title": "How msg enhanced HR workforce transformation with Amazon Bedrock and msg.ProfileMap",
      "description": "In this post, we share how msg automated data harmonization for msg.ProfileMap, using Amazon Bedrock to power its large language model (LLM)-driven data enrichment workflows, resulting in higher accuracy in HR concept matching, reduced manual workload, and improved alignment with compliance requirements under the EU AI Act and GDPR.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-msg-enhanced-hr-workforce-transformation-with-amazon-bedrock-and-msg-profilemap/",
      "pubDate": "2025-09-15T17:05:18.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-8fb81cf48b84",
      "title": "Amazon OpenSearch Service now supports OpenSearch version 3.1",
      "description": "You can now run OpenSearch version 3.1 in Amazon OpenSearch Service. OpenSearch 3.1 introduces several improvements in areas like search relevance and performance, and introduces features that simplify development of vector-driven applications for generative AI workloads.\n \nThis launch incorporates Lucene 10 that enables optimized vector field indexing resulting in faster indexing times and reduced index sizes, sparse indexing for CPU and storage efficiency improvements, and vector quantization to reduce memory usage. Other key areas of improvement include improved range query performance, which benefits log analytics and time-series workloads, and reduced latency for high-cardinality aggregations.\n \nThis launch also introduces a new Search Relevance Workbench, which provides integrated tools for teams to evaluate and optimize search quality through experimentation. Additionally, this launch includes several improvements in vector search capabilities. First, Z-score normalization improves hybrid search reliability by reducing the impact of outliers and different score scales. Finally, you can now boost efficiency of searches using memory-optimized search that enables the Faiss engine to operate efficiently by memory-mapping the index file and using the operating system's file cache to serve search requests.\n \nFor information on upgrading to OpenSearch 3.1, please see the documentation. OpenSearch 3.1 is now available in all AWS Regions where Amazon OpenSearch Service is available.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-opensearch-service-opensearch-version-3-1/",
      "pubDate": "2025-09-15T14:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "opensearch",
        "opensearch service"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "opensearch",
        "opensearch service",
        "launch",
        "ga",
        "now-available",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-79795044339d",
      "title": "Automate advanced agentic RAG pipeline with Amazon SageMaker AI",
      "description": "In this post, we walk through how to streamline your RAG development lifecycle from experimentation to automation, helping you operationalize your RAG solution for production deployments with Amazon SageMaker AI, helping your team experiment efficiently, collaborate effectively, and drive continuous improvement.",
      "link": "https://aws.amazon.com/blogs/machine-learning/automate-advanced-agentic-rag-pipeline-with-amazon-sagemaker-ai/",
      "pubDate": "2025-09-12T17:36:19.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "improvement"
      ]
    },
    {
      "id": "aws-news-8acfb46eaaa9",
      "title": "Serverless generative AI architectural patterns – Part 2",
      "description": "This post explores two complementary approaches for non-real-time scenarios: buffered asynchronous processing for time-intensive individual requests, and batch processing for scheduled or event-driven workflows.",
      "link": "https://aws.amazon.com/blogs/compute/part-2-serverless-generative-ai-architectural-patterns/",
      "pubDate": "2025-09-04T21:46:26.000Z",
      "source": "computeBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": []
    },
    {
      "id": "aws-news-a04bdd57b5ee",
      "title": "Serverless generative AI architectural patterns – Part 1",
      "description": "This two-part series explores the different architectural patterns, best practices, code implementations, and design considerations essential for successfully integrating generative AI solutions into both new and existing applications. In this post, we focus on patterns applicable for architecting real-time generative AI applications.",
      "link": "https://aws.amazon.com/blogs/compute/serverless-generative-ai-architectural-patterns/",
      "pubDate": "2025-09-04T21:45:47.000Z",
      "source": "computeBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": []
    },
    {
      "id": "aws-news-07cadadbf419",
      "title": "Effectively building AI agents on AWS Serverless",
      "description": "Imagine an AI assistant that doesn’t just respond to prompts – it reasons through goals, acts, and integrates with real-time systems. This is the promise of agentic AI. According to Gartner, by 2028 over 33% of enterprise applications will embed agentic capabilities – up from less than 1% today. While early generative AI efforts focused […]",
      "link": "https://aws.amazon.com/blogs/compute/effectively-building-ai-agents-on-aws-serverless/",
      "pubDate": "2025-08-14T22:38:10.000Z",
      "source": "computeBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga"
      ]
    },
    {
      "id": "aws-news-b0328ad8bafd",
      "title": "Deploying external boot volumes with AWS Outposts",
      "description": "Building on our previous announcement, AWS Outposts third-party storage integration for data volumes, AWS is expanding its collaboration with third-party storage solutions by introducing support for boot volumes backed by external storage arrays. In this post we show you how to boot Amazon Elastic Compute Cloud (Amazon EC2) instances on Outposts directly from NetApp on-premise […]",
      "link": "https://aws.amazon.com/blogs/compute/deploying-external-boot-volumes-with-aws-outposts/",
      "pubDate": "2025-07-17T21:18:52.000Z",
      "source": "computeBlog",
      "services": [
        "ec2",
        "outposts"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "outposts",
        "integration",
        "support",
        "announcement"
      ]
    },
    {
      "id": "aws-news-7a855eaafa04",
      "title": "Simplifying sustainability reporting using AWS and generative AI in banking",
      "description": "In this post, you learn how you can use generative AI services on Amazon Web Services (AWS) to automate your sustainability reporting requirements, reduce manual effort, and improve accuracy. You do this by implementing an automated solution for extracting, processing, and validating data from corporate reports.",
      "link": "https://aws.amazon.com/blogs/architecture/simplifying-sustainability-reporting-using-aws-and-generative-ai-in-banking/",
      "pubDate": "2025-06-26T17:54:46.000Z",
      "source": "architectureBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": []
    },
    {
      "id": "aws-news-2355834bf9d4",
      "title": "Amazon Bedrock baseline architecture in an AWS landing zone",
      "description": "In this post, we explore the Amazon Bedrock baseline architecture and how you can secure and control network access to your various Amazon Bedrock capabilities within AWS network services and tools. We discuss key design considerations, such as using Amazon VPC Lattice auth policies, Amazon Virtual Private Cloud (Amazon VPC) endpoints, and AWS Identity and Access Management (IAM) to restrict and monitor access to your Amazon Bedrock capabilities.",
      "link": "https://aws.amazon.com/blogs/architecture/amazon-bedrock-baseline-architecture-in-an-aws-landing-zone/",
      "pubDate": "2025-06-23T18:36:51.000Z",
      "source": "architectureBlog",
      "services": [
        "bedrock",
        "iam"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "iam"
      ]
    },
    {
      "id": "aws-news-54c273e45b01",
      "title": "Upgrading your AWS SDK for Go from V1 to V2 with Amazon Q Developer",
      "description": "Software development is far more than just writing code. In reality, a developer spends a large amount of time maintaining existing applications and fixing bugs. For example, migrating a Go application from the older AWS SDK for Go v1 to the newer v2 can be a significant undertaking, but it’s a crucial step to future-proof […]",
      "link": "https://aws.amazon.com/blogs/developer/upgrading-your-aws-sdk-for-go-from-v1-to-v2-with-amazon-q-developer/",
      "pubDate": "2025-06-18T06:38:24.000Z",
      "source": "developersAndDevOps",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer"
      ]
    },
    {
      "id": "aws-news-6a09dfeff5cf",
      "title": "Optimizing fleet operations using Amazon SageMaker AI and Amazon Bedrock",
      "description": "In this post, we'll explore how to maximize the value of dashcam footage through best practices for implementing and managing Computer Vision systems in commercial fleet operations. We'll demonstrate how to build and deploy edge-based machine learning models that provide real-time alerts for distracted driving behaviors, while effectively collecting, processing, and analyzing footage to train these AI models.",
      "link": "https://aws.amazon.com/blogs/architecture/optimizing-fleet-operations-using-amazon-sagemaker-ai-and-amazon-bedrock/",
      "pubDate": "2025-05-28T18:29:26.000Z",
      "source": "architectureBlog",
      "services": [
        "bedrock",
        "sagemaker"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-c4f514e85eef",
      "title": "AWS SDK for Ruby: Deprecating Ruby 2.5 & 2.6 Runtime Supports and Future Compatibility",
      "description": "Effective June 2, 2025, AWS SDK for Ruby Version 3 will no longer support following end-of-life (EOL) Ruby runtime versions: Ruby 2.5 (EOL began on 2021-04-05) Ruby 2.6 (EOL began on 2022-04-12) To ensure your applications and services remain secure, we strongly encourage you to upgrade to Ruby 2.7 or later. Moving forward, AWS SDK […]",
      "link": "https://aws.amazon.com/blogs/developer/aws-sdk-for-ruby-deprecating-ruby-2-5-2-6-runtime-supports-and-future-compatibility/",
      "pubDate": "2025-03-27T15:08:27.000Z",
      "source": "developersAndDevOps",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-5cf08af5aca4",
      "title": "Announcing the Developer Preview of Amazon S3 Transfer Manager in Rust",
      "description": "We are excited to announce the Developer Preview of the Amazon S3 Transfer Manager for Rust, a high-level utility that speeds up and simplifies uploads and downloads with Amazon Simple Storage Service (Amazon S3). Using this new library, developers can efficiently transfer data between Amazon S3 and various sources, including files, in-memory buffers, memory streams, […]",
      "link": "https://aws.amazon.com/blogs/developer/announcing-the-developer-preview-of-amazon-s3-transfer-manager-in-rust/",
      "pubDate": "2025-03-26T15:52:22.000Z",
      "source": "developersAndDevOps",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "preview"
      ]
    }
  ]
}