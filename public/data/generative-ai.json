{
  "lastUpdated": "2025-12-08T06:18:38.115Z",
  "category": "generative-ai",
  "totalItems": 51,
  "items": [
    {
      "id": "aws-news-8273e71b1636",
      "title": "Amazon OpenSearch Service now supports automatic semantic enrichment",
      "description": "Amazon OpenSearch Service now brings automatic semantic enrichment to managed clusters, matching the capability we launched for OpenSearch Serverless earlier this year. This feature allows you to leverage the power of semantic search with minimal configuration effort.\n  Traditional lexical search only matches exact phrases, often missing relevant content. Automatic semantic enrichment understands context and meaning, delivering more relevant results. For example, a search for \"eco-friendly transportation options\" finds matches about \"electric vehicles\" or \"public transportation\"—even when these exact terms aren't present. This new capability handles all semantic processing automatically, eliminating the need to manage machine learning models. It supports both English-only and multi-lingual variants, covering 15 languages including Arabic, French, Hindi, Japanese, Korean, and more. You pay only for actual usage during data ingestion, billed as OpenSearch Compute Unit (OCU) - Semantic Search. View the pricing page for cost details and a pricing example.\n  This feature is now available for Amazon OpenSearch Service domains running OpenSearch version 2.19 or later. Currently, this feature supports non-VPC domains in the following AWS Regions: US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), Europe (Ireland), and Europe (Stockholm).\n  Get started with our documentation on automatic semantic enrichment.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/opensearch-service-automatic-semantic-enrichment/",
      "pubDate": "2025-12-05T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "opensearch",
        "opensearch service"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "opensearch",
        "opensearch service",
        "launch",
        "ga",
        "now-available",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-7756d1765b26",
      "title": "TwelveLabs’ Pegasus 1.2 model now in 23 new AWS regions via Global cross-region inference",
      "description": "Amazon Bedrock introduces Global cross-Region inference for TwelveLabs' Pegasus 1.2, expanding model availability to 23 new regions in addition to the seven regions where the model was already available. You can now also access the model in all EU regions in Amazon Bedrock using Geographic cross-Region inference. Geographic cross-Region inference is ideal for workloads with data residency or compliance requirements within a specific geographic boundary, while Global cross-Region inference is recommended for applications that prioritize availability and performance across multiple geographies.\n  Pegasus 1.2 is a powerful video-first language model that can generate text based on the visual, audio, and textual content within videos. Specifically designed for long-form video, it excels at video-to-text generation and temporal understanding. With Pegasus 1.2's availability in these additional regions, you can now build video-intelligence applications closer to your data and end users, reducing latency and simplifying your architecture.\n  For a complete list of supported inference profiles and regions for Pegasus 1.2, refer to the Cross-Region Inference documentation. To get started with Pegasus 1.2, visit the Amazon Bedrock console. To learn more, read the product page and Amazon Bedrock documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/twelvelabs-pegasus-available-with-global-cross-region-inference/",
      "pubDate": "2025-12-05T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "ga",
        "support",
        "new-region"
      ]
    },
    {
      "id": "aws-news-0424929cc700",
      "title": "AWS Elastic Beanstalk now supports Node.js 24 on Amazon Linux 2023",
      "description": "AWS Elastic Beanstalk now enables customers to build and deploy Node.js 24 applications on Amazon Linux 2023 (AL2023) platform. This latest platform support allows developers to leverage the newest features and improvements in Node.js while taking advantage of the enhanced security and performance of AL2023.\n \nAWS Elastic Beanstalk is a service that provides the ability to deploy and manage applications in AWS without worrying about the infrastructure that runs those applications. Node.js 24 on AL2023 delivers updates to the V8 JavaScript engine, npm 11, and security and performance improvements. Developers can create Elastic Beanstalk environments running Node.js 24 on AL2023 through the Elastic Beanstalk Console, CLI, or API.\n \nThis platform is available in all commercial AWS Regions where Elastic Beanstalk is available, including the AWS GovCloud (US) Regions. For a complete list of regions and service offerings, see AWS Regions.\n \nTo learn more about Node.js 24 on Amazon Linux 2023, see the AWS Elastic Beanstalk Developer guide. For additional information, visit the AWS Elastic Beanstalk product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/elastic-beanstalk-node-js-24-linux-2023/",
      "pubDate": "2025-12-05T18:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "update",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-99629c57268b",
      "title": "Amazon Connect Customer Profiles launches new segmentation capabilities (Beta)",
      "description": "Amazon Connect Customer Profiles now offers new segmentation capabilities powered by Spark SQL (Beta), enabling you to build sophisticated customer segments using your complete Customer Profiles data with AI assistance.\n  You can:\n  \n \n \nAccess complete profile data: Use both custom objects and standard objects for segmentation\n \n \nLeverage SQL capabilities: Join objects, filter with statistical functions like percentiles, and standardize date fields for complex analysis\n \n \nBuild segments with AI assistance: Use natural language prompts with the Segment AI assistant to automatically generate segment definitions in Spark SQL, or write SQL directly\n \n \nValidate before deployment: Review AI-generated SQL, view natural language explanations, and get automatic segment estimates\n \n \nFor example, you can create segments like \"customers who called customer services more than 3 times in the past month about new purchases they made\" or \"high-value customers in the 90th percentile of lifetime spend\" to enable precise targeting for outbound campaigns and personalized customer experiences.\n  These new segmentation capabilities are offered alongside existing segmentation features. Both integrate seamlessly with segment membership calls, Flow blocks, and Outbound Campaigns, allowing you to choose the approach that best fits your use case.\n  Getting started: Enable Data store from the Customer Profiles page to use the new segmentation capabilities\n  Availability: Available in all AWS regions where Amazon Connect Customer Profiles is offered.\n  For more information, see Build customer segments in Amazon Connect in the Amazon Connect Administrator Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-connect-customer-profiles/",
      "pubDate": "2025-12-05T15:04:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "personalize"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "personalize",
        "launch",
        "beta"
      ]
    },
    {
      "id": "aws-news-5a5afa48e9ee",
      "title": "Amazon Q now can analyze SES email sending",
      "description": "Today, Amazon Q (Q) added support for analyzing email sending in Amazon Simple Email Service (SES). Now customers can ask Q questions about their SES resource setup and usage patterns, and Q will help them optimize their configuration and troubleshoot deliverability problems. This makes it easier to manage SES operational activities with less technical knowledge.\n  Previously, customers could use SES features such as Virtual Deliverability Manager to manage and explore their SES resource configuration and usage. SES provided convenient dashboard views and query tools to help customers find information, however customers needed deep understanding of email sending concepts to interact with the service. Now, customers can ask Q for help in optimizing resource configuration and troubleshooting deliverability challenges. Q will evaluate customer’s usage patterns and SES resource configuration, find the answers customers need, and help them understand the context without requiring pre-knowledge or manual exploration.\n  Q supports SES resource analysis in all AWS Regions where SES and Q are available.\n  For more information, see the Q documentation for information about interacting with SES through Q.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-q-analyze-ses-email-sending/",
      "pubDate": "2025-12-05T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "support"
      ]
    },
    {
      "id": "aws-news-39665ab10ff1",
      "title": "AWS launches simplified enablement of AWS CloudTrail events in Amazon CloudWatch",
      "description": "Today, AWS launches simplified enablement of AWS CloudTrail events in Amazon CloudWatch, a monitoring and logging service that helps you collect, monitor, and analyze log data from your AWS resources and applications. With this launch, you can now centrally configure collection of CloudTrail events in CloudWatch alongside other popular AWS log sources such as Amazon VPC flow logs and Amazon EKS Control Plane Logs. CloudWatch's ingestion experience provides a consolidated view that simplifies collecting telemetry from different sources for accounts in your AWS Organization thus ensuring comprehensive monitoring and data collection across your AWS environment.\n  This new integration leverages service-linked channels (SLCs) to receive events from CloudTrail without requiring trails, and also provides additional benefits such as safety-checks and termination protection. You incur both CloudTrail event delivery charges and CloudWatch Logs ingestion fees based on custom logs pricing.\n  To learn more about enablement of CloudTrail events in CloudWatch and supported AWS regions, visit the Amazon CloudWatch documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/key-enhancements-cloudtrail-events-cloudwatch/",
      "pubDate": "2025-12-05T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "eks",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "eks",
        "cloudwatch",
        "launch",
        "ga",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-116745600502",
      "title": "AWS Elastic Beanstalk now supports Python 3.14 on Amazon Linux 2023",
      "description": "AWS Elastic Beanstalk now enables customers to build and deploy Python 3.14 applications on Amazon Linux 2023 (AL2023) platform. This latest platform support allows developers to leverage the newest features and improvements in Python while taking advantage of the enhanced security and performance of AL2023.\n \nAWS Elastic Beanstalk is a service that provides the ability to deploy and manage applications in AWS without worrying about the infrastructure that runs those applications. Python 3.14 on AL2023 delivers enhanced interactive interpreter capabilities, improved error messages, important security and API improvements. Developers can create Elastic Beanstalk environments running Python 3.14 on AL2023 through the Elastic Beanstalk Console, CLI, or API.\n \nThis platform is available in all commercial AWS Regions where Elastic Beanstalk is available, including the AWS GovCloud (US) Regions. For a complete list of regions and service offerings, see AWS Regions.\n \nTo learn more about Python 3.14 on Amazon Linux 2023, see the AWS Elastic Beanstalk Developer guide. For additional information, visit the AWS Elastic Beanstalk product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/elastic-beanstalk-python-314-linux-2023/",
      "pubDate": "2025-12-05T08:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-509a6eaf3b64",
      "title": "Announcing new Amazon EC2 M9g instances powered by AWS Graviton5 processors (Preview)",
      "description": "Starting today, new general purpose Amazon Elastic Compute Cloud (Amazon EC2) M9g instances, powered by AWS Graviton5 processors, are available in preview. AWS Graviton5 is the latest in the Graviton family of processors that are custom designed by AWS to provide the best price performance for workloads in Amazon EC2. These instances offer up to 25% better compute performance, and higher networking and Amazon Elastic Block Store (Amazon EBS) bandwidth than AWS Graviton4-based M8g instances. They are up to 30% faster for databases, up to 35% faster web applications, and up to 35% faster for machine learning workloads compared to M8g.\n  M9g instances are built on the AWS Nitro System, a collection of hardware and software innovations designed by AWS. The AWS Nitro System enables the delivery of efficient, flexible, and secure cloud services with isolated multitenancy, private networking, and fast local storage. Amazon EC2 M9g instances are ideal for workloads such as application servers, microservices, gaming servers, midsize data stores, and caching fleets.\n  To learn more or request access to the M9g preview, see Amazon EC2 M9g instances. To begin your Graviton journey, visit the Level up your compute with AWS Graviton page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/ec2-m9g-instances-graviton5-processors-preview/",
      "pubDate": "2025-12-04T09:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "lex",
        "ec2",
        "graviton"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "lex",
        "ec2",
        "graviton",
        "preview",
        "ga"
      ]
    },
    {
      "id": "aws-news-a465d0dd13ac",
      "title": "Announcing TypeScript support in Strands Agents (preview) and more",
      "description": "In May, we open sourced the Strands Agents SDK, an open source python framework that takes a model-driven approach to building and running AI agents in just a few lines of code. Today, we’re announcing that TypeScript support is available in preview. Now, developers can choose between Python and TypeScript for building Strands Agents.\n  TypeScript support in Strands has been designed to provide an idiomatic TypeScript experience with full type safety, async/await support, and modern JavaScript/TypeScript patterns. Strands can be easily run in client applications, in browsers, and server-side applications in runtimes like AWS Lambda and Bedrock AgentCore. Developers can also build their entire stack in Typescript using the AWS CDK.\n  We’re also announcing three additional updates for the Strands SDK. First, edge device support for Strands Agents is generally available, extending the SDK with bidirectional streaming and additional local model providers like llama.cpp that let you run agents on small-scale devices using local models. Second, Strands steering is now available as an experimental feature, giving developers a modular prompting mechanism that provides feedback to the agent at the right moment in its lifecycle, steering agents toward a desired outcome without rigid workflows. Finally, Strands evaluations is available in preview. Evaluations gives developers the ability to systematically validate agent behavior, measure improvements, and deploy with confidence during development cycles.\n  Head to the Strands Agents GitHub to get started building.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/typescript-strands-agents-preview",
      "pubDate": "2025-12-03T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "agentcore",
        "lambda"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lambda",
        "preview",
        "experimental",
        "generally-available",
        "now-available",
        "update",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-fec165d010fd",
      "title": "Announcing Amazon EC2 General purpose M8azn instances (Preview)",
      "description": "Starting today, new general purpose high-frequency high-network Amazon Elastic Compute Cloud (Amazon EC2) M8azn instances are available for preview. These instances are powered by fifth generation AMD EPYC (formerly code named Turin) processors, offering the highest maximum CPU frequency, 5GHz in the cloud. The M8azn instances offer up to 2x compute performance versus previous generation M5zn instances. These instances also deliver 24% higher performance than M8a instances.\n  M8azn instances are built on the AWS Nitro System, a collection of hardware and software innovations designed by AWS. The AWS Nitro System enables the delivery of efficient, flexible, and secure cloud services with isolated multitenancy, private networking, and fast local storage. These instances are ideal for applications such as gaming, high-performance computing, high-frequency trading (HFT), CI/CD, and simulation modeling for the automotive, aerospace, energy, and telecommunication industries.\n  To learn more or request access to the M8azn instances preview, visit the Amazon EC2 M8a page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/aws-amazon-ec2-m8azn-preview",
      "pubDate": "2025-12-02T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "lex",
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "lex",
        "ec2",
        "preview",
        "ga"
      ]
    },
    {
      "id": "aws-news-0af7d0f0d0e9",
      "title": "Amazon FSx for NetApp ONTAP now supports Amazon S3 access",
      "description": "You can now attach Amazon S3 Access Points to your Amazon FSx for NetApp ONTAP file systems so that you can access your file data as if it were in S3. With this new capability, your file data in FSx for NetApp ONTAP is effortlessly accessible for use with the broad range of artificial intelligence, machine learning, and analytics services and applications that work with S3 while your file data continues to reside in your FSx for NetApp ONTAP file system.\n  Amazon FSx for NetApp ONTAP is the first and only complete, fully managed NetApp ONTAP file system in the cloud, allowing you to migrate on-premises applications that rely on NetApp ONTAP or other NAS appliances to AWS without having to change how you manage your data. An S3 Access Point is an endpoint that helps control and simplify how different applications or users can access data. Now, with S3 Access Points for FSx for NetApp ONTAP, you can discover new insights, innovate faster, and make even better data-driven decisions with the data you migrate to AWS. For example, you can use your data to augment generative AI applications with Amazon Bedrock, train machine learning models with Amazon SageMaker, run analysis using Amazon Glue or a wide range of AWS Data and Analytics Competency Partner solutions, and run workflows using S3-based cloud-native applications.\n  Get started with this capability by creating and attaching S3 Access Points to new FSx for NetApp ONTAP file systems using the Amazon FSx console, the AWS Command Line Interface (AWS CLI), or the AWS Software Development Kit (AWS SDK). Support for existing FSx for NetApp ONTAP file systems will come in an upcoming weekly maintenance window. This new capability is available in the select AWS Regions.\n  To get started, see the following list of resources:\n  \n \n \nAmazon FSx for NetApp ONTAP\n \n \nAmazon S3 Access Points\n \n \nAWS News Blog",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-fsx-netapp-ontap-s3-access",
      "pubDate": "2025-12-02T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "sagemaker",
        "s3",
        "glue"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "nova",
        "sagemaker",
        "s3",
        "glue",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-bfb723928032",
      "title": "Amazon EC2 P6e-GB300 UltraServers accelerated by NVIDIA GB300 NVL72 are now generally available",
      "description": "Today, AWS announces the general availability of Amazon Elastic Compute Cloud (Amazon EC2) P6e-GB300 UltraServers. P6e-GB300 UltraServers, accelerated by NVIDIA GB300 NVL72, provide 1.5x GPU memory and 1.5x FP4 compute (without sparsity) compared to P6e-GB200. \n \nCustomers can optimize performance for the most powerful models in production with P6e-GB300 for applications that require higher context and implement emerging inference techniques like reasoning and Agentic AI.\n \nTo get started with P6e-GB300 UltraServers, please contact your AWS sales representative.\n \nTo learn more about P6e UltraServers and instances, visit Amazon EC2 P6 instances.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-ec2-p6e-gb300-ultraservers-nvidia-gb300-nvl72-generally-available",
      "pubDate": "2025-12-02T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "generally-available"
      ]
    },
    {
      "id": "aws-news-5ca455cce367",
      "title": "Announcing new memory-optimized Amazon EC2 X8aedz Instances",
      "description": "AWS announces Amazon EC2 X8aedz, next generation memory optimized instances, powered by 5th Gen AMD EPYC processors (formerly code named Turin). These instances offer the highest maximum CPU frequency, 5GHz in the cloud. They deliver up to 2x higher compute performance compared to previous generation X2iezn instances.\n  X8aedz instances are built using the latest sixth generation AWS Nitro Cards and are ideal for electronic design automation (EDA) workloads such as physical layout and physical verification jobs, and relational databases that benefit from high single-threaded processor performance and a large memory footprint. The combination of 5 GHz processors and local NVMe storage enables faster processing of memory-intensive backend EDA workloads such as floor planning, logic placement, clock tree synthesis (CTS), routing, and power/signal integrity analysis.\n  X8aedz instances feature a 32:1 ratio of memory to vCPU and are available in 8 sizes ranging from 2 to 96 vCPUs with 64 to 3,072 GiB of memory, including two bare metal variants, and up to 8 TB of local NVMe SSD storage.\n  X8aedz instances are now available in US West (Oregon) and Asia Pacific (Tokyo) regions. Customers can purchase X8aedz instances via Savings Plans, On-Demand instances, and Spot instances. To get started, sign in to the AWS Management Console. For more information visit the Amazon EC2 X8aedz instance page or AWS news blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/memory-optimized-amazon-ec2-x8aedz-instances/",
      "pubDate": "2025-12-02T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "rds",
        "now-available"
      ]
    },
    {
      "id": "aws-news-72357d1afc62",
      "title": "Amazon S3 increases the maximum object size to 50 TB",
      "description": "Amazon S3 increased the maximum object size to 50 TB, a 10x increase from the previous 5 TB limit. This simplifies the processing of large objects such as high-resolution videos, seismic data files, AI training datasets and more. You can store 50 TB objects in all S3 storage classes and use them with all S3 features.\n  Optimize upload and download performance for your large objects by using the latest AWS Common Runtime (CRT) and S3 Transfer Manager in the AWS SDK. You can apply S3's storage management capabilities to these objects. For example, use S3 Lifecycle to automatically archive infrequently accessed objects to S3 Glacier storage classes, or use S3 Replication to copy objects across AWS accounts or Regions.\n  Amazon S3 supports objects up to 50 TB in all AWS Regions. To learn more about working with large objects, visit the S3 User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-s3-maximum-object-size-50-tb/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "support"
      ]
    },
    {
      "id": "aws-news-4709d37d5fa2",
      "title": "Amazon S3 Batch Operations introduces performance improvements",
      "description": "Amazon S3 Batch Operations now completes jobs up to 10x faster at a scale of up to 20 billion objects in a job, helping you accelerate large-scale storage operations.\n  With S3 Batch Operations, you can perform operations at scale such as copying objects between staging and production buckets, tagging objects for S3 Lifecycle management, or computing object checksums to verify the content of stored datasets. S3 Batch Operations now pre-processes objects, executes jobs, and generates completion reports up to 10x faster for jobs processing millions of objects with no additional configuration or cost. To get started, create a job in the AWS Management Console and specify operation type as well as filters like bucket, prefix, or creation date. S3 automatically generates the object list, creates an AWS Identity and Access Management (IAM) role with permission policies as needed, then initiates the job.\n  S3 Batch Operations performance improvements are available in all AWS Regions, except for AWS China Regions and AWS GovCloud (US) Regions. For pricing information, please visit the Management & Insights tab of the Amazon S3 pricing page. To learn more about S3 Batch Operations, visit the overview page and documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/s3-batch-operations-performance-improvements/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "s3",
        "iam"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "iam",
        "improvement"
      ]
    },
    {
      "id": "aws-news-001f362a6859",
      "title": "Amazon RDS for Oracle and SQL Server now support up to 256 TiB storage with additional storage volumes",
      "description": "Amazon Relational Database Service (Amazon RDS) for Oracle and SQL Server now support up to 256 TiB storage size, a 4x increase in storage size per database instance. Customers can add up to three additional storage volumes in addition to the primary storage volume, each up to 64 TiB storage, to their database instance. Additional storage volumes can be added, scaled up, or removed from the database instance without application downtime, so customers have the flexibility to add and adjust storage volumes over time based on changing workload requirements.\n  With additional storage volumes, customers can continue to scale database storage beyond the maximum storage size available in the primary volume. Also, customers can temporarily add volumes when they have a short-term requirement for additional storage, such as for month-end data processing or importing data from local storage, and remove unused volumes when they are no longer required. Furthermore, customers can optimize cost performance by using a combination of high-performance Provisioned IOPS SSD (io2) volumes and General Purpose (gp3) volumes for their database instance. For example, data that requires consistent IOPS performance can be stored on an io2 volume, and infrequently accessed historical data can be stored on a gp3 volume to optimize storage cost.\n  To get started, customers can create additional storage volumes in a new or existing database instance through the AWS Management Console, AWS CLI, or SDKs. For more information, visit the RDS for Oracle User Guide and RDS for SQL Server User Guide. To learn more about how customers can benefit from additional storage volumes, visit the AWS news blog post. Additional storage volumes are available in all commercial AWS Regions and the AWS GovCloud (US) Regions.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/rds-oracle-sql-server-256-tib-storage-support/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "rds",
        "support"
      ]
    },
    {
      "id": "aws-news-81efd460d076",
      "title": "Amazon Bedrock AgentCore now includes Policy (preview), Evaluations (preview) and more",
      "description": "Today, Amazon Bedrock AgentCore introduces new offerings, including Policy (preview) and Evaluations (preview), to give teams the controls and quality assurance they need to confidently scale agent deployment across their organization, transforming agents from prototypes to solutions in production.\n  Policy in AgentCore integrates with AgentCore Gateway to intercept every tool call in real time, ensuring agents stay within defined boundaries without slowing down. Teams can create policies using natural language that automatically convert to Cedar—the AWS open-source policy language—helping development, compliance, and security teams set up, understand, and audit rules without writing custom code. AgentCore Evaluations helps developers test and continuously monitor agent performance based on real-world behavior to improve quality and catch issues before they cause widespread customer impact. Developers can use 13 built-in evaluators for common quality dimensions, such as helpfulness, tools selection, and accuracy, or create custom model-based scoring systems, drastically reducing the effort required to develop evaluation infrastructure. All quality metrics are accessible through a unified dashboard powered by Amazon CloudWatch. We’ve also added new features to AgentCore Memory, AgentCore Runtime, and AgentCore Identity to support more advanced agent capabilities. AgentCore Memory now includes episodic memory, enabling agents to learn and adapt from experiences, building knowledge over time to create more humanlike interactions. AgentCore Runtime supports bidirectional streaming for natural conversations where agents simultaneously listen and respond while handling interruptions and context changes mid-conversation, unlocking powerful voice agent use cases. AgentCore Identity now supports custom claims for enhanced authentication rules across multi-tenant environments while maintaining seamless integration with your chosen identity providers.\n  AgentCore Evaluations is available in preview in four AWS Regions: US East (N. Virginia), US West (Oregon), Asia Pacific (Sydney), Europe (Frankfurt). Policy in AgentCore is available in preview in all AWS Regions where AgentCore is available.\n  Learn more about new AgentCore updates through the blog, deep dive using AgentCore resources, and get started with the AgentCore Starter Toolkit. AgentCore offers consumption-based pricing with no upfront costs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-bedrock-agentcore-policy-evaluations-preview",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "agentcore",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "cloudwatch",
        "preview",
        "ga",
        "new-feature",
        "update",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-e37cf2716a76",
      "title": "Amazon CloudWatch launches unified management and analytics for operational, security, and compliance data",
      "description": "Amazon CloudWatch now provides new data management and analytics capabilities that allow you to unify operational, security, and compliance data across your AWS environment and third-party sources. DevOps teams, security analysts, and compliance officers can now access all their data in a single place, eliminating the need to maintain multiple separate data stores and complex (extract-transform-load) ETL pipelines. CloudWatch now offers greater flexibility in where and how customers gain insights into this data, both natively in CloudWatch or with any Apache Iceberg-compatible tool.\n  With the unified data store enhancements, customers can now easily collect and aggregate logs across AWS accounts and regions aligned to geographic boundaries, business units, or persona-specific requirements. With AWS Organization-wide enablement for AWS sources such as AWS CloudTrail, Amazon VPC, and Amazon WAF, and managed collectors for third party sources such as Crowdstrike, Okta, Palo Alto Networks, CloudWatch makes it easy to bring more of your logs together. Customers can use pipelines to transform and enrich their logs to standard formats such as Open Cybersecurity Schema Framework (OCSF) for security analytics, and define facets to accelerate insights on their data. Customers can make their data available in managed Amazon S3 Tables at no additional storage charge, enabling teams to query data in Amazon SageMaker Unified Studio, Amazon Quick Suite, Amazon Athena, Amazon Redshift, or any Apache Iceberg-compatible analytics tool.\n  To get started, visit the Ingestion page in the CloudWatch console and add one or more data sources. To learn more about Amazon CloudWatch unified data store, visit the product page, pricing page, and documentation. For Regional availability, visit the AWS Builder Center.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-cloudwatch-unified-management-analytics",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q",
        "sagemaker",
        "unified studio",
        "lex",
        "s3",
        "redshift",
        "athena",
        "cloudwatch",
        "waf"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "sagemaker",
        "unified studio",
        "lex",
        "s3",
        "redshift",
        "athena",
        "cloudwatch",
        "waf",
        "launch",
        "ga",
        "enhancement"
      ]
    },
    {
      "id": "aws-news-ef6225c704a6",
      "title": "Amazon EMR Serverless eliminates local storage provisioning for Apache Spark workloads",
      "description": "Amazon EMR Serverless now offers serverless storage that eliminates local storage provisioning for Apache Spark workloads, reducing data processing costs by up to 20% and preventing job failures from disk capacity constraints. You no longer need to configure local disk type and size for each application. EMR Serverless automatically handles intermediate data operation such as shuffle with no local storage charges. You pay only for compute and memory resources your job consumes.\n  EMR Serverless offloads intermediate data operations to a fully managed, auto-scaling serverless storage that encrypts data in transit and at rest with job-level isolation. Serverless storage decouples storage from compute, allowing Spark to release workers immediately when idle rather than keeping workers active to preserve temporary data. It eliminates job failures from insufficient disk capacity and reduces costs by avoiding idle worker charges. This is particularly valuable for jobs using dynamic resource allocation, such as recommendation engines processing millions of customer interactions, where initial stages process large datasets with high parallelism then narrow as data aggregates.\n  This feature is generally available for EMR release 7.12 and later. See Supported AWS Regions for availability. To get started, visit serverless storage for EMR Serverless documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-emr-serverless-local-storage-provisioning-apache-spark-workloads",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "emr"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "emr",
        "generally-available",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-40c7988b6612",
      "title": "Amazon GuardDuty Extended Threat Detection now supports Amazon EC2 and Amazon ECS",
      "description": "AWS announces further enhancements to Amazon GuardDuty Extended Threat Detection with new capabilities to detect multistage attacks targeting Amazon Elastic Compute Cloud (Amazon EC2) instances and Amazon Elastic Container Service (Amazon ECS) clusters running on AWS Fargate or Amazon EC2. GuardDuty Extended Threat Detection uses artificial intelligence and machine learning algorithms trained at AWS scale to automatically correlate security signals and detect critical threats. It analyzes multiple security signals across network activity, process runtime behavior, malware execution, and AWS API activity over extended periods to detect sophisticated attack patterns that might otherwise go unnoticed.\n  With this launch, GuardDuty introduces two new critical-severity findings: AttackSequence:EC2/CompromisedInstanceGroup and AttackSequence:ECS/CompromisedCluster. These findings provide attack sequence information, allowing you to spend less time on initial analysis and more time responding to critical threats, minimizing business impact. For example, GuardDuty can identify suspicious processes followed by persistence attempts, crypto-mining activities, and reverse shell creation, representing these related events as a single, critical-severity finding. Each finding includes a detailed summary, events timeline, mapping to MITRE ATT&CK® tactics and techniques, and remediation recommendations.\n  While GuardDuty Extended Threat Detection is automatically enabled for GuardDuty customers at no additional cost, its detection comprehensiveness depends on your enabled GuardDuty protection plans. To improve attack sequence coverage and threat analysis of Amazon EC2 instances, enable Runtime Monitoring for EC2. To enable detection of compromised ECS clusters, enable Runtime Monitoring for Fargate or EC2 depending on your infrastructure type.\n  To get started, enable GuardDuty protection plans via the Console or API. New GuardDuty customers can start with a 30-day free trial, and existing customers who haven't used Runtime Monitoring can also try it free for 30 days. For additional information, visit the blog post and Amazon Guard Duty product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/guardduty-extended-threat-detection-ec2-ecs/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2",
        "ecs",
        "fargate"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "ecs",
        "fargate",
        "launch",
        "ga",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-0a2fabefed90",
      "title": "Announcing Amazon EC2 Trn3 UltraServers for faster, lower-cost generative AI training",
      "description": "AWS announces the general availability of Amazon Elastic Compute Cloud (Amazon EC2) Trn3 UltraServers powered by our fourth–generation AI chip Trainium3, our first 3nm AWS AI chip purpose-built to deliver the best token economics for next-generation agentic, reasoning, and video generation applications.\n  Each AWS Trainium3 chip provides 2.52 petaflops (PFLOPs) of FP8 compute, increases the memory capacity by 1.5x and bandwidth by 1.7x over Trainium2 to 144 GB of HBM3e memory, and 4.9 TB/s of memory bandwidth. Trainium3 is designed for both dense and expert-parallel workloads with advanced data types (MXFP8 and MXFP4) and improved memory-to-compute balance for real-time, multimodal, and reasoning tasks.\n  Trn3 UltraServers can scale up to 144 Trainium3 chips (362 FP8 PFLOPs total) and are available in EC2 UltraClusters 3.0 to scale to hundreds of thousands of chips. A fully configured Trn3 UltraServer delivers up to 20.7 TB of HBM3e and 706 TB/s of aggregate memory bandwidth. The next-generation Trn3 UltraServer, feature the NeuronSwitch-v1, an all-to-all fabric that doubles interchip interconnect bandwidth over Trn2 UltraServer.\n  Trn3 delivers up to 4.4x higher performance, 3.9x higher memory bandwidth and 4x better performance/watt compared to our Trn2 UltraServers, providing the best price-performance for training and serving frontier-scale models, including reinforcement learning, Mixture-of-Experts (MoE), reasoning, and long-context architectures. On Amazon Bedrock, Trainium3 is our fastest accelerator, delivering up to 3× faster performance than Trainium2 with over 5× higher output tokens per megawatt at similar latency per user.\n  New Trn3 UltraServers are built for AI researchers and powered by the AWS Neuron SDK, to unlock breakthrough performance. With native PyTorch integration, developers can train and deploy without changing a single line of model code. For AI performance engineers, we’ve enabled deeper access to Trainium3 so they can fine-tune performance, customize kernels, and push models even further. Because innovation thrives on openness, we are committed to engaging with our developers through open-source tools and resources.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-ec2-trn3-ultraservers",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "trainium",
        "trainium3",
        "neuron",
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "nova",
        "trainium",
        "trainium3",
        "neuron",
        "ec2",
        "ga",
        "integration"
      ]
    },
    {
      "id": "aws-news-9760f84bbfd6",
      "title": "AWS Support transformation: AI-powered operations with the human expertise you trust",
      "description": "AWS Support announces a transformation of its Support portfolio, simplified into three intelligent, experience-driven plans: Business Support+, Enterprise Support, and Unified Operations. Each plan combines the speed and precision of AI with the expertise of AWS engineers. Each higher plan builds on the previous one, adding faster response times, proactive guidance, and smarter operations. The result: reduced engineering burden, stronger reliability and resiliency, and streamlined cloud operations.\n  Business Support+ delivers 24/7 AI-powered assistance that understands your context, with direct engagement to AWS experts for critical issues within 30 minutes—twice as fast as current plans. Enterprise Support expands on this with designated Technical Account Managers (TAMs) who blend generative AI insights with human judgment to provide strategic operational guidance across resiliency, cost, and efficiency. It also includes AWS Security Incident Response at no additional cost, which customers can activate to automate security alert investigation and triage. Unified Operations, the top plan, is designed for mission-critical workloads—offering a global team of designated experts who deliver architecture reviews, guided testing, proactive optimization, and five-minute context-specific response times for critical incidents. Customers using AWS DevOps Agent (preview) can engage with AWS Support with one-click from an investigation when needed, giving AWS experts immediate context for faster resolution. AWS DevOps Agent is a frontier agent that resolves and proactively prevents incidents, continuously improving reliability and performance of applications in AWS, multicloud, and hybrid environments.\n  Business Support+, Enterprise Support, and Unified Operations are available in all commercial AWS Regions. Existing customers can continue with their current plans or explore the new offerings for enhanced performance and efficiency. To see how AWS blends AI intelligence and human expertise to transform your cloud operations, visit the AWS Support product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/aws-support-transformation-ai-powered-operations",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "preview",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-652da7a2225a",
      "title": "Announcing Amazon EC2 M4 Max Mac instances (Preview)",
      "description": "Amazon Web Services announces preview of Amazon EC2 M4 Max Mac instances, powered by the latest Mac Studio hardware. Amazon EC2 M4 Max Mac instances are the next-generation EC2 Mac instances, that enable Apple developers to migrate their most demanding build and test workloads onto AWS. These instances are ideal for building and testing applications for Apple platforms such as iOS, macOS, iPadOS, tvOS, watchOS, visionOS, and Safari.\n  M4 Max Mac instances are powered by the AWS Nitro System, providing up to 10 Gbps network bandwidth and 8 Gbps of Amazon Elastic Block Store (Amazon EBS) storage bandwidth. These instances are built on Apple M4 Max Mac Studio computers featuring a 16-core CPU, 40-core GPU, 16-core Neural Engine, and 128GB of unified memory. Compared to EC2 M4 Pro Mac instances, M4 Max instances offer twice the GPU cores and more than 2.5x the unified memory, offering customers more choice to match instance capabilities to their specific workload requirements and further expanding the selection of Apple silicon Mac hardware on AWS.\n \nTo learn more or request access to the Amazon EC2 M4 Max Mac instances preview, visit the Amazon EC2 Mac page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-ec2-m4-max-mac-instances-preview",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "preview"
      ]
    },
    {
      "id": "aws-news-f36ad3c2df49",
      "title": "Amazon SageMaker Catalog now exports asset metadata as queryable dataset",
      "description": "Amazon SageMaker Catalog now exports asset metadata as an Apache Iceberg table through Amazon S3 Tables. This allows data teams to query catalog inventory and answer questions such as, \"How many assets were registered last month?\", \"Which assets are classified as confidential?\", or \"Which assets lack business descriptions?\" using standard SQL without building custom ETL infrastructure for reporting.\n \nThis capability automatically converts catalog asset metadata into a queryable table accessible from Amazon Athena, SageMaker Unified Studio notebooks, AI agents, and other analytics and BI tools. The exported table includes technical metadata (such as resource_id, resource_type), business metadata (such as asset_name, business_description), ownership details, and timestamps. Data is partitioned by snapshot_date for time travel queries and automatically appears in SageMaker Unified Studio under the aws-sagemaker-catalog bucket.\n \nThis capability is available in all AWS Regions where SageMaker Catalog is supported at no additional charge. You pay only for underlying services including S3 Tables storage and Amazon Athena queries. You can control storage costs by setting retention policies on the exported tables to automatically remove records older than your specified period.\n  To get started, activate dataset export using the AWS CLI, then access the asset table through S3 Tables or SageMaker Unified Studio's Data tab within 24 hours. Query using Amazon Athena, Studio notebooks, or connect external BI tools through the S3 Tables Iceberg REST Catalog endpoint. For instructions, see the Amazon SageMaker user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/sagemaker-catalog-asset-metadata-queryable-dataset/",
      "pubDate": "2025-12-02T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "unified studio",
        "s3",
        "rds",
        "athena"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "unified studio",
        "s3",
        "rds",
        "athena",
        "support"
      ]
    },
    {
      "id": "aws-news-c5df555a7a43",
      "title": "Announcing new memory optimized Amazon EC2 X8aedz instances",
      "description": "AWS announces Amazon EC2 X8aedz, next generation memory optimized instances, powered by 5th Gen AMD EPYC processors (formerly code named Turin). These instances offer the highest maximum CPU frequency, 5GHz in the cloud. They deliver up to 2x higher compute performance compared to previous generation X2iezn instances.\n  X8aedz instances are built using the latest sixth generation AWS Nitro Cards and are ideal for electronic design automation (EDA) workloads such as physical layout and physical verification jobs, and relational databases that benefit from high single-threaded processor performance and a large memory footprint. The combination of 5 GHz processors and local NVMe storage enables faster processing of memory-intensive backend EDA workloads such as floor planning, logic placement, clock tree synthesis (CTS), routing, and power/signal integrity analysis.\n  X8aedz instances feature a 32:1 ratio of memory to vCPU and are available in 8 sizes ranging from 2 to 96 vCPUs with 64 to 3,072 GiB of memory, including two bare metal variants, and up to 8 TB of local NVMe SSD storage.\n  X8aedz instances are now available in US West (Oregon) and Asia Pacific (Tokyo) regions. Customers can purchase X8aedz instances via Savings Plans, On-Demand instances, and Spot instances. To get started, sign in to the AWS Management Console. For more information visit the Amazon EC2 X8aedz instance page or AWS news blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/new-memory-optimized-amazon-ec2-x8aedz-instances",
      "pubDate": "2025-12-02T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "rds",
        "now-available"
      ]
    },
    {
      "id": "aws-news-689229736e75",
      "title": "Amazon Bedrock AgentCore Runtime now supports bi-directional streaming",
      "description": "Amazon Bedrock AgentCore Runtime now supports bi-directional streaming, enabling real-time conversations where agents listen and respond simultaneously while handling interruptions and context changes mid-conversation. This feature eliminates conversational friction by enabling continuous, two-way communication where context is preserved throughout the interaction.\n  Traditional agents require users to wait for them to finish responding before providing clarification or corrections, creating stop-start interactions that break conversational flow and feel unnatural, especially in voice applications. Bi-directional streaming addresses this limitation by enabling continuous context handling, helping power voice agents that deliver natural conversational experiences where users can interrupt, clarify, or change direction mid-conversation, while also enhancing text-based interactions through improved responsiveness. Built into AgentCore Runtime, this feature eliminates months of engineering effort required to build real-time streaming capabilities, so developers can focus on building innovative agent experiences rather than managing complex streaming infrastructure.\n  This feature is available in all nine AWS Regions where Amazon Bedrock AgentCore Runtime is available: US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), and Europe (Ireland).\n  To learn more about AgentCore Runtime bi-directional streaming, read the blog, visit the AgentCore documentation and get started with the AgentCore Starter Toolkit. With AgentCore Runtime's consumption-based pricing, you only pay for active resources consumed during agent execution, with no charges for idle time or upfront costs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/bedrock-agentcore-runtime-bi-directional-streaming/",
      "pubDate": "2025-12-02T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "agentcore",
        "nova",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "nova",
        "lex",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-c4e599974567",
      "title": "Amazon S3 Tables now offer the Intelligent-Tiering storage class",
      "description": "Amazon S3 Tables now offer the Intelligent-Tiering storage class, which optimizes costs based on access patterns, without performance impact or operational overhead. Intelligent-Tiering automatically transitions data in tables across three low-latency access tiers as access patterns change, reducing storage costs by up to 80%. Additionally, S3 Tables automated maintenance operations such as compaction, snapshot expiration, and unreferenced file removal never tier up your data. This helps you to keep your tables optimized while saving on storage costs.\n  With the Intelligent-Tiering storage class, data in tables not accessed for 30 consecutive days automatically transitions to the Infrequent Access tier (40% lower cost than the Frequent Access tier). After 90 days without access, that data transitions to the Archive Instant Access tier (68% lower cost than the Infrequent Access tier). You can now select Intelligent-Tiering as the storage class when you create a table or set it as the default for all new tables in a table bucket.\n  The Intelligent-Tiering storage class is available in all AWS Regions where S3 Tables are available. For pricing details, visit the Amazon S3 pricing page. To learn more about S3 Tables, visit the product page, documentation, and read the AWS News Blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/s3-tables-intelligent-tiering-storage-class/",
      "pubDate": "2025-12-02T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3"
      ]
    },
    {
      "id": "aws-news-3bd2560befb0",
      "title": "AWS Transform adds new agentic AI capabilities for enterprise VMware migrations",
      "description": "AWS Transform adds powerful new agentic AI capabilities to automate VMware migrations to AWS. The migration agent collaborates with migration teams to understand business priorities and intelligently plan and migrate hundreds of applications spanning thousands of servers, significantly reducing manual effort, time, and complexity.\n  The agent can now discover your on-premises environment and prioritize applications for migration using the AWS Transform discovery tool, inventory data from various third-party discovery tools, and unstructured data such as documents, notes, and business rules. It analyzes infrastructure, database, and application details, maps dependencies, and generates migration plans grouped by business and technical priorities such as ownership, department, function, subnet, and operating systems. It generates networks with hub-and-spoke and isolated network configurations, provides flexible IP address management options, deploys to multiple accounts, generates network configurations for your AWS landing zones, and migrates from source environments like NSX, Palo Alto, Fortigate, and Cisco ACI. The agent migrates servers to AWS securely and iteratively in waves and provides clear progress updates throughout the deployment. It also migrates Windows and Linux x86 servers, hypervisors such as VMware, HyperV, Nutanix, and KVM, and bare-metal physical environments to multiple target accounts. Throughout your migration, you can ask the agent questions as it guides your decisions, whether that’s repeating or skipping steps, or adjusting plans. To simplify internal approvals, the agent also generates a detailed report with the migration plan and mapping of networks, servers, and applications.\n  With AWS Transform, you can accelerate time to value, lower risk, and reduce the complexity of VMware migrations. These new capabilities are available in all AWS Regions where AWS Transform is offered, with support for migrating servers and networks to 16 AWS Regions.\n  Learn more on the product page and user guide, and get started with AWS Transform.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/transform-vmware-agentic-ai-enterprise-migration/",
      "pubDate": "2025-12-01T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "ga",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-fbcf70ff0cc1",
      "title": "AWS Transform expands .NET transformation capabilities and enhances developer experience",
      "description": "Today, AWS announces the general availability of expanded .NET transformation capabilities and an enhanced developer experience in AWS Transform. Customers can now modernize .NET Framework and .NET code to .NET 10 or .NET Standard. New transformation capabilities include UI porting of ASP.NET Web Forms to Blazor on ASP.NET Core and porting Entity Framework ORM code. The new developer experience, available with the AWS Toolkit for Visual Studio 2026 or 2022, is customizable, interactive, and iterative. It includes an editable transformation plan, estimated transformation time, real-time updates during transformation, the ability to repeat transformations with a revised plan, and next steps markdown for easy handoff to AI code companions. With these enhancements, AWS Transform provides a path to modern .NET for more project types, supports the latest releases of .NET and Visual Studio, and gives developers oversight and control of transformations.\n \nDevelopers can now streamline their .NET modernization through an enhanced IDE experience. The process begins with automated code analysis that produces a customizable transformation plan. Developers can customize the transformation plan, such as fine-tuning package updates. Throughout the transformation, they benefit from transparent progress tracking and detailed activity logs. Upon completion, developers receive a Next Steps document that outlines remaining tasks, including Linux readiness requirements, which they can address through additional AWS Transform iterations or by leveraging AI code companion tools such as Kiro.\n \nAWS Transform is available in the following AWS Regions: US East (N. Virginia), Asia Pacific (Mumbai), Asia Pacific (Seoul), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), Europe (Frankfurt), and Europe (London).\n \nTo get started with AWS Transform, refer to the AWS Transform documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/transform-net-transformation-developer-experience/",
      "pubDate": "2025-12-01T08:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "update",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-f196750d89bd",
      "title": "Amazon Connect launches Model Context Protocol (MCP) support",
      "description": "Amazon Connect now supports Model Context Protocol (MCP), enabling AI agents for end-customer self-service and employee assistance to use standardized tools for retrieving information and completing actions. With this launch, businesses can enhance their AI agents with extensible tool capabilities that improve issue resolution. For example, an AI agent can automatically look up order status, process refunds, and update customer records during a self-service interaction without requiring human intervention.\n  With this launch, Amazon Connect provides out-of-the-box MCP tools for common tasks such as updating contact attributes and retrieving case information. You can also use flow modules as MCP tools to reuse the same business logic across both deterministic and generative AI workflows. Additionally, you can integrate custom tools or third-party services through flow modules or the Amazon Bedrock AgentCore Gateway.\n  For region availability, please see the availability of Amazon Connect features by Region. To learn more about Connect’s AI agents please visit the website or see the help documentation. To learn more about Amazon Connect, the AWS cloud-based contact center, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-mcp-support",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "agentcore",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "rds",
        "launch",
        "ga",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-13754dd52a44",
      "title": "Amazon Connect now supports AI agent assistance and summarization for Agentforce Service",
      "description": "Amazon Connect launches real-time AI agent assistance and contact summarization for Salesforce Contact Center with Amazon Connect (SCC-AC). It enables Connect AI agents to automatically leverage customer information and knowledge base articles from Salesforce CRM for accelerated issue resolution and consistent outcomes across voice and chat interactions.\n  When human intervention is required, the seamless integration within SCC-AC connects customers to agents who have a unified view of customer data, issue context, and interaction history within Agentforce Service and Agentforce Sales. Agents receive real-time voice transcripts and contextual recommendations, while supervisors gain enhanced call monitoring capabilities directly in Salesforce. Upon resolution, automated post-contact summarization enables agents to easily update Salesforce cases, streamlining administrative tasks. Administrators can deploy and configure this integrated contact center solution in minutes, leveraging Amazon Connect's voice, digital channels, and intelligent routing capabilities.\n  This feature is available in all AWS Regions where Amazon Connect is available. To learn more and get started, see the Salesforce Contact Center with Amazon Connect documentation. To learn more about Amazon Connect, see Amazon Connect and our strategic Salesforce partnership",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-ai-agent-assistance-summarization-agentforce-service",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "ga",
        "update",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-3aca50927ed4",
      "title": "Amazon Connect now provides improved analytics and monitoring for AI agents",
      "description": "Amazon Connect now provides analytics and monitoring capabilities for AI agents across self-service and agent assistance experiences. With this launch, you can measure and continuously improve AI agent performance and customer outcomes through easy to customize dashboards that provide key metrics like number of AI agent led interactions, hand-off rates, conversation turns, and average handle time. You can also compare AI agent performance across versions to identify optimal configurations and review insights to understand where AI agents are performing well and where improvements are needed. Additionally, with this launch, you can configure rules to trigger automated actions, such as sending alerts when self-service contacts are transferred to human agents with low sentiment scores. Amazon Connect also provides AI agent traces via APIs with detailed information such as request and response payloads and tool invocations, enabling you to easily understand AI agent actions and decision-making for faster troubleshooting.\n  This capabilities is available in all AWS Regions where Amazon Connect AI agents are offered. To learn more about AI agent analytics, see the Amazon Connect Administrator Guide. To learn more about Amazon Connect, the AWS contact center as a service solution on the cloud, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-improved-analytics-monitoring-ai-agents",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "rds",
        "launch",
        "improvement"
      ]
    },
    {
      "id": "aws-news-40738846eef5",
      "title": "Amazon Connect launches AI-powered predictive insights (Preview)",
      "description": "Today, Amazon Connect is launching AI-powered predictive insights that transform how businesses understand and serve their customers. This new feature set builds upon Connect's existing customer profiles, introducing five recommendation algorithms that leverage AI to analyze customer behavior patterns and interaction history. These AI-powered insights are available for both self-service and agent interactions, enabling businesses to transform all customer touchpoints – from suggesting complementary products during service calls to providing smart product discovery through intelligent chat experiences by leveraging their existing customer data within Connect Customer Profiles. Businesses can also leverage these AI-powered insights to build their Connect AI agent for specialized for sales.\n  The five recommendation algorithms are as follows: \"Recommended for You\" provides tailored suggestions based on individual user interactions patterns with any catalog; \"Similar Items\" uses generative AI to suggest alternative products or services; \"Frequently Paired Items\" powers cross-selling by identifying complementary product or service combinations, \"Popular Items\" surfaces top-performing product recommendations, and \"Trending Now\" captures real-time customer interest for timely engagement.\n  With Amazon Connect Customer Profiles, you only pay-as-you-go for utilized profiles. Public preview for AI-powered predictive insights is available in Europe (Frankfurt), US East (N. Virginia), Asia Pacific (Seoul), Asia Pacific (Tokyo), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central).\n  To learn more, visit our webpages for Customer Profiles.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-ai-powered-predictive-insights-preview",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "preview",
        "ga",
        "new-feature",
        "public-preview"
      ]
    },
    {
      "id": "aws-news-396183336a16",
      "title": "Amazon Connect enhances its agent assistance capabilities",
      "description": "Amazon Connect now provides customer service representatives with new AI agents that guide them through customer interactions by recommending actions, retrieving information, and executing tasks on their behalf. For example, an AI agent can guide a representative through processing a product return by automatically pulling order history, calculating refund amounts, and initiating the return process. These AI agents analyze conversation context and customer sentiment in real-time, actively completing tasks such as preparing documentation and handling routine processes. This enables representatives to focus on building customer relationships and handling complex situations while AI manages the background work, enhancing productivity and ensuring consistent outcomes. You can get started with out-of-the-box agents provided by Amazon Connect or easily customize AI agent behavior and actions to align with your business needs.\n  To learn more about Amazon Connect AI agents, please visit the website or see the help documentation. For region availability, please see the availability of Amazon Connect features by Region. To learn more about Amazon Connect, the AWS cloud-based contact center, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-enhances-agent-assistance-capabilities",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex"
      ]
    },
    {
      "id": "aws-news-dbcd91a33051",
      "title": "Amazon Connect now supports multiple knowledge bases and integrates with your Amazon Bedrock Knowledge Bases",
      "description": "Amazon Connect now allows you to bring your own Amazon Bedrock Knowledge Bases and supports multiple knowledge bases per AI agent, giving you greater flexibility in how you organize and access knowledge content for your AI agents. You can now connect your existing Bedrock Knowledge Bases directly to Amazon Connect AI agents in just a few clicks, with no additional setup or data duplication required. This allows you to leverage your current data sources and the Amazon Bedrock Knowledge Base connectors, including Adobe Experience Manager, Confluence, SharePoint, and OneDrive, giving you flexibility to use existing content repositories.\n  With support for multiple knowledge bases per AI agent, you can configure AI agents to query multiple sources in parallel for more comprehensive responses. For example, a financial services company can easily connect separate knowledge bases for compliance documentation, product information, and internal policies, enabling AI agents to provide complete guidance across all relevant content during customer interactions.\n  This feature is available in all AWS Regions where Amazon Connect AI agents and Amazon Bedrock Knowledge Bases are offered. To learn more about these features, see the Amazon Connect Administrator Guide. To learn more about Amazon Connect, the AWS cloud-based contact center, and Amazon Connect AI agents please visit the Amazon Connect Website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-multiple-knowledge-bases-integrates-amazon-bedrock-knowledge-bases",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "lex",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-2bf695171597",
      "title": "Amazon Connect now supports creation of custom metrics for use in dashboards and APIs",
      "description": "Amazon Connect now supports creation of custom metrics, enabling contact center supervisors to analyze tailored performance measurements without requiring technical skills. This feature provides a simple, no-code interface for performing mathematical operations (e.g., addition, subtraction, sum, average) on existing Connect data to build metrics that align with your organization's specific business requirements. Custom metrics are available to use in the dashboards and APIs.\n  With custom metrics, you can track performance in ways that matter most to your business. For example, create average handle time metrics for premium versus standard customer segments, calculate total agent time on outbound calls by product line, or measure queue performance filtered by contact type such as callbacks versus incoming calls.\n This new feature is available in all AWS regions where Amazon Connect is offered. To learn more about Amazon Connect custom metrics, see the Administrator Guide. To learn more about Amazon Connect, see the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-metric-customization/",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "rds",
        "ga",
        "new-feature",
        "support"
      ]
    },
    {
      "id": "aws-news-1b32fbad45c0",
      "title": "Amazon Connect now streams messages for AI-powered interactions",
      "description": "Amazon Connect now supports message streaming for AI-powered chat interactions. This new capability shows Connect AI agent responses as they're being generated, which reduces perceived wait times and improves the customer experience.\n  When using Amazon Connect AI agents, customers see status updates like \"One moment while I review your account\" during processing, and watch responses appear progressively. This experience gives customers confidence their request is actively being worked on while AI agents reason, invoke tools, and craft comprehensive solutions.\n  Message streaming for AI-powered interactions is now available in the following regions: US East (N. Virginia), US West (Oregon), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), Europe (Frankfurt), Europe (London) and Africa (Cape Town). To learn more, visit the Amazon Connect documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-connect-streams-messages-ai-powered-interactions",
      "pubDate": "2025-11-30T18:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "now-available",
        "update",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-81bf5f175758",
      "title": "Announcing AWS Lambda Managed Instances, a capability to run functions on your Amazon EC2 instances",
      "description": "AWS Lambda Managed Instances lets you run Lambda functions on your Amazon EC2 instances while maintaining Lambda's operational simplicity. With Lambda Managed Instances, you can access specialized compute configurations and drive cost efficiency through EC2 pricing advantages, without managing infrastructure.\n  Lambda Managed Instances fully manages all infrastructure tasks, including instance lifecycle, OS and runtime patching, built-in routing, load balancing, and auto-scaling based on configurable parameters - so you can focus on writing code. This operational simplicity extends to the extensive EC2 instance catalog, giving you access to the latest-generation processors like AWS Graviton4 and high-bandwidth networking options. You can process parallel requests within each execution environment, maximizing resource utilization and improving price-performance.\n  Lambda Managed Instances is ideal for customers requiring specialized hardware configurations, as well as those with steady-state or predictable workloads seeking to optimize costs while maintaining Lambda's serverless experience. You can further improve costs by leveraging EC2 pricing models including Compute Savings Plans and Reserved Instances.\n  Getting started is straightforward - you can continue building functions with familiar development workflows, including Console and your preferred IDEs. First, create a capacity provider that defines your compute preferences, including VPC configuration, optional instance requirements and scaling policies. Then, attach your Lambda functions to the capacity provider via the AWS Lambda Console, APIs, or Infrastructure as Code tooling. Lambda Managed Instances integrates seamlessly with all Lambda event sources and tools like Amazon CloudWatch, AWS X-Ray and AWS Config. Latest versions of Java, Node.js, Python and .NET runtimes are supported.\n  Lambda Managed Instances is now available in the US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Tokyo), and Europe (Ireland) Regions. To learn more, visit the launch blog and AWS Lambda Managed Instances documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/aws-lambda-managed-instances",
      "pubDate": "2025-11-30T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lambda",
        "ec2",
        "cloudwatch",
        "graviton"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lambda",
        "ec2",
        "cloudwatch",
        "graviton",
        "launch",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-a2412e35ffb9",
      "title": "Amazon CloudWatch incident reports now support Five Whys analysis",
      "description": "Amazon CloudWatch launched incident report generation capabilities with an AI-powered root-cause workflow that guides customers through the \"Five Why’s\" analysis technique. The feature is modeled on the correction or errors process used by both teams within Amazon and our customers to improve their operations.\n  The incident report generation capability now supports a guided, chat-based workflow powered by Amazon Q that walks customers through identifying the “Five Why’s” behind an incident. Teams can use this process to help identify the underlying root causes behind an incident. The capability leverages both human input and AI-based analysis of incident data to recommends specific measures operators can take to prevent future occurrences and improve their operations.\n  The incident report generation feature is available at no additional cost for CloudWatch customers and is available in US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Hong Kong), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), Europe (Ireland), Europe (Spain), and Europe (Stockholm).\n  You can create an incident report by first creating a CloudWatch investigation and then clicking “Incident report”. To initiate the Five Whys workflow, scroll down to the “Five Why’s” section of your report and select “Guide Me”. To learn more, visit the CloudWatch incident reports documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-cloudwatch-incident-reports-five-whys-analysis",
      "pubDate": "2025-11-30T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "cloudwatch",
        "launch",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-4a3aca511605",
      "title": "Accelerate data lake operations with Apache Iceberg V3 deletion vectors and row lineage",
      "description": "In this post, we walk you through the new capabilities in Iceberg V3, explain how deletion vectors and row lineage address these challenges, explore real-world use cases across industries, and provide practical guidance on implementing Iceberg V3 features across AWS analytics, catalog, and storage services.",
      "link": "https://aws.amazon.com/blogs/big-data/accelerate-data-lake-operations-with-apache-iceberg-v3-deletion-vectors-and-row-lineage/",
      "pubDate": "2025-11-26T22:05:47.000Z",
      "source": "bigDataBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": []
    },
    {
      "id": "aws-news-0445b6b72acd",
      "title": "How Condé Nast accelerated contract processing and rights analysis with Amazon Bedrock",
      "description": "In this post, we explore how Condé Nast used Amazon Bedrock and Anthropic’s Claude to accelerate their contract processing and rights analysis workstreams. The company’s extensive portfolio, spanning multiple brands and geographies, required managing an increasingly complex web of contracts, rights, and licensing agreements.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-conde-nast-accelerated-contract-processing-and-rights-analysis-with-amazon-bedrock/",
      "pubDate": "2025-11-26T21:37:27.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "lex"
      ]
    },
    {
      "id": "aws-news-617c09bb9eb6",
      "title": "University of California Los Angeles delivers an immersive theater experience with AWS generative AI services",
      "description": "In this post, we will walk through the performance constraints and design choices by OARC and REMAP teams at UCLA, including how AWS serverless infrastructure, AWS Managed Services, and generative AI services supported the rapid design and deployment of our solution. We will also describe our use of Amazon SageMaker AI and how it can be used reliably in immersive live experiences.",
      "link": "https://aws.amazon.com/blogs/machine-learning/university-of-california-los-angeles-delivers-an-immersive-theater-experience-with-aws-generative-ai-services/",
      "pubDate": "2025-11-26T21:20:45.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "support"
      ]
    },
    {
      "id": "aws-news-7aef8bce6a76",
      "title": "Amazon SageMaker AI introduces EAGLE based adaptive speculative decoding to accelerate generative AI inference",
      "description": "Amazon SageMaker AI now supports EAGLE-based adaptive speculative decoding, a technique that accelerates large language model inference by up to 2.5x while maintaining output quality. In this post, we explain how to use EAGLE 2 and EAGLE 3 speculative decoding in Amazon SageMaker AI, covering the solution architecture, optimization workflows using your own datasets or SageMaker's built-in data, and benchmark results demonstrating significant improvements in throughput and latency.",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-introduces-eagle-based-adaptive-speculative-decoding-to-accelerate-generative-ai-inference/",
      "pubDate": "2025-11-26T00:29:42.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-2e1c3c046458",
      "title": "Introducing Amazon S3 Transfer Manager for Swift (Developer Preview)",
      "description": "e are pleased to announce the Developer Preview release of the Amazon S3 Transfer Manager for Swift —a high-level file and directory transfer utility for \nAmazon Simple Storage Service (Amazon S3) built with the \nAWS SDK for Swift.",
      "link": "https://aws.amazon.com/blogs/developer/introducing-amazon-s3-transfer-manager-for-swift-developer-preview/",
      "pubDate": "2025-11-21T21:02:48.000Z",
      "source": "developersAndDevOps",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "preview"
      ]
    },
    {
      "id": "aws-news-d48c6bab49bb",
      "title": "Serverless strategies for streaming LLM responses",
      "description": "Modern generative AI applications often need to stream large language model (LLM) outputs to users in real-time. Instead of waiting for a complete response, streaming delivers partial results as they become available, which significantly improves the user experience for chat interfaces and long-running AI tasks. This post compares three serverless approaches to handle Amazon Bedrock LLM streaming on Amazon Web Services (AWS), which helps you choose the best fit for your application.",
      "link": "https://aws.amazon.com/blogs/compute/serverless-strategies-for-streaming-llm-responses/",
      "pubDate": "2025-11-21T03:42:56.000Z",
      "source": "computeBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-547c9eb92bd7",
      "title": "Building responsive APIs with Amazon API Gateway response streaming",
      "description": "Today, AWS announced support for response streaming in Amazon API Gateway to significantly improve the responsiveness of your REST APIs by progressively streaming response payloads back to the client. With this new capability, you can use streamed responses to enhance user experience when building LLM-driven applications (such as AI agents and chatbots), improve time-to-first-byte (TTFB) performance for web and mobile applications, stream large files, and perform long-running operations while reporting incremental progress using protocols such as server-sent events (SSE).",
      "link": "https://aws.amazon.com/blogs/compute/building-responsive-apis-with-amazon-api-gateway-response-streaming/",
      "pubDate": "2025-11-19T23:10:51.000Z",
      "source": "computeBlog",
      "services": [
        "api gateway"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "api gateway",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-053825de2c68",
      "title": "Optimize latency-sensitive workloads with Amazon EC2 detailed NVMe statistics",
      "description": "Amazon Elastic Cloud Compute (Amazon EC2) instances with locally attached NVMe storage can provide the performance needed for workloads demanding ultra-low latency and high I/O throughput. High-performance workloads, from high-frequency trading applications and in-memory databases to real-time analytics engines and AI/ML inference, need comprehensive performance tracking. Operating system tools like iostat and sar provide valuable system-level insights, and Amazon CloudWatch offers important disk IOPs and throughput measurements, but high-performance workloads can benefit from even more detailed visibility into instance store performance.",
      "link": "https://aws.amazon.com/blogs/compute/optimize-latency-sensitive-workloads-with-amazon-ec2-detailed-nvme-statistics/",
      "pubDate": "2025-11-19T21:13:06.000Z",
      "source": "computeBlog",
      "services": [
        "ec2",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "cloudwatch"
      ]
    },
    {
      "id": "aws-news-089334445f81",
      "title": "Build resilient generative AI agents",
      "description": "Generative AI agents in production environments demand resilience strategies that go beyond traditional software patterns. AI agents make autonomous decisions, consume substantial computational resources, and interact with external systems in unpredictable ways. These characteristics create failure modes that conventional resilience approaches might not address. This post presents a framework for AI agent resilience risk analysis […]",
      "link": "https://aws.amazon.com/blogs/architecture/build-resilient-generative-ai-agents/",
      "pubDate": "2025-09-30T15:11:51.000Z",
      "source": "architectureBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": []
    },
    {
      "id": "aws-news-54c273e45b01",
      "title": "Upgrading your AWS SDK for Go from V1 to V2 with Amazon Q Developer",
      "description": "Software development is far more than just writing code. In reality, a developer spends a large amount of time maintaining existing applications and fixing bugs. For example, migrating a Go application from the older AWS SDK for Go v1 to the newer v2 can be a significant undertaking, but it’s a crucial step to future-proof […]",
      "link": "https://aws.amazon.com/blogs/developer/upgrading-your-aws-sdk-for-go-from-v1-to-v2-with-amazon-q-developer/",
      "pubDate": "2025-06-18T06:38:24.000Z",
      "source": "developersAndDevOps",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer"
      ]
    },
    {
      "id": "aws-news-c4f514e85eef",
      "title": "AWS SDK for Ruby: Deprecating Ruby 2.5 & 2.6 Runtime Supports and Future Compatibility",
      "description": "Effective June 2, 2025, AWS SDK for Ruby Version 3 will no longer support following end-of-life (EOL) Ruby runtime versions: Ruby 2.5 (EOL began on 2021-04-05) Ruby 2.6 (EOL began on 2022-04-12) To ensure your applications and services remain secure, we strongly encourage you to upgrade to Ruby 2.7 or later. Moving forward, AWS SDK […]",
      "link": "https://aws.amazon.com/blogs/developer/aws-sdk-for-ruby-deprecating-ruby-2-5-2-6-runtime-supports-and-future-compatibility/",
      "pubDate": "2025-03-27T15:08:27.000Z",
      "source": "developersAndDevOps",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-5cf08af5aca4",
      "title": "Announcing the Developer Preview of Amazon S3 Transfer Manager in Rust",
      "description": "We are excited to announce the Developer Preview of the Amazon S3 Transfer Manager for Rust, a high-level utility that speeds up and simplifies uploads and downloads with Amazon Simple Storage Service (Amazon S3). Using this new library, developers can efficiently transfer data between Amazon S3 and various sources, including files, in-memory buffers, memory streams, […]",
      "link": "https://aws.amazon.com/blogs/developer/announcing-the-developer-preview-of-amazon-s3-transfer-manager-in-rust/",
      "pubDate": "2025-03-26T15:52:22.000Z",
      "source": "developersAndDevOps",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "preview"
      ]
    }
  ]
}