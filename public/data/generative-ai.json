{
  "lastUpdated": "2025-09-28T06:13:55.996Z",
  "category": "generative-ai",
  "totalItems": 67,
  "items": [
    {
      "id": "aws-news-c959ef8c3720",
      "title": "Building health care agents using Amazon Bedrock AgentCore",
      "description": "In this solution, we demonstrate how the user (a parent) can interact with a Strands or LangGraph agent in conversational style and get information about the immunization history and schedule of their child, inquire about the available slots, and book appointments. With some changes, AI agents can be made event-driven so that they can automatically send reminders, book appointments, and so on.",
      "link": "https://aws.amazon.com/blogs/machine-learning/building-health-care-agents-using-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-26T16:03:41.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore"
      ]
    },
    {
      "id": "aws-news-72cd9c93cdeb",
      "title": "Build multi-agent site reliability engineering assistants with Amazon Bedrock AgentCore",
      "description": "In this post, we demonstrate how to build a multi-agent SRE assistant using Amazon Bedrock AgentCore, LangGraph, and the Model Context Protocol (MCP). This system deploys specialized AI agents that collaborate to provide the deep, contextual intelligence that modern SRE teams need for effective incident response and infrastructure management.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-multi-agent-site-reliability-engineering-assistants-with-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-26T15:58:34.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore"
      ]
    },
    {
      "id": "aws-news-4419894b788a",
      "title": "Local file redirection is now available on Amazon AppStream 2.0 multi-session fleets",
      "description": "Amazon AppStream 2.0 is enhancing the end-user experience by introducing support for local files redirection on multi-session fleets. While this feature is already available on single-session fleets, this launch extends it to multi-session fleets, helping administrators to leverage the cost benefits of the multi-session model while providing an enhanced end-user experience.\n  Local file redirection on AppStream helps deliver benefits by enabling seamless access to local files directly from streaming applications, enhancing user productivity and experience. This feature reduces the need for manual file uploads and downloads, providing a natural, desktop-like experience with intuitive drag-and-drop functionality. Users can more efficiently manage their workflows while helping to maintain security through controlled access to local resources and secure file handling between environments.\n  This feature is available at no additional cost in all the AWS Regions where Amazon AppStream 2.0 is available. AppStream 2.0 offers pay-as-you go pricing. To get started with AppStream 2.0, see Getting Started with Amazon AppStream 2.0.\n  To enable this feature for your users, you must use an AppStream 2.0 image that uses latest AppStream 2.0 agent or has been updated using Managed AppStream 2.0 image updates released on or after September 05, 2025.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/local-file-redirection-amazon-appstream-multi-session-fleets/",
      "pubDate": "2025-09-26T15:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "now-available",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-13a535dc87a5",
      "title": "Amazon EBS increases the maximum size and provisioned performance of General Purpose (gp3) volumes",
      "description": "Amazon Elastic Block Store (Amazon EBS) now supports higher volume-level limits for its General Purpose (gp3) volumes. With this update, gp3 volumes can scale up to 64 TiB in size (4X the previous 16 TiB limit), up to 80,000 IOPS (5X the previous 16,000 IOPS limit), and up to 2,000 MiB/s throughput (2X the previous 1,000 MiB/s limit).\n  These expanded limits help reduce operational complexity for storage-intensive workloads by enabling gp3 volumes with larger capacity and higher performance. You can consolidate multiple striped volumes into a single gp3 volume, streamline architectures, and lower management overhead. The increased limits particularly benefit customers running containerized workloads with limited support for striping multiple volumes, applications that rely on single-volume architectures, and growing workloads approaching current gp3 limits. The pricing model remains unchanged: you pay for storage plus any additional IOPS and throughput provisioned beyond the baseline performance.\n  The new gp3 limits are available in all AWS Commercial Regions and AWS GovCloud (US) Regions where gp3 volumes are available. To get started and learn more, please visit the Amazon EBS user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ebs-size-provisioned-performance-gp3-volumes/",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "lex",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-82a70dbffaef",
      "title": "AWS Compute Optimizer now supports 99 new Amazon EC2 instance types",
      "description": "AWS Compute Optimizer now supports 99 additional Amazon Elastic Compute Cloud (Amazon EC2) instance types. These enhancements help you identify additional savings opportunities across your EC2 instances without specialized knowledge or manual analysis.\n  Compute Optimizer has expanded support to include the latest generation Compute Optimized (C8gn, C8gd), General Purpose (M8i, M8i-flex, M8gd), Memory Optimized (R8i, R8i-flex, R8gd), and Storage Optimized (I8ge) instance types. This expansion enables Compute Optimizer to help you take advantage of the price-to-performance improvements offered by the newest instance types.\n  This new feature is available in all AWS Regions where Compute Optimizer is available except the AWS GovCloud (US) and the China Regions. For more information about Compute Optimizer, visit our product page and documentation. You can start using Compute Optimizer through the AWS Management Console, AWS CLI, or AWS SDK.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-compute-optimizer-99-new-amazon-ec2-instance-types/",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "lex",
        "new-feature",
        "improvement",
        "enhancement",
        "support",
        "expansion"
      ]
    },
    {
      "id": "aws-news-c38dfafac29a",
      "title": "DoWhile loops now supported in Amazon Bedrock Flows",
      "description": "Today, we are excited to announce support for DoWhile loops in Amazon Bedrock Flows. With this powerful new capability, you can create iterative, condition-based workflows directly within your Amazon Bedrock flows, using Prompt nodes, AWS Lambda functions, Amazon Bedrock Agents, Amazon Bedrock Flows inline code, Amazon Bedrock Knowledge Bases, Amazon Simple Storage Service (Amazon S3), […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/dowhile-loops-now-supported-in-amazon-bedrock-flows/",
      "pubDate": "2025-09-25T20:25:08.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-1510ce03e38b",
      "title": "How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock",
      "description": "In this post, we explore how we built a multi-agent conversational AI system using Amazon Bedrock that delivers knowledge-grounded property investment advice. We explore the agent architecture, model selection strategy, and comprehensive continuous evaluation system that facilitates quality conversations while facilitating rapid iteration and improvement.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-prophero-built-an-intelligent-property-investment-advisor-with-continuous-evaluation-using-amazon-bedrock/",
      "pubDate": "2025-09-25T19:25:23.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "improvement"
      ]
    },
    {
      "id": "aws-news-daeb8d6f30a5",
      "title": "Accelerate benefits claims processing with Amazon Bedrock Data Automation",
      "description": "In the benefits administration industry, claims processing is a vital operational pillar that makes sure employees and beneficiaries receive timely benefits, such as health, dental, or disability payments, while controlling costs and adhering to regulations like HIPAA and ERISA. In this post, we examine the typical benefit claims processing workflow and identify where generative AI-powered automation can deliver the greatest impact.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-benefits-claims-processing-with-amazon-bedrock-data-automation/",
      "pubDate": "2025-09-25T19:20:16.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-b54db45a9ae5",
      "title": "Amazon Bedrock AgentCore Runtime, Browser, and Code Interpreter add support for VPC, AWS PrivateLink, CloudFormation, and tagging",
      "description": "Amazon Bedrock AgentCore Runtime, Browser, and Code Interpreter services now support Amazon Virtual Private Cloud (VPC) connectivity, AWS PrivateLink, AWS CloudFormation, and resource tagging, enabling developers to deploy AI agents with enhanced enterprise security and infrastructure automation capabilities. AgentCore Runtime enables you to deploy and scale dynamic AI agents securely using any framework, protocol, or model. AgentCore Browser enables web-based interactions such as form filling, data extraction, and QA testing, while AgentCore Code Interpreter provides secure execution of agent-generated code.\n  With VPC support, you can now securely connect AgentCore Runtime, Browser, and Code Interpreter services to private resources such as databases, internal APIs, and services within your VPC without internet exposure. AWS PrivateLink provides private connectivity between your VPC and Amazon Bedrock AgentCore services, while CloudFormation support enables automated resource provisioning through infrastructure as code. Resource tagging allows you to implement comprehensive cost allocation, access control, and resource organization across your AgentCore deployments.\n  Amazon Bedrock AgentCore is currently in preview and available in the following AWS Regions: US East (N. Virginia), US West (Oregon), Asia Pacific (Sydney), and Europe (Frankfurt).\n  To learn more, see Configuring VPC for AgentCore and Use Interface VPC endpoints (AWS PrivateLink) with AgentCore. For CloudFormation resources, visit the AgentCore CloudFormation Reference, and to get started with tagging, see the Tagging AgentCore resources.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-bedrock-agentcore-runtime-browser-code-interpreter-vpc-privatelink-cloudformation-tagging",
      "pubDate": "2025-09-25T18:00:00.000Z",
      "source": "whats-new",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "preview",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-7a5d69e1edad",
      "title": "Amazon EC2 I7i instances now available in AWS Europe (Milan) and AWS US West (N. California)",
      "description": "AWS is announcing the availability of high performance Storage optimized Amazon EC2 I7i instances in AWS Europe (Milan) and US West (N. California) regions. Powered by 5th Gen Intel Xeon Processors with an all-core turbo frequency of 3.2 GHz, these new instances deliver up to 23% better compute performance and more than 10% better price performance over previous generation I4i instances. Powered by 3rd generation AWS Nitro SSDs, I7i instances offer up to 45TB of NVMe storage with up to 50% better real-time storage performance, up to 50% lower storage I/O latency, and up to 60% lower storage I/O latency variability compared to I4i instances.\n  I7i instances offer compute and storage performance for x86-based storage optimized instances in Amazon EC2 ideal for I/O intensive and latency-sensitive workloads that demand very high random IOPS performance with real-time latency to access the small to medium size datasets. Additionally, torn write prevention feature support up to 16KB block sizes, enabling customers to eliminate database performance bottlenecks.\n  I7i instances are available in eleven sizes - nine virtual sizes up to 48xlarge and two bare metal sizes - delivering up to 100Gbps of network bandwidth and 60Gbps of Amazon Elastic Block Store (EBS) bandwidth. To learn more, visit the I7i instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-i7i-instances-available-in-milan-california/",
      "pubDate": "2025-09-25T17:01:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-4240934578e0",
      "title": "Amazon CloudWatch now supports resource tags when monitoring vended metrics",
      "description": "Today, Amazon CloudWatch announces support for a new tag-based telemetry experience to help customers monitor their metrics and set up their alarms using AWS resources tags. This new capability simplifies monitoring cloud infrastructure at scale by automatically adapting alarms and metrics analysis as resources change. DevOps engineers and cloud administrators can now create dynamic monitoring views that align with their organizational structure using their existing AWS resource tags.\n  Tag-based querying filtering eliminates the manual overhead of updating alarms and dashboards after deployments, freeing teams to focus on innovation rather than maintenance. This provides faster, targeted insights that match how teams organize their systems. Teams can query AWS default metrics using their existing resource tags, making it easier to troubleshoot issues and maintain operational visibility while focusing on core business initiatives.\n  CloudWatch tag-based filtering is available in the following regions: US East (N. Virginia); US East (Ohio); US West (N. California); US West (Oregon); Asia Pacific (Tokyo); Asia Pacific (Seoul); Asia Pacific (Singapore); Asia Pacific (Sydney); Asia Pacific (Mumbai); Asia Pacific (Osaka); Canada (Central); Europe (Frankfurt); Europe (Ireland); Europe (London); Europe (Paris); Europe (Stockholm) and South America (São Paulo).\n  To get started, simply enable tag enriched telemetry with one click in the Amazon CloudWatch Settings, or through the AWS Command Line Interface (AWS CLI), and AWS SDKs to use your existing AWS resource tags to monitor your infrastructure. Learn more on the Amazon CloudWatch documentation page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-cloudwatch-tags-observability",
      "pubDate": "2025-09-25T09:00:00.000Z",
      "source": "whats-new",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-b049add8cd1b",
      "title": "Amazon EC2 Allowed AMIs setting adds new parameters for enhanced AMI governance",
      "description": "Allowed AMIs, the Amazon EC2 account-wide setting that enables you to limit the discovery and use of Amazon Machine Images (AMIs) within your Amazon Web Services accounts, adds support for four new parameters — marketplace codes, deprecation time, creation date and AMI names.\n  Previously, you could specify accounts or owner aliases that you trust in your Allowed AMIs setting. Starting today, you can use the four new parameters to define additional criteria to further reduce risk of inadvertently launching instances with non-compliant or unauthorized AMIs. Marketplace codes can be provided to limit the use of Marketplace AMIs, the deprecation time and creation date parameters can be used to limit the use of outdated AMIs, and AMI name parameter can be used to restrict usage to AMIs with specific naming pattern. You can also leverage Declarative Policies to configure these parameters to perform AMI governance across your organization.\n  These additional parameters are now supported in all AWS regions including AWS China (Beijing) Region, operated by Sinnet, and AWS China (Ningxia) Region, operated by NWCD, and AWS GovCloud (US). To learn more, please visit the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-allowed-amis-setting-parameters-ami-governance/",
      "pubDate": "2025-09-25T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-6a85e20f60f0",
      "title": "PostgreSQL 18.0 is now available in Amazon RDS Database Preview Environment",
      "description": "Amazon RDS for PostgreSQL 18.0 is now available in the Amazon RDS Database Preview Environment, allowing you to evaluate the latest PostgreSQL features while leveraging the benefits of a fully managed database service. This preview environment provides you a sandbox where you can test applications and explore new PostgreSQL 18.0 capabilities before they become generally available.\n \nPostgreSQL 18.0 includes \"skip scan\" support for multicolumn B-tree indexes and improves WHERE clause handling for OR and IN conditions. It introduces parallel Generalized Inverted Index (GIN) builds and updates join operations. It now supports Universally Unique Identifiers Version 7 (UUIDv7), which combines timestamp-based ordering with traditional UUID uniqueness to boost performance in high-throughput distributed systems. Observability improvements show buffer usage counts and index lookups during query execution, along with per-connection I/O utilization metric. Please refer to the RDS PostgreSQL release documentation for more details.\n \nAmazon RDS Database Preview Environment database instances are retained for a maximum period of 60 days and are automatically deleted after the retention period. Amazon RDS database snapshots that are created in the preview environment can only be used to create or restore database instances within the preview environment. You can use the PostgreSQL dump and load functionality to import or export your databases from the preview environment.\n \nAmazon RDS Database Preview Environment database instances are priced as per the pricing in the US East (Ohio) Region.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/postgresql-180-amazon-rds-database-preview-environment/",
      "pubDate": "2025-09-25T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "preview",
        "generally-available",
        "now-available",
        "update",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-36218ab4636b",
      "title": "AWS announces unlimited network burst duration on EC2 I8g and I7i instances",
      "description": "Today, AWS eliminated the networking bandwidth burst duration limitations for Amazon EC2 I7i and I8g instances on sizes larger than 4xlarge. This update doubles the Network Bandwidth available at all times for i7i and i8g instances on sizes larger than 4xlarge. Previously, these instance sizes had a baseline bandwidth and used a network I/O credit mechanism to burst beyond their baseline bandwidth on a best effort basis. Today these instance sizes can sustain their maximum performance indefinitely. With this improvement, customers running memory and network intensive workloads on larger instance sizes can now consistently maintain their maximum network bandwidth without interruption, delivering more predictable performance for applications that require sustained high-throughput network connectivity. This change applies only to instance sizes larger than 4xlarge, while smaller instances will continue to operate with their existing baseline and burst bandwidth configurations.\n \nAmazon EC2 I7i and I8g instances are designed for I/O intensive workloads that require rapid data access and real-time latency from storage. These instances excel at handling transactional, real-time, distributed databases, including MySQL, PostgreSQL, Hbase and NoSQL solutions like Aerospike, MongoDB, ClickHouse, and Apache Druid. They're also optimized for real-time analytics platforms such as Apache Spark, data lakehouse, and AI LLM pre-processing for training. These instances have up to 1.5 TiB of memory, and 45 TB local instance storage. They deliver up to 100 Gbps of network performance bandwidth, and 60 Gbps of dedicated bandwidth for Amazon Elastic Block Store (EBS).\n \nTo learn more, see Amazon EC2 I7i and I8g instances. To get started, see AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-announces-unlimited-network-burst-duration-i8g-i7i",
      "pubDate": "2025-09-24T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai",
        "machine-learning"
      ],
      "tags": [
        "update",
        "improvement"
      ]
    },
    {
      "id": "aws-news-f76fdca33bd0",
      "title": "Running deep research AI agents on Amazon Bedrock AgentCore",
      "description": "AI agents are evolving beyond basic single-task helpers into more powerful systems that can plan, critique, and collaborate with other agents to solve complex problems. Deep Agents—a recently introduced framework built on LangGraph—bring these capabilities to life, enabling multi-agent workflows that mirror real-world team dynamics. The challenge, however, is not just building such agents but […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/running-deep-research-ai-agents-on-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-23T20:35:23.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex"
      ]
    },
    {
      "id": "aws-news-86dbb31f2325",
      "title": "Amazon DataZone is now available in 3 additional commercial regions",
      "description": "Amazon DataZone is now available in AWS Asia Pacific (Hong Kong), Asia Pacific (Malaysia) and Europe (Zurich) Regions.\n  Amazon DataZone is a fully managed data management service to catalog, discover, analyze, share, and govern data between data producers and consumers in your organization. With Amazon DataZone, data producers populate the business data catalog with structured data assets from AWS Glue Data Catalog and Amazon Redshift tables. Data consumers search and subscribe to data assets in the data catalog and share with other collaborators working on the same business use case. Consumers can analyze their subscribed data assets with tools—such as Amazon Redshift or Amazon Athena query editors—that are directly accessed from the Amazon DataZone portal. The integrated publishing and subscription workflow provides access to auditing capabilities across projects.\n  For more information on AWS Regions where Amazon DataZone is available in preview, see supported regions.\n \nAdditionally, Amazon DataZone powers governance in the next generation of Amazon SageMaker, which simplifies the discovery, governance, and collaboration of data and AI across your lakehouse, AI models, and GenAI applications. With Amazon SageMaker Catalog (built on Amazon DataZone), users can securely discover and access approved data and models using semantic search with generative AI–created metadata, or they could just ask Amazon Q Developer using natural language to find their data. For more information on AWS Regions where the next generation of SageMaker is available, see supported regions. To learn more about the next generation of SageMaker, visit the product webpage.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-datazone-additional-regions/",
      "pubDate": "2025-09-23T18:00:00.000Z",
      "source": "whats-new",
      "services": [
        "amazon q",
        "q developer",
        "sagemaker"
      ],
      "categories": [
        "generative-ai",
        "machine-learning",
        "search"
      ],
      "tags": [
        "amazon q",
        "q developer",
        "sagemaker",
        "preview",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-b648bd753ce9",
      "title": "Integrate tokenization with Amazon Bedrock Guardrails for secure data handling",
      "description": "In this post, we show you how to integrate Amazon Bedrock Guardrails with third-party tokenization services to protect sensitive data while maintaining data reversibility. By combining these technologies, organizations can implement stronger privacy controls while preserving the functionality of their generative AI applications and related systems.",
      "link": "https://aws.amazon.com/blogs/machine-learning/integrate-tokenization-with-amazon-bedrock-guardrails-for-secure-data-handling/",
      "pubDate": "2025-09-23T17:31:04.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "ga"
      ]
    },
    {
      "id": "aws-news-b8a7a01ed381",
      "title": "Accelerate AI agent development with the Nova Act IDE extension",
      "description": "The Nova Act extension is a new IDE-integrated tool that enables developers to create browser automation agents using natural language through the Nova Act model, offering features like Builder Mode, chat capabilities, and predefined templates while streamlining the development process without leaving their preferred development environment.",
      "link": "https://aws.amazon.com/blogs/aws/accelerate-ai-agent-development-with-the-nova-act-ide-extension/",
      "pubDate": "2025-09-23T16:01:04.000Z",
      "source": "news-blog",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-2d74e91ea4b5",
      "title": "Amazon Nova Act extension: Build and test AI agents within your IDE",
      "description": "We’re excited today to announce the Amazon Nova Act extension - a tool that transforms how you build with Nova Act by bringing the entire agent development experience directly into IDEs like Visual Studio Code, Kiro, and Cursor. The Nova Act extension consolidates natural language based script creation, granular scripting precision, and robust browser testing into a single, unified user interface, eliminating the need to switch between multiple tools across development, validation, and iteration.\n  The Nova Act extension is built on top of the Nova Act SDK, available in research preview since March 2025. The Nova Act extension addresses feedback we have received from developers and consolidates the agent development lifecycle, from ideation to production, into one unified user interface within your IDE.\n  The Nova Act extension is available today from your IDE’s extension marketplace. The Nova Act GitHub repository includes documentation and examples to get started.\n  Learn more about the Nova Act extension and see the Nova Act extension in action at our blog post.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-nova-act-extension-build-test-ai-agents-ide/",
      "pubDate": "2025-09-23T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "preview"
      ]
    },
    {
      "id": "aws-news-56f37bafb064",
      "title": "Amazon EC2 R8gb instances are now generally available",
      "description": "Today, AWS announces the general availability of the new Amazon Elastic Block Storage (Amazon EBS) optimized Amazon Elastic Compute Cloud (Amazon EC2) R8gb instances. These instances are powered by AWS Graviton4 processors to deliver up to 30% better compute performance than AWS Graviton3 processors. At up to 150 Gbps of EBS bandwidth, these instances offer higher EBS performance compared to same-sized equivalent Graviton4-based instances. Take advantage of the higher block storage performance offered by these new EBS optimized EC2 instances to scale the performance and throughput of workloads such as high performance databases and NoSQL databases, while optimizing the cost of running your workloads.\n \nFor increased scalability, these instances offer instance sizes up to 24xlarge, including one metal size, up to 768 GiB of memory, up to 150 Gbps of EBS bandwidth, up to 200 Gbps of networking bandwidth. These instances support Elastic Fabric Adapter (EFA) networking on the 16xlarge, 24xlarge, and metal-24xl sizes, which enables lower latency and improved cluster performance for workloads deployed on tightly coupled clusters.\n \nThe new R8gb instances are available in US East (N. Virginia) and US West (Oregon) regions. Metal sizes are only available in US East (N. Virginia) region.\n \nTo learn more, see Amazon R8gb Instances. To begin your Graviton journey, visit the Level up your compute with AWS Graviton page. To get started, see AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-r8gb-instances/",
      "pubDate": "2025-09-23T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "generally-available",
        "support"
      ]
    },
    {
      "id": "aws-news-f6ea4658ae0e",
      "title": "AWS Weekly Roundup: Amazon Q Developer, AWS Step Functions, AWS Cloud Club Captain deadline, and more (September 22, 2025)",
      "description": "Three weeks ago, I published a post about the new AWS Region in New Zealand (ap-southeast-6). This led to an incredible opportunity to visit New Zealand, where I met passionate builders and presented at several events including Serverless and Platform Engineering meetup, AWS Tools and Programming meetup, AWS Cloud Clubs in Auckland, and AWS Community […]",
      "link": "https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-q-developer-aws-step-functions-aws-cloud-club-captain-deadline-and-more-september-22-2025/",
      "pubDate": "2025-09-22T16:21:44.000Z",
      "source": "news-blog",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer"
      ]
    },
    {
      "id": "aws-news-77e33a4e16c4",
      "title": "Amazon Connect flow designer now supports analytics mode",
      "description": "Amazon Connect now offers new enhanced analytics in the drag-and-drop flow designer that help you make data-driven decisions when building and optimizing your flows. Amazon Connect flows allow you to create end-to-end self-service and automated customer experiences such as interactive voice response (IVR), step-by-step guides, and back office processes and tasks. With this launch, you can now view aggregate metrics on how customers move through each step in the flow including where they run into errors or abandon the experience. For example, you can see how many conversational AI interactions result in transfers to agent queues or when customers end up in the wrong queue because an error in the flow configuration. These new capabilities help you identify behavioral patterns and evaluate root causes, allowing you to deliver better outcomes for customers.\n  This new capability is included with Amazon Connect (with unlimited AI) pricing. To learn more about this feature, see the Amazon Connect Administrator Guide. This feature is available in all AWS regions that offers Amazon Connect. To learn more about Amazon Connect, the AWS cloud-based contact center, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-connect-flow-designer-analytics-mode/",
      "pubDate": "2025-09-22T16:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "launch",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-d5a772368a74",
      "title": "Move your AI agents from proof of concept to production with Amazon Bedrock AgentCore",
      "description": "This post explores how Amazon Bedrock AgentCore helps you transition your agentic applications from experimental proof of concept to production-ready systems. We follow the journey of a customer support agent that evolves from a simple local prototype to a comprehensive, enterprise-grade solution capable of handling multiple concurrent users while maintaining security and performance standards.",
      "link": "https://aws.amazon.com/blogs/machine-learning/move-your-ai-agents-from-proof-of-concept-to-production-with-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-19T16:09:26.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "experimental",
        "support"
      ]
    },
    {
      "id": "aws-news-9c6439e1f1c9",
      "title": "Amazon RDS for MySQL announces Innovation Release 9.4 in Amazon RDS Database Preview Environment",
      "description": "Amazon RDS for MySQL now supports community MySQL Innovation Release 9.4 in the Amazon RDS Database Preview Environment, allowing you to evaluate the latest Innovation Release on Amazon RDS for MySQL. You can deploy MySQL 9.4 in the Amazon RDS Database Preview Environment which provides the benefits of a fully managed database, making it simpler to set up, operate, and monitor databases.\n  MySQL 9.4 is the latest Innovation Release from the MySQL community. MySQL Innovation releases include bug fixes, security patches, as well as new features. MySQL Innovation releases are supported by the community until the next innovation minor, whereas MySQL Long Term Support (LTS) Releases, such as MySQL 8.0 and MySQL 8.4, are supported by the community for up to eight years. Please refer to the MySQL 9.4 release notes and Amazon RDS MySQL release notes for more details.\n  Amazon RDS Database Preview Environment supports both Single-AZ and Multi-AZ deployments on the latest generation of instance classes. Amazon RDS Database Preview Environment database instances are retained for a maximum of 60 days and are automatically deleted after the retention period. Amazon RDS database snapshots created in the Preview Environment can only be used to create or restore database instances within the Preview Environment.\n  Amazon RDS Database Preview Environment database instances are priced the same as production RDS instances created in the US East (Ohio) Region. For further information, see Working with the Database Preview Environment. To get started with the Preview Environment from the RDS console, navigate here.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-rds-mysql-innovation-release-94-database-preview-environment/",
      "pubDate": "2025-09-19T15:00:00.000Z",
      "source": "whats-new",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "preview",
        "ga",
        "new-feature",
        "support"
      ]
    },
    {
      "id": "aws-news-d073fb3f6dab",
      "title": "Announcing AWS Neuron SDK 2.26.0",
      "description": "Today, AWS announces the general availability of Neuron SDK 2.26.0, delivering improvements for deep learning workloads on AWS Inferentia and Trainium-based instances. This release introduces support for PyTorch 2.8 and JAX 0.6.2, along with enhanced inference capabilities on Trainium2 (Trn2) instances. These updates enable developers to leverage the latest frameworks while benefiting from improved model deployment flexibility and performance optimizations.\n  With Neuron SDK 2.26.0, customers can now deploy FLUX.1-dev image generation model, along with Llama 4 Scout and Maverick variants (beta) on Trn2 instances. The release introduces expert parallelism support (beta) for efficient distribution of Mixture-of-Experts (MoE) models across multiple NeuronCores, and adds new capabilities through new Neuron Kernel Interface (NKI) APIs. The updated Neuron Profiler provides improved capabilities, including system profile grouping for distributed workloads.\n  The new SDK version is available in all AWS Regions supporting Inferentia and Trainium instances, offering enhanced performance and monitoring capabilities for machine learning workloads.\n  To learn more and for a full list of new features and enhancements, see:\n  \n \n \nAWS Neuron 2.26.0 release notes\n \n \nTrn2 Instances\n \n \nTrn1 Instances\n \n \nInf2 Instances",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-neuron-2-26-announce/",
      "pubDate": "2025-09-19T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "lex",
        "trainium",
        "inferentia",
        "neuron"
      ],
      "categories": [
        "generative-ai",
        "machine-learning",
        "natural-language"
      ],
      "tags": [
        "lex",
        "trainium",
        "inferentia",
        "neuron",
        "beta",
        "new-feature",
        "update",
        "improvement",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-3105df2f3e57",
      "title": "Qwen models are now available in Amazon Bedrock",
      "description": "Amazon Bedrock has expanded its model offerings with the addition of Qwen 3 foundation models enabling users to access and deploy them in a fully managed, serverless environment. These models feature both mixture-of-experts (MoE) and dense architectures to support diverse use cases including advanced code generation, multi-tool business automation, and cost-optimized AI reasoning.",
      "link": "https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/",
      "pubDate": "2025-09-18T22:02:11.000Z",
      "source": "news-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai",
        "specialized"
      ],
      "tags": [
        "bedrock",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-151bf1dc2ea9",
      "title": "DeepSeek-V3.1 model now available in Amazon Bedrock",
      "description": "AWS launches DeepSeek-V3.1 as a fully managed models in Amazon Bedrock. DeepSeek-V3.1 is a hybrid open weight model that switches between thinking mode for detailed step-by-step analysis and non-thinking mode for faster responses.",
      "link": "https://aws.amazon.com/blogs/aws/deepseek-v3-1-now-available-in-amazon-bedrock/",
      "pubDate": "2025-09-18T21:49:48.000Z",
      "source": "news-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "launch",
        "now-available"
      ]
    },
    {
      "id": "aws-news-c753ba529f3a",
      "title": "Scale visual production using Stability AI Image Services in Amazon Bedrock",
      "description": "This post was written with Alex Gnibus of Stability AI. Stability AI Image Services are now available in Amazon Bedrock, offering ready-to-use media editing capabilities delivered through the Amazon Bedrock API. These image editing tools expand on the capabilities of Stability AI’s Stable Diffusion 3.5 models (SD3.5) and Stable Image Core and Ultra models, which […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/scale-visual-production-using-stability-ai-image-services-in-amazon-bedrock/",
      "pubDate": "2025-09-18T21:25:49.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available"
      ]
    },
    {
      "id": "aws-news-a073e4c7ece2",
      "title": "Prompting for precision with Stability AI Image Services in Amazon Bedrock",
      "description": "Amazon Bedrock now offers Stability AI Image Services: 9 tools that improve how businesses create and modify images. The technology extends Stable Diffusion and Stable Image models to give you precise control over image creation and editing. Clear prompts are critical—they provide art direction to the AI system. Strong prompts control specific elements like tone, […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/prompting-for-precision-with-stability-ai-image-services-in-amazon-bedrock/",
      "pubDate": "2025-09-18T21:25:34.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-897537d328af",
      "title": "Amazon Q Developer CLI announces support for remote MCP servers",
      "description": "Amazon Q Developer CLI announces support for remote MCP servers. Remote MCP servers improve scalability and security of the tools you use within your development tasks. Not only does it reduce the use of compute resources by moving to a centralized server, it also helps you better manage access and security. You can now integrate with MCP servers such as Atlassian, and GitHub that support HTTP and support OAuth based authentication.\n  To configure a remote MCP server, specify the transport type as HTTP, the URL where users will get authentication credentials, and any optional headers to include when making the request. You can configure remote MCP servers in your custom agent configuration or in mcp.json. When a CLI session is initiated, you will see the list of MCP servers to load and can query the list for the authentication URL. Once you successfully complete the authentication steps, Q Developer CLI will query the tools available from the MCP server and make it available to the agent.\n  Remote MCP servers are available in Amazon Q Developer CLI and Amazon Q Developer IDE plugins. For more information, check out the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-q-developer-remote-mcp-servers/",
      "pubDate": "2025-09-18T17:00:00.000Z",
      "source": "whats-new",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer",
        "support"
      ]
    },
    {
      "id": "aws-news-3150c633eaf5",
      "title": "Stability AI Image Services now available in Amazon Bedrock",
      "description": "Amazon Bedrock announces the availability of Stability AI Image Services, a comprehensive suite of 9 specialized image editing tools designed to accelerate professional creative workflows. Stability AI Image Services enable granular control over image editing with a range of tools designed to work with your creative process, allowing you to take a single concept from ideation to finished product with precision and flexibility.\n  Stability AI Image Services offers two categories of image editing capabilities: Edit tools: Remove Background, Erase Object, Search and Replace, Search and Recolor, and Inpaint let you make targeted modifications to specific parts of your images. Control tools: Structure, Sketch, Style Guide, and Style Transfer give you powerful ways to generate variations based on existing images or sketches.\n  Stability AI Image Services is now available in Amazon Bedrock through the API and is supported in US West (Oregon), US East (N. Virginia), and US East (Ohio). For more information on supported regions, visit the Amazon Bedrock Model Support by Regions guide. For more details about Stability AI Image Services and its capabilities, visit the Stability AI product page and Stability AI documentation page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/stability-ai-image-services-generally-available-amazon-bedrock",
      "pubDate": "2025-09-18T16:00:00.000Z",
      "source": "whats-new",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-804168525168",
      "title": "Monitor Amazon Bedrock batch inference using Amazon CloudWatch metrics",
      "description": "In this post, we explore how to monitor and manage Amazon Bedrock batch inference jobs using Amazon CloudWatch metrics, alarms, and dashboards to optimize performance, cost, and operational efficiency.",
      "link": "https://aws.amazon.com/blogs/machine-learning/monitor-amazon-bedrock-batch-inference-using-amazon-cloudwatch-metrics/",
      "pubDate": "2025-09-18T15:33:07.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-966a8a4d6e6a",
      "title": "OpenAI open weight models expand to new regions on Amazon Bedrock",
      "description": "Today, AWS announces the expansion of OpenAI open weight models on Amazon Bedrock to eight new regions. This expansion brings these powerful AI models closer to customers in various parts of the world, enabling lower latency and improved performance for a wide range of AI-powered applications.\n  With this expansion, the OpenAI open weight models are now available in the following AWS Regions: US East (N. Virginia), Asia Pacific (Tokyo), Europe (Stockholm), Asia Pacific (Mumbai), Europe (Ireland), South America (São Paulo), Europe (London), and Europe (Milan), in addition to the previously supported region of US West (Oregon). This broader availability allows more customers to leverage these state-of-the-art AI models while keeping their data within their preferred geographic locations, helping to address data residency requirements and reduce network latency.\n  To learn more about OpenAI open weight models on Amazon Bedrock and how to get started, visit the Amazon Bedrock console or check out our documentation. For more information about the initial release of these models on Amazon Bedrock, refer to our previous blog post.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/open-ai-open-weight-models-new-regions-amazon-bedrock",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whats-new",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "now-available",
        "support",
        "new-region",
        "expansion"
      ]
    },
    {
      "id": "aws-news-0f3ee5cb5f1f",
      "title": "Qwen3 models are now available fully managed in Amazon Bedrock",
      "description": "Amazon Bedrock continues to expand model choice by adding four Qwen3 open weight foundation models, now available as fully managed, serverless offerings. The lineup includes: Qwen3-Coder-480B-A35B-Instruct, Qwen3-Coder-30B-A3B-Instruct, Qwen3-235B-A22B-Instruct-2507, and Qwen3-32B for efficient dense computation. These models feature both dense and Mixture-of-Experts (MoE) architectures, providing flexible options for various development needs.\n  These open weight models enable you to build powerful AI applications with advanced agentic capabilities, without managing any infrastructure. The two Qwen3-Coder models excel at agentic coding and complex software engineering tasks, offering state-of-the-art performance for function calling and tool use. The 235B model delivers efficient general reasoning and instruction following across diverse tasks, while the 32B dense model provides a more traditional architecture suitable for a wide range of computational tasks.\n  Qwen3 models (32B, Coder-30B) are available today in the US East (N. Virginia), US West (Oregon), Asia Paciﬁc (Mumbai, Tokyo), Europe (Ireland, London, Milan, Stockholm), and South America (São Paulo) AWS Regions. Qwen 235B is available today in theUS West (Oregon), Asia Paciﬁc (Mumbai, Tokyo), and Europe (London, Milan, Stockholm) AWS Regions. Qwen Coder-480B is available today in the US West (Oregon), Asia Paciﬁc (Mumbai, Tokyo), and Europe (London, Stockholm) AWS Regions. Check the full Region list for future updates. To learn more, read the blog, product page, Amazon Bedrock pricing, and documentation. To get started with Qwen in Amazon Bedrock, visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/qwen3-models-fully-managed-amazon-bedrock",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whats-new",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available",
        "update"
      ]
    },
    {
      "id": "aws-news-b2e219fc1dd9",
      "title": "DeepSeek-V3.1 model now available fully managed in Amazon Bedrock",
      "description": "DeepSeek-V3.1 is now available as a fully managed foundation model in Amazon Bedrock. This advanced open weight model allows you to switch between thinking mode for detailed step-by-step analysis and non-thinking mode for quicker responses. With comprehensive multilingual support, it delivers enhanced accuracy and reduced hallucinations compared to previous DeepSeek models, while maintaining visibility into its decision-making process.\n  You can use DeepSeek-V3.1's enterprise-grade capabilities across critical business functions, from state-of-the-art software development to complex mathematical reasoning and data analysis. The model excels at sophisticated problem-solving tasks, demonstrating strong performance in coding benchmarks and technical challenges. Its enhanced tool-calling capabilities and seamless workflow integration make it ideal for building AI agents and automating enterprise processes, while its transparent reasoning approach helps teams understand and trust its outputs.\n  \n DeepSeek-V3.1 is now available in the US West (Oregon), Asia Paciﬁc (Tokyo), Asia Paciﬁc (Mumbai), Europe (London), and Europe (Stockholm) AWS Regions. To learn more, read the blog, product page, Amazon Bedrock pricing, and documentation. To get started with DeepSeek in Amazon Bedrock, visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/deepseek-v3-1-model-fully-managed-amazon-bedrock",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whats-new",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-ab65551d4b6c",
      "title": "Amazon OpenSearch Serverless now supports Disk-Optimized Vectors",
      "description": "We are excited to announce the launch of disk-optimized vector support for Amazon OpenSearch Serverless, offering customers a cost-effective solution for vector search operations without compromising on accuracy and recall rates. This new feature enables organizations to implement high-quality vector search capabilities while significantly reducing operational costs.\n  With the introduction of Disk Optimized Vectors, customers can now choose between memory-optimized and disk-optimized vector storage options. The disk-optimized option delivers the same high accuracy and recall rates as memory-optimized vectors at lower cost. While this option may introduce slightly higher latency, it's ideal for use cases where sub-millisecond response times aren't critical such as semantic search applications, recommendation systems, and other AI-powered search scenarios.\n  Amazon OpenSearch Serverless, our fully managed deployment option, eliminates the complexities of infrastructure management for search and analytics workloads. The service automatically scales compute capacity, measured in OpenSearch Compute Units (OCUs), based on your workload demands.\n \nPlease refer to the AWS Regional Services List for more information about Amazon OpenSearch Service availability. To learn more about OpenSearch Serverless, see the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/opensearch-serverless-disk-optimized-vectors",
      "pubDate": "2025-09-18T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language",
        "search"
      ],
      "tags": [
        "lex",
        "launch",
        "ga",
        "new-feature",
        "support"
      ]
    },
    {
      "id": "aws-news-4cc4978a07d1",
      "title": "Supercharge your organization’s productivity with the Amazon Q Business browser extension",
      "description": "In this post, we showed how to use the Amazon Q Business browser extension to give your team seamless access to AI-driven insights and assistance. The browser extension is now available in US East (N. Virginia) and US West (Oregon) AWS Regions for Mozilla, Google Chrome, and Microsoft Edge as part of the Lite Subscription.",
      "link": "https://aws.amazon.com/blogs/machine-learning/supercharge-your-organizations-productivity-with-the-amazon-q-business-browser-extension/",
      "pubDate": "2025-09-17T19:37:32.000Z",
      "source": "ml-blog",
      "services": [
        "amazon q",
        "q business"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q business",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-914f37d8d608",
      "title": "Build Agentic Workflows with OpenAI GPT OSS on Amazon SageMaker AI and Amazon Bedrock AgentCore",
      "description": "In this post, we show how to deploy gpt-oss-20b model to SageMaker managed endpoints and demonstrate a practical stock analyzer agent assistant example with LangGraph, a powerful graph-based framework that handles state management, coordinated workflows, and persistent memory systems.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-agentic-workflows-with-openai-gpt-oss-on-amazon-sagemaker-ai-and-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-17T19:31:44.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "agentcore",
        "sagemaker"
      ],
      "categories": [
        "generative-ai",
        "machine-learning"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-f7a834be9ed0",
      "title": "Amazon EC2 I8ge instances now available in AWS Europe (Frankfurt)",
      "description": "Starting today, Amazon Elastic Compute Cloud (Amazon EC2) storage optimized I8ge instances are available in AWS Europe (Frankfurt) region. I8ge instances are powered by AWS Graviton4 processors to deliver up to 60% better compute performance compared to previous generation Graviton2-based storage optimized Amazon EC2 instances. I8ge instances use the latest third generation AWS Nitro SSDs, local NVMe storage that deliver up to 55% better real-time storage performance per TB while offering up to 60% lower storage I/O latency and up to 75% lower storage I/O latency variability compared to previous generation Im4gn instances. At 120 TB, I8ge instances have the highest storage density among AWS Graviton-based storage optimized Amazon EC2 instances. These instances are built on the AWS Nitro System, which oﬄoads CPU virtualization, storage, and networking functions to dedicated hardware and software enhancing the performance and security for your workloads.\n \nI8ge instances offer instance sizes up to 48xlarge including two metal sizes, 1,536 GiB of memory, and 120 TB instance storage. At 300 Gbps, these instances have the highest networking bandwidth among storage optimized Amazon EC2 instances. They are ideal for real-time applications that require much larger storage density such as relational databases, non-relational databases, streaming databases, search queries and data analytics.\n \nTo learn more, see Amazon EC2 I8ge instances. To begin your Graviton journey, visit the Level up your compute with AWS Graviton page. To get started, see AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-i8ge-instances-in-europe-frankfurt",
      "pubDate": "2025-09-17T17:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "now-available"
      ]
    },
    {
      "id": "aws-news-949eb2c3ce5a",
      "title": "AWS Parallel Computing Service (PCS) now supports Amazon EC2 Capacity Blocks for ML",
      "description": "AWS Parallel Computing Service (PCS) now supports Amazon EC2 Capacity Blocks for ML. You can now use Amazon EC2 instances reserved using EC2 Capacity Blocks natively in PCS clusters.\n  Native support for EC2 Capacity Blocks in PCS simplifies capacity planning for cutting-edge GPU-based workloads in Slurm clusters, helping to ensure that GPU capacity is available when and where it’s needed. EC2 Capacity Blocks can be associated with PCS compute node groups via an EC2 Launch Template.\n  PCS is a managed service that makes it easier for you to run and scale your high performance computing (HPC) workloads and build scientific and engineering models on AWS using Slurm. You can use PCS to build complete, elastic environments that integrate compute, storage, networking, and visualization tools. PCS simplifies cluster operations with managed updates and built-in observability features, helping to remove the burden of maintenance. You can work in a familiar environment, focusing on your research and innovation instead of worrying about infrastructure.\n  PCS now supports EC2 Capacity Blocks in all AWS Regions where both services are available. Read more about PCS support for EC2 Capacity Blocks in the PCS User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-parallel-computing-service-ec2-capacity-blocks-ml/",
      "pubDate": "2025-09-17T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "launch",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-284b40fb9a60",
      "title": "Amazon EC2 I7i instances now available in South America (São Paulo), Canada West (Calgary) regions",
      "description": "Amazon Web Services (AWS) announces the availability of high performance Storage Optimized Amazon EC2 I7i instances in the AWS South America (São Paulo), Canada West (Calgary) regions. Powered by 5th generation Intel Xeon Scalable processors with an all-core turbo frequency of 3.2 GHz, these new instances deliver up to 23% better compute performance and more than 10% better price performance over previous generation I4i instances. Powered by 3rd generation AWS Nitro SSDs, I7i instances offer up to 45TB of NVMe storage with up to 50% better real-time storage performance, up to 50% lower storage I/O latency, and up to 60% lower storage I/O latency variability compared to I4i instances.\n  I7i instances offer the best compute and storage performance for x86-based storage optimized instances in Amazon EC2, ideal for I/O intensive and latency-sensitive workloads that demand very high random IOPS performance with real-time latency to access the small to medium size datasets (multi-TBs). Additionally, torn write prevention feature support up to 16KB block sizes, enabling customers to eliminate database performance bottlenecks.\n  I7i instances are available in eleven sizes - nine virtual sizes up to 48xlarge and two bare metal sizes - delivering up to 100Gbps of network bandwidth and 60Gbps of Amazon Elastic Block Store (EBS) bandwidth.\n To learn more, visit the I7i instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-i7i-instances-sao-paulo-calgary-regions/",
      "pubDate": "2025-09-16T17:30:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-f789b4eea2b5",
      "title": "Amazon Lex provides generative AI based enhanced natural language understanding in eight new languages",
      "description": "Amazon Lex now allows you to leverage large language models (LLMs) to improve the natural language understanding of your deterministic conversational AI bots in eight new languages: Chinese, Japanese, Korean, Portuguese, Catalan, French, Italian, and German. With this capability, your voice- and chat-bots can better handle complex utterances, maintain accuracy despite spelling errors, and extract key information from verbose inputs to fulfill the customer’s request. For example, a customer could say ‘Hi I want to book a flight for my wife, my two kids and myself’, and the LLM will properly identify to book flight tickets for four people.\n \nThis feature is available in 10 commercial AWS Regions where Amazon Connect is available: Europe (Ireland), Europe (Frankfurt), US East (N. Virginia), Asia Pacific (Seoul), Europe (London), Asia Pacific (Tokyo), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central). To learn more about this feature, visit Amazon Lex documentation or to learn how Amazon Connect and Amazon Lex deliver cloud-based conversational AI experiences for contact centers, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-lex-generative-ai-natural-language-eight-languages/",
      "pubDate": "2025-09-16T17:00:00.000Z",
      "source": "whats-new",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "lex",
        "ga"
      ]
    },
    {
      "id": "aws-news-26d7a1c18ba9",
      "title": "New fault action in AWS FIS to inject I/O latency on Amazon EBS volumes",
      "description": "Today, Amazon EBS announced a new latency injection action in AWS Fault Injection Service (FIS), a fully managed service for running fault injection experiments. You can now use this action to inject I/O latency on your volumes as part of a controlled testing experiment to understand how your mission-critical applications respond to storage faults. With the new fault action, you can test your architecture against elevated storage latency, allowing you to observe application behavior and fine-tune your monitoring and recovery processes to ensure high availability.\n  EBS volumes are designed to meet the needs of highly available, latency-sensitive applications such as Oracle, SAP HANA, and Microsoft SQL Server. The latency injection action simulates degraded I/O performance on your volume to replicate the real-world signals, such as Amazon CloudWatch alarms and operating system timeouts, that occur during storage performance issues. Using this action, you can build confidence that your application can withstand and quickly recover from disruptions that cause high I/O latency on your EBS volume. To get started, you can directly use the pre-defined latency injection experiment templates available in the EBS and FIS consoles. Alternatively, you can customize these experiment templates or create your own experiment templates to meet your application-specific testing needs. You can integrate these latency injection experiments into your existing chaos engineering tests, continuous integration, and release testing, as well as combine multiple FIS actions in one experiment.\n  This new action is available in all AWS Regions where AWS FIS is available. To learn more, visit the EBS FIS actions user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-fis-action-inject-io-latency-on-ebs/",
      "pubDate": "2025-09-16T17:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "integration"
      ]
    },
    {
      "id": "aws-news-15d87cb93331",
      "title": "Streamline access to ISO-rating content changes with Verisk rating insights and Amazon Bedrock",
      "description": "In this post, we dive into how Verisk Rating Insights, powered by Amazon Bedrock, large language models (LLM), and Retrieval Augmented Generation (RAG), is transforming the way customers interact with and access ISO ERC changes.",
      "link": "https://aws.amazon.com/blogs/machine-learning/streamline-access-to-iso-rating-content-changes-with-verisk-rating-insights-and-amazon-bedrock/",
      "pubDate": "2025-09-16T16:43:42.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-431447666038",
      "title": "Unified multimodal access layer for Quora’s Poe using Amazon Bedrock",
      "description": "In this post, we explore how the AWS Generative AI Innovation Center and Quora collaborated to build a unified wrapper API framework that dramatically accelerates the deployment of Amazon Bedrock FMs on Quora’s Poe system. We detail the technical architecture that bridges Poe’s event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, demonstrate how a template-based configuration system reduced deployment time from days to 15 minutes, and share implementation patterns for protocol translation, error handling, and multi-modal capabilities.",
      "link": "https://aws.amazon.com/blogs/machine-learning/unified-multimodal-access-layer-for-quoras-poe-using-amazon-bedrock/",
      "pubDate": "2025-09-16T16:40:11.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "nova"
      ]
    },
    {
      "id": "aws-news-7f6d11754731",
      "title": "Amazon EC2 supports detailed performance stats on all NVMe local volumes",
      "description": "Today, Amazon announced the availability of detailed performance statistics for Amazon EC2 instance store NVMe volumes. This new capability delivers real-time visibility into the performance of your AWS Nitro System-based EC2 instance store NVMe volumes, making it easier to monitor storage health and quickly resolve application performance issues.\n \nWith EC2 detailed performance statistics, you can access 11 comprehensive metrics at one second granularity to monitor input/output (I/O) statistics of your locally attached NVMe volumes, including queue length measurements, IOPS, throughput, and detailed I/O latency histograms. These metrics are similar to the detailed performance statistics available for EBS volumes, providing a consistent monitoring experience across both storage types. The granular visibility provided by these metrics helps you identify specific workloads affected by performance variations, and optimize your application's IO patterns for maximum efficiency. Additionally, the metrics include latency histograms broken down by IO size, providing even more detailed insights into performance patterns.\n  Detailed performance statistics for EC2 instance store NVMe volumes are available by default for all Nitro-based EC2 instances with locally attached NVMe volumes across all AWS Commercial and China Regions, at no additional charge.\n \nTo learn more about the EC2 instance store NVMe detailed performance statistics and how to access them, please visit the documentation here.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-detailed-performance-stats-nvme-local-volumes/",
      "pubDate": "2025-09-16T16:30:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-caf42165b637",
      "title": "AWS Storage Gateway now supports IPv6",
      "description": "AWS Storage Gateway announces Internet Protocol version 6 (IPv6) support for AWS Storage Gateway endpoints, APIs, and gateway appliance interfaces. This enhancement enables both IPv6 and IPv4 access to our new dual-stack endpoints. The existing AWS Storage Gateway endpoints supporting IPv4 only will remain available for backwards compatibility.\n  AWS Storage Gateway provides on-premises access to data stored in AWS storage. With this launch, customers can standardize their applications and workflows for managing their AWS Storage Gateway resources on IPv6 while maintaining backward compatibility with IPv4 clients. By using the new dual-stack capabilities in the Storage Gateway appliances, service endpoints, and APIs, customers can transition from IPv4 to IPv6 gradually without needed to switch all their networking at once.\n  AWS Storage Gateway support for IPv6 is available in all AWS Regions where the service is offered. To learn more, visit the AWS Storage Gateway user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-storage-gateway-ipv6",
      "pubDate": "2025-09-16T14:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "ga",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-5429585a0e6f",
      "title": "AWS Transfer Family is now available in AWS Asia Pacific (Taipei) region",
      "description": "Customers in AWS Asia Pacific (Taipei) Region can now use AWS Transfer Family for file transfers over Secure File Transfer Protocol (SFTP), File Transfer Protocol (FTP), FTP over SSL (FTPS) and Applicability Statement 2 (AS2).\n  AWS Transfer Family provides fully managed file transfers for Amazon Simple Storage Service (Amazon S3) and Amazon Elastic File System (Amazon EFS) over SFTP, FTP, FTPS and AS2 protocols. In addition to file transfers, Transfer Family enables common file processing and event-driven automation for managed file transfer (MFT) workflows, helping customers to modernize and migrate their business-to-business file transfers to AWS.\n  To learn more about AWS Transfer Family, visit our product page and user-guide. See the AWS Region Table for complete regional availability information.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-transfer-family-asia-pacific-taipei-region/",
      "pubDate": "2025-09-16T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "now-available"
      ]
    },
    {
      "id": "aws-news-98a88cfc16cc",
      "title": "Amazon OpenSearch Service announces Derived Source for storage optimization",
      "description": "Amazon OpenSearch Service introduces support for Derived Source, a new feature that can help reduce the amount of storage required for your OpenSearch Service domains. With derived source support, you can skip storing source fields and dynamically derive them when required. \n  OpenSearch stores each ingested document in the _source field and also indexes individual fields for search. The _source field can consume significant storage space. To reduce storage use, you can configure OpenSearch to skip storing the _source field and instead reconstruct it dynamically when needed, for example, during search, get, mget, reindex, or update operations.\n  Derived Source is available in all regions where OpenSearch 3.1 is supported. The feature is opt-in and can be enabled at index creation using composite index settings.\n  Please refer to the AWS Regional Services List for more information about Amazon OpenSearch Service availability. To learn more about Derived Source, see the OpenSearch documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-opensearch-derived-source/",
      "pubDate": "2025-09-16T04:30:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "new-feature",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-4bc44e9f4d66",
      "title": "Amazon S3 Batch Operations now supports managing buckets or prefixes in a single step in AWS Management Console",
      "description": "Amazon S3 Batch Operations now supports managing objects within an S3 bucket, prefix, suffix, or more, in a single step in AWS Management Console. When creating an S3 Batch Operation, customers can specify the objects on which to perform the operation. With this feature, you have the option to instead specify an entire bucket, prefix, suffix, creation date, or storage class. Amazon S3 Batch Operations will then quickly apply the operation to all the matching objects and notify you when the job completes.\n \nS3 Batch Operations lets you easily perform one-time or recurring batch workloads such as copying objects between staging and production buckets, restoring archived backups from S3 Glacier storage classes, or computing objects checksum to verify the content of stored datasets, at any scale. After starting your job, S3 Batch Operations automatically processes all of the objects that match your filtering criteria. You will receive a detailed completion report with the status of each object once the job completes.\n  This feature of S3 Batch Operations is available in all AWS Regions. You can get started through AWS Management Console, AWS Command Line Interface (CLI), or the AWS Software Development Kit (SDK) client. For pricing information, please visit the Management & Insights tab of the Amazon S3 pricing page. To learn more about S3 Batch Operations, visit the S3 User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-s3-batch-operations-managing-buckets-console",
      "pubDate": "2025-09-15T21:30:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "support"
      ]
    },
    {
      "id": "aws-news-bd724f32f406",
      "title": "Amazon SageMaker HyperPod announces health monitoring agent support for Slurm clusters",
      "description": "Today, Amazon SageMaker HyperPod announces the general availability of the health monitoring agent for Slurm clusters. SageMaker HyperPod helps you provision resilient clusters for running machine learning (ML) workloads and developing state-of-the-art models such as large language models (LLMs), diffusion models, and foundation models (FMs). The health monitoring agent performs passive, background health checks of instances to identify problems in key areas without impact on application behavior or performance, flags failures instantly, and replaces any unhealthy instances to keep your training jobs running smoothly. \n \nThe agent runs continuously on all GPU- or Trainium-based nodes in your HyperPod cluster, watching for hardware issues such as unresponsive GPUs or NVLink error counters. When a fault is detected, it marks the node as unhealthy and automatically reboots or replaces it with a healthy node, keeping your jobs running without requiring manual intervention. The agent also follows a co-ordinated approach to handling failures with the job auto-resume functionality available with Slurm clusters. For example, jobs with auto-resume enabled will continue from the last saved checkpoint once nodes are replaced by the agent. This hands-free recovery—already available on HyperPod clusters orchestrated with Amazon EKS—now gives Slurm clusters the same resilient environment, helping teams train large models for weeks without disruption and reclaim time and costs that would otherwise be lost to mid-run failures. In addition, customers can now also reboot their nodes using a simple command in case of intermittent issues such as GPU driver issues requiring reset. \n \nHealth monitoring agent for Slurm is available in all regions where HyperPod is generally available. The agent is auto-enabled on all newly created Slurm clusters; to enable it on an existing cluster, simply upgrade to the latest HyperPod AMI by calling the UpdateClusterSoftware API. To learn more, visit the Amazon SageMaker HyperPod documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-sagemaker-hyperpod-health-monitoring-agent-slurm/",
      "pubDate": "2025-09-15T18:00:00.000Z",
      "source": "whats-new",
      "services": [
        "sagemaker",
        "hyperpod",
        "trainium"
      ],
      "categories": [
        "generative-ai",
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "trainium",
        "generally-available",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-591c4b0afb89",
      "title": "How msg enhanced HR workforce transformation with Amazon Bedrock and msg.ProfileMap",
      "description": "In this post, we share how msg automated data harmonization for msg.ProfileMap, using Amazon Bedrock to power its large language model (LLM)-driven data enrichment workflows, resulting in higher accuracy in HR concept matching, reduced manual workload, and improved alignment with compliance requirements under the EU AI Act and GDPR.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-msg-enhanced-hr-workforce-transformation-with-amazon-bedrock-and-msg-profilemap/",
      "pubDate": "2025-09-15T17:05:18.000Z",
      "source": "ml-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-94338f4e0c14",
      "title": "AWS Weekly Roundup: Strands Agents 1M+ downloads, Cloud Club Captain, AI Agent Hackathon, and more (September 15, 2025)",
      "description": "Last week, Strands Agents, AWS open source for agentic AI SDK just hit 1 million downloads and earned 3,000+ GitHub Stars less than 4 months since launching as a preview in May 2025. With Strands Agents, you can build production-ready, multi-agent AI systems in a few lines of code. We’ve continuously improved features including support […]",
      "link": "https://aws.amazon.com/blogs/aws/aws-weekly-roundup-strands-agents-1m-downloads-cloud-club-captain-ai-agent-hackathon-and-more-september-15-2025/",
      "pubDate": "2025-09-15T16:45:14.000Z",
      "source": "news-blog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "preview",
        "support"
      ]
    },
    {
      "id": "aws-news-8fb81cf48b84",
      "title": "Amazon OpenSearch Service now supports OpenSearch version 3.1",
      "description": "You can now run OpenSearch version 3.1 in Amazon OpenSearch Service. OpenSearch 3.1 introduces several improvements in areas like search relevance and performance, and introduces features that simplify development of vector-driven applications for generative AI workloads.\n \nThis launch incorporates Lucene 10 that enables optimized vector field indexing resulting in faster indexing times and reduced index sizes, sparse indexing for CPU and storage efficiency improvements, and vector quantization to reduce memory usage. Other key areas of improvement include improved range query performance, which benefits log analytics and time-series workloads, and reduced latency for high-cardinality aggregations.\n \nThis launch also introduces a new Search Relevance Workbench, which provides integrated tools for teams to evaluate and optimize search quality through experimentation. Additionally, this launch includes several improvements in vector search capabilities. First, Z-score normalization improves hybrid search reliability by reducing the impact of outliers and different score scales. Finally, you can now boost efficiency of searches using memory-optimized search that enables the Faiss engine to operate efficiently by memory-mapping the index file and using the operating system's file cache to serve search requests.\n \nFor information on upgrading to OpenSearch 3.1, please see the documentation. OpenSearch 3.1 is now available in all AWS Regions where Amazon OpenSearch Service is available.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-opensearch-service-opensearch-version-3-1/",
      "pubDate": "2025-09-15T14:30:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai",
        "search"
      ],
      "tags": [
        "launch",
        "ga",
        "now-available",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-217d740871a7",
      "title": "Announcing on-demand deployment for custom Meta Llama models in Amazon Bedrock",
      "description": "Starting today, customers can use the on-demand deployment option in Amazon Bedrock for their Meta Llama 3.3 models that have been fine-tuned or distilled in Bedrock. Models customized on or after September 15, 2025 will be eligible.\n  This enables Bedrock customers to reduce costs by processing requests in real time without requiring pre-provisioned compute resources. Customers only pay for what they use, eliminating the need for an always-on infrastructure.\n  Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models from leading AI companies via a single API. Amazon Bedrock also provides a broad set of capabilities customers need to build generative AI applications with security, privacy, and responsible AI built in.\n  To get started, visit documentation here.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/on-demand-deployment-custom-meta-llama-models-amazon-bedrock",
      "pubDate": "2025-09-15T14:00:00.000Z",
      "source": "whats-new",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-339b3a3ddf03",
      "title": "Amazon Managed Service for Prometheus now available in 11 additional AWS Regions",
      "description": "Amazon Managed Service for Prometheus is now available in Asia Pacific (Jakarta), Asia Pacific (Hyderabad), Asia Pacific (Osaka), Asia Pacific (Melbourne), Asia Pacific (Taipei), Canada West (Calgary), Europe (Spain), Israel (Tel Aviv), Mexico (Central), Middle East (Bahrain), and US West (N. California). Amazon Managed Service for Prometheus is a fully managed Prometheus-compatible monitoring service that makes it easy to monitor and alarm on operational metrics at scale.\n \nThe list of all supported regions where Amazon Managed Service for Prometheus is generally available can be found in the user guide. Customers can send up to 1 billion active metrics to a single workspace and can create multiple workspaces per account, where a workspace is a logical space dedicated to the storage and querying of Prometheus metrics.\n \nTo learn more about Amazon Managed Service for Prometheus, visit the user guide or product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-managed-service-prometheus-11-regions/",
      "pubDate": "2025-09-15T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "generally-available",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-2ed9629f3124",
      "title": "Amazon ECS Service Connect adds support for cross-account workloads",
      "description": "Amazon ECS Service Connect now supports seamless communication between services residing in different AWS accounts through integration with AWS Resource Access Manager (AWS RAM). This enhancement simplifies resource sharing, reduces duplication, and promotes consistent service-to-service communication across environments for organizations with multi-account architectures.\n  Amazon ECS Service Connect leverages AWS Cloud Map namespaces for storing information about ECS services and tasks. To enable seamless cross-account communication between Amazon ECS Service Connect services, you can now share the underlying AWS Cloud Map namespaces using AWS RAM with individual AWS accounts, specific Organizational Units (OUs), or your entire AWS Organization. To get started, create a resource share in AWS RAM, add the namespaces you want to share, and specify the principals (accounts, OUs, or the organization) that should have access. This enables platform engineers to use the same namespace to register Amazon ECS Service Connect services residing in multiple AWS accounts, simplifying service discovery and connectivity. Application developers can then build services that rely on a consistent, shared registry without worrying about availability or synchronization across accounts. Cross-account connectivity support improves operational efficiency and makes it easier to scale Amazon ECS workloads as your organization grows by reducing duplication and streamlining access to common services.\n \nThis feature is available with both Fargate and EC2 launch modes now in all commercial AWS Regions via the AWS Management Console, API, SDK, CLI, and CloudFormation. To learn more, please refer to the Amazon ECS Service Connect documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ecs-service-connect-cross-account-workloads",
      "pubDate": "2025-09-12T19:32:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "ga",
        "enhancement",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-79795044339d",
      "title": "Automate advanced agentic RAG pipeline with Amazon SageMaker AI",
      "description": "In this post, we walk through how to streamline your RAG development lifecycle from experimentation to automation, helping you operationalize your RAG solution for production deployments with Amazon SageMaker AI, helping your team experiment efficiently, collaborate effectively, and drive continuous improvement.",
      "link": "https://aws.amazon.com/blogs/machine-learning/automate-advanced-agentic-rag-pipeline-with-amazon-sagemaker-ai/",
      "pubDate": "2025-09-12T17:36:19.000Z",
      "source": "ml-blog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "generative-ai",
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "improvement"
      ]
    },
    {
      "id": "aws-news-3aaaf4df1897",
      "title": "Announcing Amazon EC2 M4 and M4 Pro Mac instances",
      "description": "AWS has launched new EC2 M4 and M4 Pro Mac instances based on Apple M4 Mac mini, offering improved performance over previous generations and featuring up to 48GB memory and 2TB storage for iOS/macOS development workloads.",
      "link": "https://aws.amazon.com/blogs/aws/announcing-amazon-ec2-m4-and-m4-pro-mac-instances/",
      "pubDate": "2025-09-12T16:30:25.000Z",
      "source": "news-blog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch"
      ]
    },
    {
      "id": "aws-news-1d6dabcf2cd0",
      "title": "Announcing general availability of Amazon EC2 M4 and M4 Pro Mac instances",
      "description": "Starting today, Amazon Elastic Compute Cloud (Amazon EC2) M4 and M4 Pro Mac instances are now generally available (GA). M4 Mac instances offer up to 20% better application build performance compared to M2 Mac instances, while M4 Pro Mac instances deliver up to 15% better application build performance compared to M2 Pro Mac instances. These instances are ideal for building and testing applications for Apple platforms such as iOS, macOS, iPadOS, tvOS, watchOS, visionOS, and Safari.\n  M4 and M4 Pro Mac instances are powered by the AWS Nitro System, providing up to 10 Gbps network bandwidth and 8 Gbps of Amazon Elastic Block Store (Amazon EBS) storage bandwidth. M4 Mac instances are built on Apple M4 Mac Mini computers featuring 10‑core CPU, 10‑core GPU, 24GB unified memory, and 16‑core Neural Engine. M4 Pro Mac instances feature a 14‑core CPU, 20‑core GPU, 48GB unified memory, and 16‑core Neural Engine. Both instance families come with a new 2TB instance store volume per EC2 Mac Dedicated Host, providing low latency storage for improved caching and build/test performance.\n  M4 and M4 Pro Mac instances enable Apple developers to migrate their most demanding build and test workloads onto AWS and run significantly more tests in parallel using multiple Xcode simulators. This accelerates application iterations and reduces time to market. Customers now have access to the most advanced Apple silicon Macs on AWS to meet their requirements, while also enabling them to modernize their Apple CI/CD with dozens of AWS services. M4 and M4 Pro Mac instances support macOS Sequoia version 15.6 and newer AMIs (Amazon Machine Images).\n \nAmazon EC2 M4 and M4 Pro Mac instances are available in US East (N. Virginia) and US West (Oregon). To learn more or get started, see our launch blog, Amazon EC2 Mac Instances or visit the EC2 Mac documentation reference.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-m4-pro-mac-instances-generally-available",
      "pubDate": "2025-09-12T15:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "generally-available",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-c1f00f103971",
      "title": "Amazon SageMaker notebooks now support P6-B200 instance type",
      "description": "We are pleased to announce general availability of Amazon EC2 P6-B200 instances on SageMaker notebooks.\n  Amazon EC2 P6-B200 instances are powered by 8 NVIDIA Blackwell GPUs with 1440 GB of high-bandwidth GPU memory and 5th Generation Intel Xeon processors (Emerald Rapids). These instances deliver up to 2x better performance compared to P5en instances for AI training. Customers can use P6-B200 instances to interactively develop and fine-tune large foundation models, including LLMs, mixture of experts models, and multi-modal reasoning models. These instances enable efficient experimentation with larger models directly in JupyterLab or CodeEditor environments for generative AI applications such as enterprise copilots and content generation across text, images, and video.\n  Amazon EC2 P6-B200 instances are available for SageMaker notebooks in the AWS US East (Ohio) and US West (Oregon) regions.\n  Visit developer guides for instructions on setting up and using JupyterLab and CodeEditor applications on SageMaker Studio and SageMaker notebook instances.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-sagemaker-notebooks-p6-b200-instance-type",
      "pubDate": "2025-09-12T14:00:00.000Z",
      "source": "whats-new",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "generative-ai",
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "support"
      ]
    },
    {
      "id": "aws-news-5f7dcd4b3fbc",
      "title": "Amazon SageMaker Unified Studio supports remote connection from VS Code",
      "description": "Today, AWS announces remote connection from Visual Studio Code (VS Code) to Amazon SageMaker Unified Studio. This new capability allows developers to leverage their VS Code setup while accessing the scalable compute resources of Amazon SageMaker. By connecting VS Code to SageMaker Unified Studio, you can maintain your existing development workflows and configurations within a unified environment for AWS analytics and AI/ML services.\n  SageMaker Unified Studio, part of the next generation of Amazon SageMaker, offers a broad set of fully managed cloud interactive development environments (IDE), including JupyterLab and Code Editor based on Code-OSS (Open Source Software) like VS Code. Starting today, you can use your customized local VS Code setup while accessing your compute resources and data in Amazon SageMaker. Authentication is simple and secure using the AWS Toolkit extension in VS Code. This integration provides a streamlined path from your local development environment to scalable infrastructure for running data processing, SQL analytics, and ML workflows.\n  This feature is available in all Regions where Amazon SageMaker Unified Studio is available. To learn more, refer to the Administrator Guide and User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/sagemaker-unified-studio-vs-code/",
      "pubDate": "2025-09-12T07:00:00.000Z",
      "source": "whats-new",
      "services": [
        "sagemaker",
        "unified studio"
      ],
      "categories": [
        "generative-ai",
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "unified studio",
        "integration",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-17539d332bd2",
      "title": "Amazon RDS Proxy announces support for end-to-end IAM authentication",
      "description": "Amazon Relational Database Service (RDS) Proxy now supports end-to-end IAM authentication for connections to Amazon Aurora and RDS database instances. This feature allows you to connect from your applications to your databases through RDS Proxy using AWS Identity and Access Management (IAM) authentication. End-to-end IAM authentication simplifies credential management, reduces credential rotation overhead, and enables you to leverage IAM's robust authentication and authorization capabilities throughout your database connection path.\n  With end-to-end IAM authentication, you can now connect to your databases through RDS Proxy without needing to register or store credentials in Secrets Manager. End-to-end IAM authentication is available for MySQL and PostgreSQL database engines in all AWS Regions where RDS Proxy is supported.\n  Many applications, including those built on modern serverless architectures, may need to have a high number of open connections to the database or may frequently open and close database connections, exhausting the database memory and compute resources. Amazon RDS Proxy allows applications to pool and share database connections, improving your database efficiency and application scalability. RDS Proxy helps improve application scalability, resiliency, and security.\n  For information on supported database engine versions and regional availability of RDS Proxy, refer to our RDS and Aurora documentations.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-rds-proxy-end-to-end-iam-authentication/",
      "pubDate": "2025-09-12T07:00:00.000Z",
      "source": "whats-new",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "support"
      ]
    },
    {
      "id": "aws-news-60dbc3b49098",
      "title": "AWS Weekly Roundup: Amazon EC2, Amazon Q Developer, IPv6 updates, and more (September 1, 2025)",
      "description": "My LinkedIn feed was absolutely packed this week with pictures from the AWS Heroes Summit event in Seattle. It was heartwarming to see so many familiar faces and new Heroes coming together. For those not familiar with the AWS Heroes program, it’s a global community recognition initiative that honors individuals who make outstanding contributions to […]",
      "link": "https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-ec2-amazon-q-developer-ipv6-updates-and-more-september-1-2025/",
      "pubDate": "2025-09-01T12:59:02.000Z",
      "source": "news-blog",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer",
        "update"
      ]
    },
    {
      "id": "aws-news-9388e1a4988e",
      "title": "AWS Weekly Roundup: Amazon Aurora 10th anniversary, Amazon EC2 R8 instances, Amazon Bedrock and more (August 25, 2025)",
      "description": "As I was preparing for this week’s roundup, I couldn’t help but reflect on how database technology has evolved over the past decade. It’s fascinating to see how architectural decisions made years ago continue to shape the way we build modern applications. This week brings a special milestone that perfectly captures this evolution in cloud […]",
      "link": "https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-aurora-10th-anniversary-amazon-ec2-r8-instances-amazon-bedrock-and-more-august-25-2025/",
      "pubDate": "2025-08-25T16:30:43.000Z",
      "source": "news-blog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-0b47e55e58e4",
      "title": "AWS named as a Leader in 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services for 15 years in a row",
      "description": "AWS is recognized as a Leader in the 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services for the fifteenth consecutive year. In the report, Gartner once again placed AWS highest on the “Ability to Execute” axis. We believe this reflects our ongoing commitment to giving customers the broadest and deepest set of capabilities to accelerate innovation as well as unparalleled security, reliability, and performance they can trust for their most critical applications.",
      "link": "https://aws.amazon.com/blogs/aws/aws-named-as-a-leader-in-2025-gartner-magic-quadrant-for-strategic-cloud-platform-services-for-15-years-in-a-row/",
      "pubDate": "2025-08-15T16:59:11.000Z",
      "source": "news-blog",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "ga"
      ]
    },
    {
      "id": "aws-news-d2be49d3bcd1",
      "title": "Celebrating 10 years of Amazon Aurora innovation",
      "description": "Amazon Aurora is celebrating its 10th anniversary with a livestream event on August 21, 2025, highlighting a decade of database innovation since its groundbreaking architecture that decoupled storage from compute.",
      "link": "https://aws.amazon.com/blogs/aws/celebrating-10-years-of-amazon-aurora-innovation/",
      "pubDate": "2025-08-15T16:01:02.000Z",
      "source": "news-blog",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova"
      ]
    }
  ]
}