{
  "lastUpdated": "2026-02-20T06:40:44.081Z",
  "category": "generative-ai",
  "totalItems": 58,
  "items": [
    {
      "id": "aws-news-86cc833a7d39",
      "title": "AWS IAM Identity Center is now available in the Asia Pacific (New Zealand) AWS Region",
      "description": "You can now deploy AWS IAM Identity Center in 38 AWS Regions, including Asia Pacific (New Zealand).\n  IAM Identity Center is the recommended service for managing workforce access to AWS applications. It enables you to connect your existing source of workforce identities to AWS once and offer your users single sign on experience across AWS. It powers the personalized experiences offered by AWS applications, such as Amazon Q, and the ability to define and audit user-aware access to data in AWS services, such as Amazon Redshift. It can also help you manage access to multiple AWS accounts from a central place. IAM Identity Center is available at no additional cost in these AWS Regions.\n  To learn more about IAM Identity Center, visit the product detail page. To get started, see the IAM Identity Center user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-iam-identity-center-asia-pacific-new-zealand-region/",
      "pubDate": "2026-02-19T22:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q",
        "personalize",
        "redshift",
        "iam",
        "iam identity center"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "personalize",
        "redshift",
        "iam",
        "iam identity center",
        "now-available"
      ]
    },
    {
      "id": "aws-news-e40d7347f41b",
      "title": "Amazon EC2 G7e instances now available in Asia Pacific (Tokyo) region",
      "description": "Starting today, Amazon EC2 G7e instances accelerated by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs are now available in  Asia Pacific (Tokyo) region. G7e instances offer up to 2.3x inference performance compared to G6e.\n \nCustomers can use G7e instances to deploy large language models (LLMs), agentic AI models, multimodal generative AI models, and physical AI models. G7e instances offer the highest performance for spatial computing workloads as well as workloads that require both graphics and AI processing capabilities. G7e instances feature up to 8 NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, with 96 GB of memory per GPU, and 5th Generation Intel Xeon processors. They support up to 192 virtual CPUs (vCPUs) and up to 1600 Gbps of networking bandwidth. G7e instances support NVIDIA GPUDirect Peer to Peer (P2P) that boosts performance for multi-GPU workloads. Multi-GPU G7e instances also support NVIDIA GPUDirect Remote Direct Memory Access (RDMA) with EFA in EC2 UltraClusters, reducing latency for small-scale multi-node workloads.\n \nYou can use G7e instances for Amazon EC2 in the following AWS Regions: US West (Oregon), US East (N. Virginia, Ohio) and Asia Pacific (Tokyo). You can purchase G7e instances as On-Demand Instances, Spot Instances, or as part of Savings Plans.\n \nTo get started, visit the AWS Management Console, AWS Command Line Interface (CLI), and AWS SDKs. To learn more, visit G7e instances.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-g7e-instances-tokyo-region/",
      "pubDate": "2026-02-19T19:11:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-f2a2ebebbf85",
      "title": "Amazon MQ now supports ActiveMQ minor version 5.19",
      "description": "Amazon MQ now supports ActiveMQ minor version 5.19, which introduces several improvements and fixes compared to the previous version of ActiveMQ supported by Amazon MQ. Amazon MQ manages the patch version upgrades for your brokers. All brokers on ActiveMQ version 5.19 will be automatically upgraded to the next compatible and secure patch version in your scheduled maintenance window.\n \nIf you are utilizing prior versions of ActiveMQ, such as 5.18, we strongly recommend you to upgrade to ActiveMQ 5.19. You can easily perform this upgrade with just a few clicks in the AWS Management Console. To learn more about upgrading, consult the ActiveMQ Version Management section in the Amazon MQ Developer Guide. To learn more about the changes in ActiveMQ 5.19, see the Amazon MQ release notes. This version is available across all AWS Regions where Amazon MQ is available.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-mq-activemq-5-19/",
      "pubDate": "2026-02-19T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "q developer",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-ee449f2b77f8",
      "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
      "description": "In this post, we explain how you can use the Flyte Python SDK to orchestrate and scale AI/ML workflows. We explore how the Union.ai 2.0 system enables deployment of Flyte on Amazon Elastic Kubernetes Service (Amazon EKS), integrating seamlessly with AWS services like Amazon Simple Storage Service (Amazon S3), Amazon Aurora, AWS Identity and Access Management (IAM), and Amazon CloudWatch. We explore the solution through an AI workflow example, using the new Amazon S3 Vectors service.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte/",
      "pubDate": "2026-02-19T16:28:21.000Z",
      "source": "mlBlog",
      "services": [
        "s3 vectors",
        "s3",
        "eks",
        "iam",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3 vectors",
        "s3",
        "eks",
        "iam",
        "cloudwatch"
      ]
    },
    {
      "id": "aws-news-edadc2bb22a2",
      "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
      "description": "In this blog post, we will guide you through establishing data source connectivity between Amazon Quick Sight and Snowflake through secure key pair authentication.",
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source/",
      "pubDate": "2026-02-19T16:06:41.000Z",
      "source": "mlBlog",
      "services": [
        "amazon q"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "support"
      ]
    },
    {
      "id": "aws-news-08e9f45f3f3c",
      "title": "Build unified intelligence with Amazon Bedrock AgentCore",
      "description": "In this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of the Customer Agent and Knowledge Engine (CAKE).",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore/",
      "pubDate": "2026-02-18T23:54:29.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore"
      ]
    },
    {
      "id": "aws-news-91f61eda3045",
      "title": "How CyberArk uses Apache Iceberg and Amazon Bedrock to deliver up to 4x support productivity",
      "description": "CyberArk is a global leader in identity security. Centered on intelligent privilege controls, it provides comprehensive security for human, machine, and AI identities across business applications, distributed workforces, and hybrid cloud environments. In this post, we show you how CyberArk redesigned their support operations by combining Iceberg’s intelligent metadata management with AI-powered automation from Amazon Bedrock. You’ll learn how to simplify data processing flows, automate log parsing for diverse formats, and build autonomous investigation workflows that scale automatically.",
      "link": "https://aws.amazon.com/blogs/big-data/how-cyberark-uses-apache-iceberg-and-amazon-bedrock-to-deliver-up-to-4x-support-productivity/",
      "pubDate": "2026-02-18T20:17:24.000Z",
      "source": "bigDataBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-2aad6a140c91",
      "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
      "description": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with Amazon use case-specific evaluation approaches and metrics.",
      "link": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon/",
      "pubDate": "2026-02-18T19:21:28.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex"
      ]
    },
    {
      "id": "aws-news-679794ccdee2",
      "title": "Amazon Aurora DSQL now integrates with Kiro powers and AI agent skills",
      "description": "Today, AWS announces Amazon Aurora DSQL integration with Kiro powers and AI agent skills, enabling developers to build Aurora DSQL-backed applications faster with AI agent-assisted development. These integrations bundle the Aurora DSQL Model Context Protocol (MCP) server with development best practices, so AI agents can help you with Aurora DSQL schema design, performance optimization, and database operations out of the box.\n  Kiro powers is a registry of curated and pre-packaged MCP servers, steering files, and agent hooks to accelerate specialized software development and deployment use cases. With the Kiro power for Aurora DSQL, agents have instant access to specialized knowledge, so developers can work confidently without any prior context, reducing trial-and-error development cycles. The power is available within the Kiro IDE for one-click installation.\n  The Aurora DSQL skill extends the same capabilities to additional AI coding agents through the Skills CLI. Developers can install the skill with a single command and select their preferred agents including Kiro CLI, Claude Code, Gemini, Codex, Cursor, Copilot, Cline, Windsurf, Roo, OpenCode, and more. When developers work on database tasks, the agent dynamically loads relevant skill guidance, including Aurora DSQL Postgres-compatible SQL patterns, distributed database design, and IAM authentication, eliminating the need to repeatedly provide the same context across conversations. As Aurora DSQL adds new features, future skill releases will include updated patterns and guidance, ensuring that agents always have current best practices.\n \nFor more information on the Aurora DSQL Kiro power and agent skills, visit the Aurora DSQL steering documentation and GitHub page. Get started with Aurora DSQL for free with the AWS Free Tier.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-aurora-dsql-integrates-with-kiro-powers-and-agent-skills",
      "pubDate": "2026-02-18T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "iam"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "iam",
        "new-feature",
        "update",
        "integration"
      ]
    },
    {
      "id": "aws-news-4c5f420d9248",
      "title": "Amazon OpenSearch Service now supports storage optimized  i7i  instances",
      "description": "Amazon OpenSearch Service now supports latest generation x86 based high performance Storage Optimized i7i instances. Powered by 5th generation Intel Xeon Scalable processors, I7i instances deliver up to 23% better compute performance and more than 10% better price performance over previous generation I4i instances.\n  I7i instances have 3rd generation AWS Nitro SSDs with up to 50% better real-time storage performance, up to 50% lower storage I/O latency, and up to 60% lower storage I/O latency variability compared to I4i instances. Built on the AWS Nitro System, these instances oﬄoad CPU virtualization, storage, and networking functions to dedicated hardware and software enhancing the performance and security for your workloads.\n  Amazon OpenSearch Service supports i7i instances in following AWS Regions US East (N. Virginia, Ohio), US West (N. California, Oregon), Canada (Central), Canada West (Calgary), Europe (Frankfurt, Ireland, London, Milan, Spain, Stockholm, Zurich ), Africa (Cape Town), Asia Pacific (Hong Kong, Hyderabad, Jakarta, Malaysia, Melbourne, Mumbai, Osaka, Seoul, Singapore, Sydney, Tokyo), Middle East (UAE), South America (São Paulo) & AWS GovCloud (US-West).\n  For region specific availability & pricing, visit our pricing page. To learn more about Amazon OpenSearch Service and its capabilities, visit our product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-opensearch-service-supports-i7i-instances",
      "pubDate": "2026-02-18T04:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "opensearch",
        "opensearch service"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "opensearch",
        "opensearch service",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-fce7e3d09543",
      "title": "Amazon Connect now includes agent time-off requests in draft schedules",
      "description": "Amazon Connect now includes agent time-off requests in draft schedules, making it easier for you to view why an agent was not scheduled on a particular day or part of the day. For example, when generating schedules for next month, you can see that an agent who typically works Monday to Friday wasn't scheduled for the first week because they're on leave without needing to check the published schedules or troubleshooting configuration as to why agent was not scheduled. This launch helps schedulers quickly identify coverage gaps and adjust schedules before publishing them to agents.\n  This feature is available in all AWS Regions where Amazon Connect agent scheduling is available. To learn more about Amazon Connect agent scheduling, click here.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/02/amazon-connect-time-off-draft-schedules",
      "pubDate": "2026-02-17T18:50:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "launch",
        "ga"
      ]
    },
    {
      "id": "aws-news-8f5cee2fa771",
      "title": "Amazon Connect now supports multi-line text fields on case templates",
      "description": "Amazon Connect now supports larger, multi-line text fields on case templates allowing agents to capture detailed free-form notes and structured data directly within cases. These fields expand vertically to accommodate multiple paragraphs, making it easier to document root cause analysis, transaction details, investigation findings, or customer-facing updates.\n  Amazon Connect Cases is available in the following AWS regions: US East (N. Virginia), US West (Oregon), Canada (Central), Europe (Frankfurt), Europe (London), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), and Africa (Cape Town) AWS regions. To learn more and get started, visit the Amazon Connect Cases webpage and documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-connect-cases-multiline-text-fields/",
      "pubDate": "2026-02-17T17:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-a7709e940494",
      "title": "AWS HealthImaging launches additional metrics for monitoring data stores",
      "description": "AWS HealthImaging has launched additional metrics through Amazon CloudWatch that enable monitoring storage at the account and data store levels. These new metrics help customers better understand their medical imaging storage and growth trends over time.\n  HealthImaging now provides customers with granular CloudWatch metrics to monitor their data stores. Customers can track storage by volume, number of image sets, and the number of DICOM studies, series, and instances. These metrics provide the insights needed to manage both single-tenant and multi-tenant workloads at petabyte scale. To learn more, visit Using Amazon CloudWatch with HealthImaging.\n  AWS HealthImaging is a HIPAA-eligible service that empowers healthcare providers and their software partners to store, analyze, and share medical images. AWS HealthImaging is generally available in the following AWS Regions: US East (N. Virginia), US West (Oregon), Asia Pacific (Sydney), and Europe (Ireland).",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-healthimaging-additional-metrics/",
      "pubDate": "2026-02-16T22:38:00.000Z",
      "source": "whatsNew",
      "services": [
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "cloudwatch",
        "launch",
        "generally-available"
      ]
    },
    {
      "id": "aws-news-dc0ca48d2f86",
      "title": "Verisk cuts processing time and storage costs with Amazon Redshift and lakehouse",
      "description": "Verisk, a catastrophe modeling SaaS provider serving insurance and reinsurance companies worldwide, cut processing time from hours to minutes-level aggregations while reducing storage costs by implementing a lakehouse architecture with Amazon Redshift and Apache Iceberg. If you’re managing billions of catastrophe modeling records across hurricanes, earthquakes, and wildfires, this approach eliminates the traditional compute-versus-cost trade-off by separating storage from processing power. In this post, we examine Verisk’s lakehouse implementation, focusing on four architectural decisions that delivered measurable improvements.",
      "link": "https://aws.amazon.com/blogs/big-data/verisk-cuts-processing-time-and-storage-costs-with-amazon-redshift-and-lakehouse/",
      "pubDate": "2026-02-16T18:10:44.000Z",
      "source": "bigDataBlog",
      "services": [
        "redshift",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "redshift",
        "rds",
        "ga",
        "improvement"
      ]
    },
    {
      "id": "aws-news-1e249c19f8e8",
      "title": "Kiro is now available in AWS GovCloud (US) Regions",
      "description": "Kiro brings agentic AI development capabilities to workloads with elevated compliance needs in AWS GovCloud (US-East) and AWS GovCloud (US-West) Regions. Kiro is an agentic AI with an integrated development environment (IDE) and command-line interface (CLI) that helps you go from prototype to production with spec-driven development. From simple to complex tasks, Kiro works alongside you to turn prompts into detailed specs, then into working code, docs, and tests—so what you build is exactly what you want and ready to share with your team.\n  Kiro's agents help you solve challenging problems and automate tasks like generating documentation and unit tests. With native Model Context Protocol (MCP) support, Kiro connects to documentation, databases, APIs, and other enterprise resources, providing capability for mission-critical development workflows.\n  Kiro in AWS GovCloud (US) Regions uses enterprise authentication via AWS IAM Identity Center. To learn more about building with Kiro in AWS GovCloud (US), read the blog post. For more details about Kiro in AWS GovCloud (US), visit the GovCloud documentation or contact your AWS account team for more information. To learn more about Kiro, visit the Kiro product page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/kiro-launch-aws-govcloud-us/",
      "pubDate": "2026-02-16T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "ecs",
        "iam",
        "iam identity center"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "ecs",
        "iam",
        "iam identity center",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-9ac22c4b7599",
      "title": "Amazon EC2 supports nested virtualization on virtual Amazon EC2 instances",
      "description": "Starting today, customers can create nested environments within virtualized Amazon EC2 instances. Previously, customers could only create and manage virtual machines inside bare metal EC2 instances. With this launch, customers can create nested virtual machines by running KVM or Hyper-V on virtual EC2 instances. Customers can leverage this capability for use cases such as running emulators for mobile applications, simulating in-vehicle hardware for automobiles, and running Windows Subsystem for Linux on Windows workstations.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-nested-virtualization-on-virtual",
      "pubDate": "2026-02-16T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "launch",
        "support"
      ]
    },
    {
      "id": "aws-news-190fdcd4ca2e",
      "title": "Amazon EC2 High Memory U7i instances now available in additional regions",
      "description": "Amazon EC2 High Memory instances are now available in new regions - U7i-6tb.112xlarge instances in AWS South America (Sao Paulo) and Europe (Milan), U7i-12tb.224xlarge in AWS GovCloud (US-East), and U7in-16tb.224xlarge instances in Europe (London). U7i instances are part of AWS 7th generation and are powered by custom fourth generation Intel Xeon Scalable Processors (Sapphire Rapids). U7i-6tb instances offer 6TiB of DDR5 memory, U7i-12tb instances offer 12TiB of DDR5 memory, and U7in-16tb instances offer 16TiB of DDR5 memory, enabling customers to scale transaction processing throughput in a fast-growing data environment.\n  U7i-6tb instances offer 448 vCPUs and support up to 100Gbps Elastic Block Storage (EBS) and deliver up to 100Gbps of network bandwidth. U7i-12tb instances offer 896 vCPUs, support up to 100Gbps Elastic Block Storage (EBS) and deliver up to 100Gbps of network bandwidth. U7in-16tb instances offer 896 vCPUs, support up to 100Gbps Elastic Block Storage (EBS) and deliver up to 200Gbps of network bandwidth for faster data loading and backups. All U7i instances support ENA Express. \n \nU7i instances are ideal for customers using mission-critical in-memory databases like SAP HANA, Oracle, and SQL Server.\n  To learn more about U7i instances, visit the High Memory instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-highmem-instances-available/",
      "pubDate": "2026-02-13T23:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available",
        "support",
        "new-region"
      ]
    },
    {
      "id": "aws-news-354bc14bf132",
      "title": "Customize AI agent browsing with proxies, profiles, and extensions in Amazon Bedrock AgentCore Browser",
      "description": "Today, we are announcing three new capabilities that address these requirements: proxy configuration, browser profiles, and browser extensions. Together, these features give you fine-grained control over how your AI agents interact with the web. This post will walk through each capability with configuration examples and practical use cases to help you get started.",
      "link": "https://aws.amazon.com/blogs/machine-learning/customize-ai-agent-browsing-with-proxies-profiles-and-extensions-in-amazon-bedrock-agentcore-browser/",
      "pubDate": "2026-02-13T22:57:34.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore"
      ]
    },
    {
      "id": "aws-news-426c2cf99ac0",
      "title": "Amazon Connect launches in-app notifications to surface critical operational alerts to business users",
      "description": "Amazon Connect now supports in-app notifications in the workspace header, visible from any page, so your team can stay informed without interrupting their workflow— whether configuring, analyzing data, or servicing customers. A notification icon appears in the header of every workspace page, with a badge indicating unread messages. Click the icon to view messages, access relevant resources through embedded links, and manage read/unread status—all without navigating away from your current task. For example, if all supervisors need to complete a certain training by end of week, a notification can be published to non-compliant users to remind them.\n  The new notification APIs enable you to programmatically send targeted messages to specific audiences within your organization, ensuring teams stay aware of urgent updates, policy changes, and action items requiring immediate attention. Amazon Connect will also leverage this capability to deliver system updates and important announcements.\n  In-app notifications are available in all AWS regions where Amazon Connect is available and offer public API and AWS CloudFormation support. To learn more about in-app notifications, see the Amazon Connect Administrator Guide. To learn more about Amazon Connect, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-connect-in-app-notifications",
      "pubDate": "2026-02-13T17:20:00.000Z",
      "source": "whatsNew",
      "services": [
        "cloudformation"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "cloudformation",
        "launch",
        "ga",
        "update",
        "support",
        "announcement"
      ]
    },
    {
      "id": "aws-news-86a52dfe7164",
      "title": "Amazon Connect now provides real time AI-powered overviews and recommended next actions for Tasks",
      "description": "Amazon Connect now provides AI-powered Task overviews with suggested next actions so agents can understand work items faster and resolve them more quickly. For example, when an agent receives a Task to process a refund request submitted through an online form, Amazon Connect summarizes earlier activities such as verifying order details, checking return eligibility, and confirming the payment method, and then presents recommended next steps to complete the refund.\n  To enable this feature, add the Connect assistant flow block to your flows before a Task contact is assigned to your agent. You can guide the recommendations of your generative AI-powered Tasks assistant by adding knowledge bases.\n  This new feature is available in all AWS regions where Amazon Connect real time agent assistance is available. To learn more and get started, refer to the help documentation, pricing page, or visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/connect-tasks-ai-assistance",
      "pubDate": "2026-02-13T17:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "new-feature"
      ]
    },
    {
      "id": "aws-news-4ac2d5c3fee1",
      "title": "AI meets HR: Transforming talent acquisition with Amazon Bedrock",
      "description": "In this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, AWS Lambda, and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.",
      "link": "https://aws.amazon.com/blogs/machine-learning/ai-meets-hr-transforming-talent-acquisition-with-amazon-bedrock/",
      "pubDate": "2026-02-12T20:18:58.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "lambda"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "lambda"
      ]
    },
    {
      "id": "aws-news-5dc8f40dd93f",
      "title": "Build long-running MCP servers on Amazon Bedrock AgentCore with Strands Agents integration",
      "description": "In this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build production-ready AI agents that can handle complex, time-intensive operations reliably.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-long-running-mcp-servers-on-amazon-bedrock-agentcore-with-strands-agents-integration/",
      "pubDate": "2026-02-12T20:16:20.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex",
        "integration"
      ]
    },
    {
      "id": "aws-news-95cd2d9f146e",
      "title": "NVIDIA Nemotron 3 Nano 30B MoE model is now available in Amazon SageMaker JumpStart",
      "description": "Today we’re excited to announce that the NVIDIA Nemotron 3 Nano 30B model with  3B active parameters is now generally available in the Amazon SageMaker JumpStart model catalog. You can accelerate innovation and deliver tangible business value with Nemotron 3 Nano on Amazon Web Services (AWS) without having to manage model deployment complexities. You can power your generative AI applications with Nemotron capabilities using the managed deployment capabilities offered by SageMaker JumpStart.",
      "link": "https://aws.amazon.com/blogs/machine-learning/nvidia-nemotron-3-nano-30b-is-now-available-in-amazon-sagemaker-jumpstart/",
      "pubDate": "2026-02-11T19:38:47.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "sagemaker",
        "jumpstart",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "sagemaker",
        "jumpstart",
        "lex",
        "generally-available",
        "now-available"
      ]
    },
    {
      "id": "aws-news-e198bdd73bc5",
      "title": "AWS announces 6 new locations for AWS Data Transfer Terminal",
      "description": "AWS Data Transfer Terminal is now available in six additional locations in Seattle and Phoenix (US), London (UK), Paris (France), Sydney (Australia), and Tokyo (Japan), expanding availability alongside existing locations in San Francisco, Los Angeles, and New York City (US), and Munich (Germany). AWS Data Transfer Terminal is a secure, physical location where you can bring your storage devices and upload data to AWS including Amazon Simple Storage Service (Amazon S3), Amazon Elastic File System (Amazon EFS), and others using a high throughput network connection.\n  Data Transfer Terminals are ideal for customers who need to transfer large amounts of data to the AWS quickly and securely. Common use cases span various industries and applications, including video production data for processing in the media and entertainment industry, training data for Advanced Driver Assistance Systems (ADAS) in the automotive industry, migrating legacy data in the financial services industry, and uploading equipment sensor data in the industrial and agricultural sectors. Once uploaded, you can immediately leverage AWS services like Amazon Athena for analysis, Amazon SageMaker for machine learning, or Amazon Elastic Compute Cloud (Amazon EC2) for application development, reducing data processing time from weeks to minutes.\n  To learn more, visit the Data Transfer Terminal product page and documentation. To get started, make a reservation at your nearby Data Transfer Terminal in the AWS Console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-data-transfer-terminal-6-new-locations/",
      "pubDate": "2026-02-11T19:02:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "s3",
        "ec2",
        "eks",
        "athena"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "s3",
        "ec2",
        "eks",
        "athena",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-ddcbfc655623",
      "title": "Amazon RDS for MariaDB now supports community MariaDB minor versions 10.6.25, 10.11.16, 11.4.10, and 11.8.6",
      "description": "Amazon Relational Database Service (Amazon RDS) for MariaDB now supports community MariaDB minor versions 10.6.25, 10.11.16, 11.4.10, and 11.8.6. We recommend that you upgrade to the latest minor versions to fix known security vulnerabilities in prior versions of MariaDB, and to benefit from the bug fixes, performance improvements, and new functionality added by the MariaDB community.\n  You can leverage automatic minor version upgrades to automatically upgrade your databases to more recent minor versions during scheduled maintenance windows. You can also leverage Amazon RDS Managed Blue/Green deployments for safer, simpler, and faster updates to your MariaDB instances. Learn more about upgrading your database instances, including automatic minor version upgrades and Blue/Green Deployments, in the Amazon RDS User Guide.\n  Amazon RDS for MariaDB makes it straightforward to set up, operate, and scale MariaDB deployments in the cloud. Learn more about pricing details and regional availability at Amazon RDS for MariaDB. Create or update a fully managed Amazon RDS database in the Amazon RDS Management Console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-rds-mariadb-community-versions/",
      "pubDate": "2026-02-11T16:23:00.000Z",
      "source": "whatsNew",
      "services": [
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "rds",
        "update",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-3537a41f10b9",
      "title": "Amazon DocumentDB (with MongoDB compatibility) is Now Available in the Europe (Zurich) Region",
      "description": "Amazon DocumentDB (with MongoDB compatibility) is now available in the Europe (Zurich) region adding to the list of available regions where you can use Amazon DocumentDB.\n \nAmazon DocumentDB is a fully managed, native JSON database that makes it simple and cost-effective to operate critical document workloads at virtually any scale without managing infrastructure. Amazon DocumentDB is designed to give you the scalability and durability you need when operating mission-critical MongoDB workloads. Storage scales automatically up to 128TiB without any impact to your application. In addition, Amazon DocumentDB natively integrates with AWS Database Migration Service (DMS), Amazon CloudWatch, AWS CloudTrail, AWS Lambda, AWS Backup and more. Amazon DocumentDB supports millions of requests per second and can be scaled out to 15 low latency read replicas in minutes with no application downtime.\n \nTo learn more about Amazon DocumentDB, please visit the Amazon DocumentDB product page and pricing page.\n \nYou can create a Amazon DocumentDB cluster from the AWS Management console, AWS Command Line Interface (CLI), or SDK.",
      "link": "https://aws.amazon.comabout-aws/whats-new/2026/02/amazon-documentdb-mongodb-compatibility-europe-zurich-region",
      "pubDate": "2026-02-11T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lambda",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lambda",
        "cloudwatch",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-123e63417c8b",
      "title": "Amazon DocumentDB (with MongoDB compatibility) is Now Available in the Asia Pacific (Melbourne) Region",
      "description": "Amazon DocumentDB (with MongoDB compatibility) is now available in the Asia Pacific (Melbourne) region adding to the list of available regions where you can use Amazon DocumentDB.\n \nAmazon DocumentDB is a fully managed, native JSON database that makes it simple and cost-effective to operate critical document workloads at virtually any scale without managing infrastructure. Amazon DocumentDB is designed to give you the scalability and durability you need when operating mission-critical MongoDB workloads. Storage scales automatically up to 128TiB without any impact to your application. In addition, Amazon DocumentDB natively integrates with AWS Database Migration Service (DMS), Amazon CloudWatch, AWS CloudTrail, AWS Lambda, AWS Backup and more. Amazon DocumentDB supports millions of requests per second and can be scaled out to 15 low latency read replicas in minutes with no application downtime.\n \nTo learn more about Amazon DocumentDB, please visit the Amazon DocumentDB product page and pricing page.\n \nYou can create a Amazon DocumentDB cluster from the AWS Management console, AWS Command Line Interface (CLI), or SDK.",
      "link": "https://aws.amazon.comabout-aws/whats-new/2026/02/amazon-documentdb-mongodb-compatibility-asia-pacific-melbourne-region",
      "pubDate": "2026-02-11T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lambda",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lambda",
        "cloudwatch",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-f28ad49fa3a4",
      "title": "Swann provides Generative AI to millions of IoT Devices using Amazon Bedrock",
      "description": "This post shows you how to implement intelligent notification filtering using Amazon Bedrock and its gen-AI capabilities. You'll learn model selection strategies, cost optimization techniques, and architectural patterns for deploying gen-AI at IoT scale, based on Swann Communications deployment across millions of devices.",
      "link": "https://aws.amazon.com/blogs/machine-learning/swann-provides-generative-ai-to-millions-of-iot-devices-using-amazon-bedrock/",
      "pubDate": "2026-02-11T15:48:15.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-0b76fca6e392",
      "title": "How LinqAlpha assesses investment theses using Devil’s Advocate on Amazon Bedrock",
      "description": "LinqAlpha is a Boston-based multi-agent AI system built specifically for institutional investors. The system supports and streamlines agentic workflows across company screening, primer generation, stock price catalyst mapping, and now, pressure-testing investment ideas through a new AI agent called Devil’s Advocate. In this post, we share how LinqAlpha uses Amazon Bedrock to build and scale Devil’s Advocate.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-linqalpha-assesses-investment-theses-using-devils-advocate-on-amazon-bedrock/",
      "pubDate": "2026-02-11T15:45:30.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "support"
      ]
    },
    {
      "id": "aws-news-2c6246a1ff48",
      "title": "Iberdrola enhances IT operations using Amazon Bedrock AgentCore",
      "description": "Iberdrola, one of the world’s largest utility companies, has embraced cutting-edge AI technology to revolutionize its IT operations in ServiceNow. Through its partnership with AWS, Iberdrola implemented different agentic architectures using Amazon Bedrock AgentCore, targeting three key areas: optimizing change request validation in the draft phase, enriching incident management with contextual intelligence, and simplifying change model selection using conversational AI. These innovations reduce bottlenecks, help teams accelerate ticket resolution, and deliver consistent and high-quality data handling throughout the organization.",
      "link": "https://aws.amazon.com/blogs/machine-learning/iberdrola-enhances-it-operations-using-amazon-bedrock-agentcore/",
      "pubDate": "2026-02-10T18:31:57.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "nova",
        "ga"
      ]
    },
    {
      "id": "aws-news-814a29034488",
      "title": "Amazon EC2 C8id, M8id, and R8id instances are available in additional regions",
      "description": "Amazon Elastic Compute Cloud (EC2) C8id, M8id, and R8id instances powered by custom Intel Xeon 6 processors feature up to 384 vCPUs, 3TiB of memory, and 22.8TB of NVMe SSD storage and deliver up to 43% higher performance and 3.3x more memory bandwidth compared to previous generation C6id, M6id, and R6id instances. Starting today, C8id and M8id instances are available in Europe (Frankfurt) and Asia Pacific (Tokyo) regions, with M8id also available in Europe (Spain) region. Additionally, R8id instances are now available in Europe (Spain) and Asia Pacific (Tokyo) regions.\n  These instances deliver up to 46% higher performance for I/O intensive database workloads, and up to 30% faster query results for I/O intensive real-time data analytics than previous sixth-generation instances. Additionally, these instances support Instance Bandwidth Configuration, allowing 25% flexible allocation between network and EBS bandwidth, allocating resources optimally for each workload.\n  C8id instances are ideal for compute-intensive workloads such as high-performance web servers, batch processing, distributed analytics, ad serving, video encoding, and gaming servers. M8id instances are well-suited for balanced workloads including application servers, microservices, enterprise applications, and small to medium databases. R8id instances are ideal for memory-intensive workloads such as in-memory databases, real-time big data analytics, large in-memory caches, and scientific computing applications.\n  C8id, M8id and R8id instances are available in US East (N. Virginia, Ohio), US West (Oregon), Europe (Frankfurt), and Asia Pacific (Tokyo) regions. M8id and R8id instances are additionally available in Europe (Spain) region. Customers can purchase these instances via Savings Plans, On-Demand instances, and Spot instances. For more information visit the Amazon EC2 instance type page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/c8id-m8id-and-r8id-in-additional-regions/",
      "pubDate": "2026-02-10T18:21:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "ec2",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-47906801ca88",
      "title": "Amazon Bedrock adds support for six fully-managed open weights models",
      "description": "Amazon Bedrock now supports six new models spanning frontier reasoning and agentic coding: DeepSeek V3.2, MiniMax M2.1, GLM 4.7, GLM 4.7 Flash, Kimi K2.5, and Qwen3 Coder Next. These six models bring customers access to the most capable open weights models available today, delivering frontier-class performance at significantly lower inference costs. They collectively cover the full spectrum of enterprise AI workloads: DeepSeek V3.2 and Kimi K2.5 push the frontier on reasoning and agentic intelligence, GLM 4.7 and Minimax 2.1 set new standards for autonomous coding with massive output windows, and Qwen3 Coder Next and GLM 4.7 Flash offer lightweight, cost-efficient alternatives purpose-built for production deployment.\n \nThese models on Amazon Bedrock are powered by Project Mantle, a new distributed inference engine for large-scale machine learning model serving on Amazon Bedrock. Project Mantle simplifies and expedites onboarding of new models onto Amazon Bedrock, provides highly performant and reliable serverless inference with sophisticated quality of service controls, unlocks higher default customer quotas with automated capacity management and unified pools, and provides out-of-the-box compatibility with OpenAI API specifications.\n \nTo learn more and get started, visit Amazon Bedrock console or the service documentation here. To get started with Amazon Bedrock OpenAI API-compatible service endpoints, visit documentation here.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-bedrock-adds-support-six-open-weights-models",
      "pubDate": "2026-02-10T16:02:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "rds"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "rds",
        "support",
        "new-model"
      ]
    },
    {
      "id": "aws-news-cf2d10da92a8",
      "title": "Amazon EKS Auto Mode Announces Enhanced Logging for its Managed Kubernetes Capabilities",
      "description": "Amazon Elastic Kubernetes Service (Amazon EKS) Auto Mode’s managed capabilities can now be configured as log delivery sources using Amazon CloudWatch Vended Logs. This integration enables customers to monitor and troubleshoot their EKS Auto Mode clusters more effectively by automatically collecting logs from Auto Mode’s managed Kubernetes capabilities for compute autoscaling, block storage, load balancing, and pod networking.\n  Customers can configure log delivery for Auto Mode capabilities using CloudWatch APIs or the AWS Console. Each Auto Mode capability can be configured as a CloudWatch Vended Logs delivery source, enabling reliable, secure log delivery with built-in AWS authentication and authorization at a reduced price compared to standard CloudWatch Logs. Customers can deliver these logs to CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose destinations.\n  This feature is available today in all regions where EKS Auto Mode is available. Standard CloudWatch Logs, S3, or Kinesis charges apply depending on the chosen destination.\n  To learn more about EKS Auto Mode logging capabilities, visit the Amazon EKS documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-eks-auto-mode-enhanced-logging",
      "pubDate": "2026-02-10T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "s3",
        "eks",
        "kinesis",
        "cloudwatch"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "eks",
        "kinesis",
        "cloudwatch",
        "integration"
      ]
    },
    {
      "id": "aws-news-dc88a6fd4ed0",
      "title": "Amazon Redshift now supports allocating extra compute for automatic optimizations",
      "description": "Amazon Redshift now supports allocating extra compute for automatic optimization features, known as autonomics. Database administrators managing Amazon Redshift workloads can now allocate additional resources for their clusters to enable autonomics even during periods of high user activity, eliminating the need to manually schedule optimizations such as Automatic Table Optimization (ATO), Automatic Table Sorting (ATS), Auto Vacuum, and Auto Analyze.\n  This enhancement extends Amazon Redshift's autonomics capabilities to automatically leverage extra compute resources, to run reliably without impacting user workloads. It also includes a cost control feature for provisioned clusters, allowing database administrators to limit the amount of resources available to autonomics. Additionally, the new SYS_AUTOMATIC_OPTIMIZATION system table enhances observability by providing detailed information on autonomics operations for both provisioned clusters and serverless workgroups.\n  This feature is available in all AWS Regions where Amazon Redshift is supported. To learn more, see Allocating extra compute resources for automatic database optimization.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-redshift-allocate-extra-compute-for-automatic-optimizations",
      "pubDate": "2026-02-09T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "redshift"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "redshift",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-5846bfa0757e",
      "title": "Scale LLM fine-tuning with Hugging Face and Amazon SageMaker AI",
      "description": "In this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications.",
      "link": "https://aws.amazon.com/blogs/machine-learning/scale-llm-fine-tuning-with-hugging-face-and-amazon-sagemaker-ai/",
      "pubDate": "2026-02-09T16:48:46.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "sagemaker",
        "lex"
      ]
    },
    {
      "id": "aws-news-e0de1c85e10d",
      "title": "New Relic transforms productivity with generative AI on AWS",
      "description": "Working with the Generative AI Innovation Center, New Relic NOVA (New Relic Omnipresence Virtual Assistant) evolved from a knowledge assistant into a comprehensive productivity engine. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.",
      "link": "https://aws.amazon.com/blogs/machine-learning/new-relic-transforms-productivity-with-generative-ai-on-aws/",
      "pubDate": "2026-02-09T16:45:16.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova",
        "ga"
      ]
    },
    {
      "id": "aws-news-88d1e4796f36",
      "title": "Accelerate agentic application development with a full-stack starter template for Amazon Bedrock AgentCore",
      "description": "In this post, you will learn how to deploy Fullstack AgentCore Solution Template (FAST) to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-agentic-application-development-with-a-full-stack-starter-template-for-amazon-bedrock-agentcore/",
      "pubDate": "2026-02-09T16:40:58.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "integration"
      ]
    },
    {
      "id": "aws-news-bc5baeadbafe",
      "title": "Introducing Multipart Download Support for AWS SDK for .NET Transfer Manager",
      "description": "The new multipart download support in AWS SDK for .NET Transfer Manager improves the performance of downloading large objects from Amazon Simple Storage Service (Amazon S3). Customers are looking for better performance and parallelization of their downloads, especially when working with large files or datasets. The AWS SDK for .NET Transfer Manager (version 4 only) […]",
      "link": "https://aws.amazon.com/blogs/developer/introducing-multipart-download-support-for-aws-sdk-for-net-transfer-manager/",
      "pubDate": "2026-02-09T16:27:06.000Z",
      "source": "developersAndDevOps",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "support"
      ]
    },
    {
      "id": "aws-news-43e867f93eaa",
      "title": "Amazon Bedrock AgentCore Browser now supports browser profiles",
      "description": "Amazon Bedrock AgentCore Browser now supports browser profiles, enabling you to reuse authentication state across multiple browser sessions without repeated login flows. This feature reduces session setup time from minutes to tens of seconds for enterprise customers processing hundreds or thousands of automated browser sessions daily.\n \nBrowser profiles persist and reuse browser data including cookies and local storage across multiple sessions. You authenticate to a website once and save the session to a browser profile. When you start a new session using that saved profile, your authentication state is preserved, and you remain logged in. This enables agents to perform tasks on authenticated websites without manual login intervention. You can choose flexible session modes for both read-only and persistent operations, enabling parallel processing where multiple sessions use the same profile simultaneously.\n \nThis feature is available in all 14 AWS Regions where Amazon Bedrock AgentCore Browser is available: US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Paris), Europe (Stockholm), Asia Pacific (Seoul), and Canada (Central).\n \nTo learn more, visit the Browser Profiles documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-bedrock-agentcore-browser-profiles",
      "pubDate": "2026-02-06T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-29ddb9057340",
      "title": "AWS Config now supports 30 new resource types",
      "description": "AWS Config now supports 30 additional AWS resource types across key services including Amazon EKS, Amazon Q, and AWS IoT. This expansion provides greater coverage over your AWS environment, enabling you to more effectively discover, assess, audit, and remediate an even broader range of resources.\n  With this launch, if you have enabled recording for all resource types, then AWS Config will automatically track these new additions. The newly supported resource types are also available in Config rules and Config aggregators.\n  You can now use AWS Config to monitor the following newly supported resource types in all AWS Regions where the supported resources are available:\n  Resource Types:\n  \n \n \n  \nAWS::ApplicationSignals::ServiceLevelObjective \n   AWS::IoT::SoftwarePackage \n  \nAWS::ARCZonalShift::AutoshiftObserverNotificationStatus      \n   AWS::IoT::TopicRule \n  \nAWS::B2BI::Transformer \n   AWS::IoTWireless::Destination \n  \nAWS::CE::CostCategory \n   AWS::IoTWireless::DeviceProfile \n  \nAWS::CleanRooms::ConfiguredTable \n   AWS::IoTWireless::NetworkAnalyzerConfiguration  \n  \nAWS::CleanRooms::Membership \n   AWS::IoTWireless::TaskDefinition \n  \nAWS::CodeArtifact::PackageGroup \n   AWS::IoTWireless::WirelessGateway \n  \nAWS::Connect::Prompt \n   AWS::Kinesis::ResourcePolicy \n  \nAWS::EKS::Nodegroup \n   AWS::PCAConnectorSCEP::Connector \n  \nAWS::GameLift::MatchmakingRuleSet \n   AWS::QBusiness::Application \n  \nAWS::GameLift::Script \n   AWS::QuickSight::DataSet \n  \nAWS::Glue::Crawler \n   AWS::QuickSight::Dashboard \n  \nAWS::InternetMonitor::Monitor \n   AWS::Route53::DNSSEC \n  \nAWS::IoT::BillingGroup \n   AWS::SSM::PatchBaseline \n  \nAWS::IoT::ResourceSpecificLogging \n   AWS::Transfer::User",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-config-new-resource-types",
      "pubDate": "2026-02-06T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "amazon q",
        "eks",
        "kinesis",
        "glue",
        "quicksight"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "eks",
        "kinesis",
        "glue",
        "quicksight",
        "launch",
        "ga",
        "support",
        "expansion"
      ]
    },
    {
      "id": "aws-news-eb570f9242d7",
      "title": "Reduce Mean Time to Resolution with an observability agent",
      "description": "In this post, we present an observability agent using OpenSearch Service and Amazon Bedrock AgentCore that can help surface root cause and get insights faster, handle multiple query-correlation cycles, and ultimately reduce MTTR even further.",
      "link": "https://aws.amazon.com/blogs/big-data/reduce-mean-time-to-resolution-with-an-observability-agent/",
      "pubDate": "2026-02-05T19:48:33.000Z",
      "source": "bigDataBlog",
      "services": [
        "bedrock",
        "agentcore",
        "opensearch",
        "opensearch service"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "opensearch",
        "opensearch service"
      ]
    },
    {
      "id": "aws-news-093fbdc9670f",
      "title": "Amazon EC2 G6e instances now available in the UAE region",
      "description": "Starting today, the Amazon EC2 G6e instances powered by NVIDIA L40S Tensor Core GPUs is now available in Middle East (UAE) Region. G6e instances can be used for a wide range of machine learning and spatial computing use cases.\n \nCustomers can use G6e instances to deploy large language models (LLMs) and diffusion models for generating images, video, and audio. Additionally, the G6e instances will unlock customers’ ability to create larger, more immersive 3D simulations and digital twins for spatial computing workloads. G6e instances feature up to 8 NVIDIA L40S Tensor Core GPUs with 48 GB of memory per GPU and third generation AMD EPYC processors. They also support up to 192 vCPUs, up to 400 Gbps of network bandwidth, up to 1.536 TB of system memory, and up to 7.6 TB of local NVMe SSD storage. \n  Amazon EC2 G6e instances are available today in the AWS US East (N. Virginia, Ohio), US West (Oregon), Asia Pacific (Tokyo, Seoul), Middle East (UAE) and Europe (Frankfurt, Spain, Stockholm) Regions. Customers can purchase G6e instances as On-Demand Instances, Reserved Instances, Spot Instances, or as part of Savings Plans.\n  To get started, visit the AWS Management Console, AWS Command Line Interface (CLI), and AWS SDKs. To learn more, visit the G6e instance page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-g6e-instances-uae-region/",
      "pubDate": "2026-02-05T19:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-639bbb72d5dd",
      "title": "Amazon EC2 I7ie instances now available in AWS Canada (Central)",
      "description": "AWS is announcing Amazon EC2 I7ie instances are now available in AWS Canada (Central) regions. Designed for large storage I/O intensive workloads, I7ie instances are powered by 5th Gen Intel Xeon Processors with an all-core turbo frequency of 3.2 GHz, offering up to 40% better compute performance and 20% better price performance over existing I3en instances. I7ie instances offer up to 120TB local NVMe storage density (highest in the cloud) for storage optimized instances and offer up to twice as many vCPUs and memory compared to prior generation instances. Powered by 3rd generation AWS Nitro SSDs, I7ie instances deliver up to 65% better real-time storage performance, up to 50% lower storage I/O latency, and 65% lower storage I/O latency variability compared to I3en instances.\n  I7ie are high density storage optimized instances, ideal for workloads requiring fast local storage with high random read/write performance at very low latency consistency to access large data sets. These instances are available in 9 different virtual sizes and deliver up to 100Gbps of network bandwidth and 60Gbps of bandwidth for Amazon Elastic Block Store (EBS).\n  To learn more, visit the I7ie instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-i7ie-instances-available-aws-canada/",
      "pubDate": "2026-02-05T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available"
      ]
    },
    {
      "id": "aws-news-22acca8edad5",
      "title": "Amazon EC2 High Memory U7i-6TB instances now available in AWS GovCloud (US-West)",
      "description": "Amazon EC2 High Memory U7i instances with 6TB of memory (u7i-6tb.112xlarge) are now available in AWS GovCloud (US-West). U7i instances are part of AWS 7th generation and are powered by custom fourth generation Intel Xeon Scalable Processors (Sapphire Rapids). U7i-6tb instances offer 6TiB of DDR5 memory, enabling customers to scale transaction processing throughput in a fast-growing data environment.\n  U7i-6tb instances offer 448 vCPUs, support up to 100Gbps Elastic Block Storage (EBS) for faster data loading and backups, deliver up to 100Gbps of network bandwidth, and support ENA Express. U7i instances are ideal for customers using mission-critical in-memory databases like SAP HANA, Oracle, and SQL Server.\n  To learn more about U7i instances, visit the High Memory instances page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-u7i-6tb-instances-available/",
      "pubDate": "2026-02-05T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-76536849a1c8",
      "title": "AWS Batch now supports unmanaged compute environments for Amazon EKS",
      "description": "AWS Batch now extends its job scheduling capabilities to unmanaged compute environments on Amazon EKS. With unmanaged EKS compute environments, you can leverage AWS Batch's job orchestration while maintaining full control over your Kubernetes infrastructure for security, compliance, or operational requirements.\n  With this capability, you can create unmanaged compute environments through CreateComputeEnvironment API and AWS Batch console by selecting your existing EKS cluster and specifying a Kubernetes namespace, then associate your EKS nodes with the compute environment using kubectl labeling.\n  AWS Batch supports developers, scientists, and engineers in running efficient batch processing for ML model training, simulations, and analysis at any scale. Unmanaged compute environments on Amazon EKS are available today in all AWS regions where AWS Batch is available. For more information, see the AWS Batch User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-batch-on-eks-unmanaged-compute-environments",
      "pubDate": "2026-02-04T20:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "eks"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "eks",
        "support"
      ]
    },
    {
      "id": "aws-news-d7fcc4742b73",
      "title": "Amazon EC2 G7e instances now available in US West (Oregon) region",
      "description": "Starting today, Amazon EC2 G7e instances accelerated by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs are now available in US West (Oregon) region. G7e instances offer up to 2.3x inference performance compared to G6e.\n \nCustomers can use G7e instances to deploy large language models (LLMs), agentic AI models, multimodal generative AI models, and physical AI models. G7e instances offer the highest performance for spatial computing workloads as well as workloads that require both graphics and AI processing capabilities. G7e instances feature up to 8 NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, with 96 GB of memory per GPU, and 5th Generation Intel Xeon processors. They support up to 192 virtual CPUs (vCPUs) and up to 1600 Gbps of networking bandwidth. G7e instances support NVIDIA GPUDirect Peer to Peer (P2P) that boosts performance for multi-GPU workloads. Multi-GPU G7e instances also support NVIDIA GPUDirect Remote Direct Memory Access (RDMA) with EFA in EC2 UltraClusters, reducing latency for small-scale multi-node workloads.\n \nYou can use G7e instances for Amazon EC2 in the following AWS Regions: US West (Oregon), US East (N. Virginia) and US East (Ohio). You can purchase G7e instances as On-Demand Instances, Spot Instances, or as part of Savings Plans.\n \nTo get started, visit the AWS Management Console, AWS Command Line Interface (CLI), and AWS SDKs. To learn more, visit G7e instances.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-g7e-instances-oregon-region/",
      "pubDate": "2026-02-04T19:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ec2",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-fe5f77e82b59",
      "title": "Introducing Amazon EC2 C8id, M8id, and R8id instances",
      "description": "AWS is announcing the general availability of new Amazon EC2 C8id, M8id, and R8id instances powered by custom Intel Xeon 6 processors. These instances deliver up to 43% higher performance and 3.3x more memory bandwidth compared to previous generation C6id, M6id, and R6id instances.\n  C8id, M8id, and R8id instances offer up to 384 vCPUs, 3TiB of memory, and 22.8TB of NVMe SSD storage, 3x more than previous generation instances. These instances deliver up to 46% higher performance for I/O intensive database workloads, and up to 30% faster query results for I/O intensive real-time data analytics than previous sixth-generation instances. Additionally, these instances support Instance Bandwidth Configuration, allowing 25% flexible allocation between network and EBS bandwidth, allocating resources optimally for each workload.\n  C8id instances are ideal for compute-intensive workloads such as high-performance web servers, batch processing, distributed analytics, ad serving, video encoding, and gaming servers. M8id instances are well-suited for balanced workloads including application servers, microservices, enterprise applications, and small to medium databases. R8id instances are ideal for memory-intensive workloads such as in-memory databases, real-time big data analytics, large in-memory caches, and scientific computing applications.\n  C8id, M8id and R8id instances are available in US East (N. Virginia), US East (Ohio), and US West (Oregon). R8id instances are additionally available in Europe (Frankfurt). Customers can purchase these instances via Savings Plans, On-Demand instances, and Spot instances. For more information visit the Amazon EC2 instance type page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-c8id-m8id-r8id-instances/",
      "pubDate": "2026-02-04T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "ec2"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "ec2",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-68e81d0b319e",
      "title": "Optimizing Flink’s join operations on Amazon EMR with Alluxio",
      "description": "In this post, we show you how to implement real-time data correlation using Apache Flink to join streaming order data with historical customer and product information, enabling you to make informed decisions based on comprehensive, up-to-date analytics. We also introduce an optimized solution to automatically load Hive dimension table data into Alluxio Universal Flash Storage (UFS) through the Alluxio cache layer. This enables Flink to perform temporal joins on changing data, accurately reflecting the content of a table at specific points in time.",
      "link": "https://aws.amazon.com/blogs/big-data/optimizing-flinks-join-operations-on-amazon-emr-with-alluxio/",
      "pubDate": "2026-02-03T18:41:31.000Z",
      "source": "bigDataBlog",
      "services": [
        "emr"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "emr"
      ]
    },
    {
      "id": "aws-news-2bbf3449a6da",
      "title": "More room to build: serverless services now support payloads up to 1 MB",
      "description": "To support cloud applications that increasingly depend on rich contextual data, AWS is raising the maximum payload size from 256 KB to 1 MB for asynchronous AWS Lambda function invocations, Amazon Amazon SQS, and Amazon EventBridge. Developers can use this enhancement to build and maintain context-rich event-driven systems and reduce the need for complex workarounds such as data chunking or external large object storage.",
      "link": "https://aws.amazon.com/blogs/compute/more-room-to-build-serverless-services-now-support-payloads-up-to-1-mb/",
      "pubDate": "2026-01-29T22:16:14.000Z",
      "source": "computeBlog",
      "services": [
        "lex",
        "lambda",
        "eventbridge",
        "sqs"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex",
        "lambda",
        "eventbridge",
        "sqs",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-36722fddcb63",
      "title": "Building zero trust generative AI applications in healthcare with AWS Nitro Enclaves",
      "description": "In healthcare, generative AI is transforming how \nmedical professionals analyze data, \nsummarize clinical notes, and \ngenerate insights to improve patient outcomes. From \nautomating medical documentation to assisting in \ndiagnostic reasoning, large language models (LLMs) have the potential to augment clinical workflows and accelerate research. However, these innovations also introduce significant privacy, security, and intellectual property challenges.",
      "link": "https://aws.amazon.com/blogs/compute/building-zero-trust-generative-ai-applications-in-healthcare-with-aws-nitro-enclaves/",
      "pubDate": "2025-12-12T19:06:03.000Z",
      "source": "computeBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-b56aaf668b93",
      "title": "Architecting conversational observability for cloud applications",
      "description": "In this post, we walk through building a generative AI–powered troubleshooting assistant for Kubernetes. The goal is to give engineers a faster, self-service way to diagnose and resolve cluster issues, cut down Mean Time to Recovery (MTTR), and reduce the cycles experts spend finding the root cause of issues in complex distributed systems.",
      "link": "https://aws.amazon.com/blogs/architecture/architecting-conversational-observability-for-cloud-applications/",
      "pubDate": "2025-12-11T15:59:39.000Z",
      "source": "architectureBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "lex"
      ]
    },
    {
      "id": "aws-news-2e1c3c046458",
      "title": "Introducing Amazon S3 Transfer Manager for Swift (Developer Preview)",
      "description": "e are pleased to announce the Developer Preview release of the Amazon S3 Transfer Manager for Swift —a high-level file and directory transfer utility for \nAmazon Simple Storage Service (Amazon S3) built with the \nAWS SDK for Swift.",
      "link": "https://aws.amazon.com/blogs/developer/introducing-amazon-s3-transfer-manager-for-swift-developer-preview/",
      "pubDate": "2025-11-21T21:02:48.000Z",
      "source": "developersAndDevOps",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "preview"
      ]
    },
    {
      "id": "aws-news-d48c6bab49bb",
      "title": "Serverless strategies for streaming LLM responses",
      "description": "Modern generative AI applications often need to stream large language model (LLM) outputs to users in real-time. Instead of waiting for a complete response, streaming delivers partial results as they become available, which significantly improves the user experience for chat interfaces and long-running AI tasks. This post compares three serverless approaches to handle Amazon Bedrock LLM streaming on Amazon Web Services (AWS), which helps you choose the best fit for your application.",
      "link": "https://aws.amazon.com/blogs/compute/serverless-strategies-for-streaming-llm-responses/",
      "pubDate": "2025-11-21T03:42:56.000Z",
      "source": "computeBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-547c9eb92bd7",
      "title": "Building responsive APIs with Amazon API Gateway response streaming",
      "description": "Today, AWS announced support for response streaming in Amazon API Gateway to significantly improve the responsiveness of your REST APIs by progressively streaming response payloads back to the client. With this new capability, you can use streamed responses to enhance user experience when building LLM-driven applications (such as AI agents and chatbots), improve time-to-first-byte (TTFB) performance for web and mobile applications, stream large files, and perform long-running operations while reporting incremental progress using protocols such as server-sent events (SSE).",
      "link": "https://aws.amazon.com/blogs/compute/building-responsive-apis-with-amazon-api-gateway-response-streaming/",
      "pubDate": "2025-11-19T23:10:51.000Z",
      "source": "computeBlog",
      "services": [
        "api gateway"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "api gateway",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-089334445f81",
      "title": "Build resilient generative AI agents",
      "description": "Generative AI agents in production environments demand resilience strategies that go beyond traditional software patterns. AI agents make autonomous decisions, consume substantial computational resources, and interact with external systems in unpredictable ways. These characteristics create failure modes that conventional resilience approaches might not address. This post presents a framework for AI agent resilience risk analysis […]",
      "link": "https://aws.amazon.com/blogs/architecture/build-resilient-generative-ai-agents/",
      "pubDate": "2025-09-30T15:11:51.000Z",
      "source": "architectureBlog",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": []
    },
    {
      "id": "aws-news-54c273e45b01",
      "title": "Upgrading your AWS SDK for Go from V1 to V2 with Amazon Q Developer",
      "description": "Software development is far more than just writing code. In reality, a developer spends a large amount of time maintaining existing applications and fixing bugs. For example, migrating a Go application from the older AWS SDK for Go v1 to the newer v2 can be a significant undertaking, but it’s a crucial step to future-proof […]",
      "link": "https://aws.amazon.com/blogs/developer/upgrading-your-aws-sdk-for-go-from-v1-to-v2-with-amazon-q-developer/",
      "pubDate": "2025-06-18T06:38:24.000Z",
      "source": "developersAndDevOps",
      "services": [
        "amazon q",
        "q developer"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "amazon q",
        "q developer"
      ]
    },
    {
      "id": "aws-news-c4f514e85eef",
      "title": "AWS SDK for Ruby: Deprecating Ruby 2.5 & 2.6 Runtime Supports and Future Compatibility",
      "description": "Effective June 2, 2025, AWS SDK for Ruby Version 3 will no longer support following end-of-life (EOL) Ruby runtime versions: Ruby 2.5 (EOL began on 2021-04-05) Ruby 2.6 (EOL began on 2022-04-12) To ensure your applications and services remain secure, we strongly encourage you to upgrade to Ruby 2.7 or later. Moving forward, AWS SDK […]",
      "link": "https://aws.amazon.com/blogs/developer/aws-sdk-for-ruby-deprecating-ruby-2-5-2-6-runtime-supports-and-future-compatibility/",
      "pubDate": "2025-03-27T15:08:27.000Z",
      "source": "developersAndDevOps",
      "services": [],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-5cf08af5aca4",
      "title": "Announcing the Developer Preview of Amazon S3 Transfer Manager in Rust",
      "description": "We are excited to announce the Developer Preview of the Amazon S3 Transfer Manager for Rust, a high-level utility that speeds up and simplifies uploads and downloads with Amazon Simple Storage Service (Amazon S3). Using this new library, developers can efficiently transfer data between Amazon S3 and various sources, including files, in-memory buffers, memory streams, […]",
      "link": "https://aws.amazon.com/blogs/developer/announcing-the-developer-preview-of-amazon-s3-transfer-manager-in-rust/",
      "pubDate": "2025-03-26T15:52:22.000Z",
      "source": "developersAndDevOps",
      "services": [
        "s3"
      ],
      "categories": [
        "generative-ai"
      ],
      "tags": [
        "s3",
        "preview"
      ]
    }
  ]
}