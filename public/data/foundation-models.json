{
  "lastUpdated": "2025-12-14T06:15:07.590Z",
  "category": "foundation-models",
  "totalItems": 21,
  "items": [
    {
      "id": "aws-news-27b9a3890ec7",
      "title": "Building a voice-driven AWS assistant with Amazon Nova Sonic",
      "description": "In this post, we explore how to build a sophisticated voice-powered AWS operations assistant using Amazon Nova Sonic for speech processing and Strands Agents for multi-agent orchestration. This solution demonstrates how natural language voice interactions can transform cloud operations, making AWS services more accessible and operations more efficient.",
      "link": "https://aws.amazon.com/blogs/machine-learning/building-a-voice-driven-aws-assistant-with-amazon-nova-sonic/",
      "pubDate": "2025-12-12T18:07:57.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-7b9f7a2bf518",
      "title": "How Harmonic Security improved their data-leakage detection system with low-latency fine-tuned models using Amazon SageMaker, Amazon Bedrock, and Amazon Nova Pro",
      "description": "This post walks through how Harmonic Security used Amazon SageMaker AI, Amazon Bedrock, and Amazon Nova Pro to fine-tune a ModernBERT model, achieving low-latency, accurate, and scalable data leakage detection.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-harmonic-security-improved-their-data-leakage-detection-system-with-low-latency-fine-tuned-models-using-amazon-sagemaker-amazon-bedrock-and-amazon-nova-pro/",
      "pubDate": "2025-12-11T18:28:15.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova",
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-8104f30c91af",
      "title": "Implement automated smoke testing using Amazon Nova Act headless mode",
      "description": "This post shows how to implement automated smoke testing using Amazon Nova Act headless mode in CI/CD pipelines. We use SauceDemo, a sample ecommerce application, as our target for demonstration. We demonstrate setting up Amazon Nova Act for headless browser automation in CI/CD environments and creating smoke tests that validate key user workflows. We then show how to implement parallel execution to maximize testing efficiency, configure GitLab CI/CD for automatic test execution on every deployment, and apply best practices for maintainable and scalable test automation.",
      "link": "https://aws.amazon.com/blogs/machine-learning/implement-automated-smoke-testing-using-amazon-nova-act-headless-mode/",
      "pubDate": "2025-12-10T19:04:24.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-2742955c1b90",
      "title": "Real-world reasoning: How Amazon Nova Lite 2.0 handles complex customer support scenarios",
      "description": "This post evaluates the reasoning capabilities of our latest offering in the Nova family, Amazon Nova Lite 2.0, using practical scenarios that test these critical dimensions. We compare its performance against other models in the Nova family—Lite 1.0, Micro, Pro 1.0, and Premier—to elucidate how the latest version advances reasoning quality and consistency.",
      "link": "https://aws.amazon.com/blogs/machine-learning/real-world-reasoning-how-amazon-nova-lite-2-0-handles-complex-customer-support-scenarios/",
      "pubDate": "2025-12-09T20:50:42.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "lex",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-43c9c14d8626",
      "title": "Create an intelligent insurance underwriter agent powered by Amazon Nova 2 Lite and Amazon Quick Suite",
      "description": "In this post, we demonstrate how to build an intelligent insurance underwriting agent that addresses three critical challenges: unifying siloed data across CRM systems and databases, providing explainable and auditable AI decisions for regulatory compliance, and enabling automated fraud detection with consistent underwriting rules. The solution combines Amazon Nova 2 Lite for transparent risk assessment, Amazon Bedrock AgentCore for managed MCP server infrastructure, and Amazon Quick Suite for natural language interactions—delivering a production-ready system that underwriters can deploy in under 30 minutes .",
      "link": "https://aws.amazon.com/blogs/machine-learning/create-an-intelligent-insurance-underwriter-agent-powered-by-amazon-nova-2-lite-and-amazon-quick-suite/",
      "pubDate": "2025-12-08T16:30:37.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "nova",
        "amazon q"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "nova",
        "amazon q"
      ]
    },
    {
      "id": "aws-news-6e0869a825a5",
      "title": "Amazon Bedrock now supports Responses API from OpenAI",
      "description": "Amazon Bedrock now supports Responses API on new OpenAI API-compatible service endpoints. Responses API enables developers to achieve asynchronous inference for long-running inference workloads, simplifies tool use integration for agentic workflows, and also supports stateful conversation management. Instead of requiring developers to pass the entire conversation history with each request, Responses API enables them to automatically rebuild context without manual history management. These new service endpoints support both streaming and non-streaming modes, enable reasoning effort support within Chat Completions API, and require only a base URL change for developers to integrate within existing codebases with OpenAI SDK compatibility.\n  \n Chat Completions with reasoning effort support is available for all Amazon Bedrock models powered by Project Mantle, a new distributed inference engine for large-scale machine learning model serving on Amazon Bedrock. Project Mantle simplifies and expedites onboarding of new models onto Amazon Bedrock, provides highly performant and reliable serverless inference with sophisticated quality of service controls, unlocks higher default customer quotas with automated capacity management and unified pools, and provides out-of-the-box compatibility with OpenAI API specifications. Responses API support is available today starting with OpenAI's GPT OSS 20B/120B models, with support for other models coming soon.\n To get started, visit the service documentation here",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-bedrock-responses-api-from-openai/",
      "pubDate": "2025-12-04T12:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "integration",
        "support",
        "coming-soon",
        "new-model"
      ]
    },
    {
      "id": "aws-news-15dab4dd881e",
      "title": "New serverless model customization capability in Amazon SageMaker AI",
      "description": "Amazon Web Services (AWS) announces a new serverless model customization capability that empowers AI developers to quickly customize popular models with supervised fine-tuning and the latest techniques like reinforcement learning. Amazon SageMaker AI is a fully managed service that brings together a broad set of tools to enable high-performance, low-cost AI model development for any use case. \n \nMany AI developers seek to customize models with proprietary data for improved accuracy, but this often requires lengthy iteration cycles. For example, AI developers must define a use case and prepare data, select a model and customization technique, train the model, then evaluate the model for deployment. Now AI developers can simplify the end-to-end model customization workflow, from data preparation to evaluation and deployment, and accelerate the process. With an easy-to-use interface, AI developers can quickly get started and customize popular models, including Amazon Nova, Llama, Qwen, DeepSeek, and GPT-OSS, with their own data. They can use supervised fine-tuning and the latest customization techniques such as reinforcement learning and direct preference optimization. In addition, AI developers can use the AI agent-guided workflow (in preview), and use natural language to generate synthetic data, analyze data quality, and handle model training and evaluation—all entirely serverless. \n \nYou can use this easy-to-use interface in the following AWS Regions: Europe (Ireland), US East (N. Virginia), Asia Pacific (Tokyo), and US West (Oregon). To join the waitlist to access the AI agent-guided workflow, visit the sign-up page. \n \nTo learn more, visit the SageMaker AI model customization page and blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/new-serverless-model-customization-capability-amazon-sagemaker-ai",
      "pubDate": "2025-12-03T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "sagemaker",
        "preview"
      ]
    },
    {
      "id": "aws-news-ef3aea785303",
      "title": "Introducing elastic training on Amazon SageMaker HyperPod",
      "description": "Amazon SageMaker HyperPod now supports elastic training, enabling organizations to accelerate foundation model training by automatically scaling training workloads based on resource availability and workload priorities. This represents a fundamental shift from training with a fixed set of resources, as it saves hours of engineering time spent reconfiguring training jobs based on compute availability.\n \nAny change in compute availability previously required manually halting training, reconfiguring training parameters, and restarting jobs—a process that requires distributed training expertise and leaves expensive AI accelerators sitting idle during training job reconfiguration. Elastic training automatically expands training jobs to absorb idle AI accelerators and seamlessly contracting when higher-priority workloads need resources—all without halting training entirely.\n \nBy eliminating manual reconfiguration overhead and ensuring continuous utilization of available compute, elastic training can help save time previously spent on infrastructure management, reduce costs by maximizing cluster utilization, and accelerate time-to-market. Training can start immediately with minimal resources and grow opportunistically as capacity becomes available.\n \nSageMaker HyperPod is available in all regions where Amazon SageMaker HyperPod is currently available. Organizations can enable elastic training with zero code changes using HyperPod recipes for publicly available models including Llama and GPT OSS. For custom model architectures, customers can integrate elastic training capabilities through lightweight configuration updates and minimal code modifications, making it accessible to teams without requiring distributed systems expertise.\n \nTo get started, visit the Amazon SageMaker HyperPod product page and see the elastic training documentation for implementation guidance.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/elastic-training-amazon-sagemaker-hyperpod/",
      "pubDate": "2025-12-03T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "hyperpod",
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "organizations",
        "ga",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-06f56a16182f",
      "title": "Amazon Bedrock now supports reinforcement fine-tuning delivering 66% accuracy gains on average over base models",
      "description": "Amazon Bedrock now supports reinforcement fine-tuning, helping you improve model accuracy without needing deep machine learning expertise or large sums of labeled data. Amazon Bedrock automates the reinforcement fine-tuning workflow, making this advanced model customization technique accessible to everyday developers. Models learn to align with your specific requirements using a small set of prompts rather than the large sums of data needed for traditional fine-tuning methods, enabling teams to get started quickly. This capability teaches models through feedback on multiple possible responses to the same prompt, improving their judgement of what makes a good response. Reinforcement fine-tuning in Amazon Bedrock delivers 66% accuracy gains on average over base models so you can use smaller, faster, and more cost-effective model variants while maintaining high quality.\n \nOrganizations struggle to adapt AI models to their unique business needs, forcing them to choose between generic models with average performance or expensive, complex customization that requires specialized talent, infrastructure, and risky data movement. Reinforcement fine-tuning in Amazon Bedrock removes this complexity by making advanced model customization fast, automated, and secure. You can train models by uploading training data directly from your computer or choose from datasets already stored in Amazon S3, eliminating the need for any labeled datasets. You can define reward functions using verifiable rule-based graders or AI-based judges along with built-in templates to optimize your models for both objective tasks such as code generation or math reasoning, and subjective tasks such as instruction following or chatbot interactions. Your proprietary data never leaves AWS's secure, governed environment during the entire customization process, mitigating security and compliance concerns.\n \nYou can get started with reinforcement fine-tuning in Amazon Bedrock through the Amazon Bedrock console and via the Amazon Bedrock APIs. At launch, you can use reinforcement fine-tuning with Amazon Nova 2 Lite with support for additional models coming soon. To learn more about reinforcement fine-tuning in Amazon Bedrock, read the launch blog, pricing page, and documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/bedrock-reinforcement-fine-tuning-66-base-models/",
      "pubDate": "2025-12-03T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "lex",
        "s3",
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "lex",
        "s3",
        "organizations",
        "launch",
        "ga",
        "support",
        "coming-soon"
      ]
    },
    {
      "id": "aws-news-11d5bd89eb5c",
      "title": "Announcing Amazon Nova 2 Sonic for real-time conversational AI",
      "description": "Today, Amazon announces the availability of Amazon Nova 2 Sonic, our speech-to-speech model for natural, real-time conversational AI.  It offers best-in-class streaming speech understanding with robustness to background noise and users’ speaking styles, efficient dialog handling, and speech generation with expressive voices that can speak natively in multiple languages (Polyglot voices). It has superior reasoning, instruction following, and tool invocation accuracy over the previous model.\n \nNova 2 Sonic builds on the capabilities introduced in the original Nova Sonic model with new features including expanded language support (Portuguese and Hindi), polyglot voices that enable the model to speak different languages with native expressivity using the same voice, and turn-taking controllability to allow developers to set low, medium, or high pause sensitivity. The model also adds cross-modal interaction, allowing users to seamlessly switch between voice and text in the same session, asynchronous tool calling to support multi-step tasks without interrupting conversation flow, and a one-million token context window for sustained interactions.\n \nDevelopers can integrate Nova Sonic 2 directly into real-time voice systems using Amazon Bedrock’s bidirectional streaming API. Nova Sonic 2 now also seamlessly integrates with Amazon Connect and other leading telephony providers, including Vonage, Twilio, and AudioCodes, as well as open source frameworks such as LiveKit and Pipecat.\n \nAmazon Nova 2 Sonic is available in Amazon Bedrock in the following AWS Regions: US East (N. Virginia), US West (Oregon), and Asia Pacific (Tokyo). To learn more, read the AWS News Blog and the Amazon Nova Sonic User Guide. To get started with Nova Sonic 2 in Amazon Bedrock, visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-nova-2-sonic-real-time-conversational-ai/",
      "pubDate": "2025-12-02T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "new-feature",
        "support"
      ]
    },
    {
      "id": "aws-news-313f260093e1",
      "title": "Mistral Large 3 and Ministral 3 family now available first  on Amazon Bedrock",
      "description": "Customers can now use Mistral Large 3 and the Ministral 3 family of models available first on Amazon Bedrock as well as additional models including Voxtral Mini 1.0, Voxtral Small 1.0, and Magistral Small 1.2 on Amazon Bedrock, a platform for building generative AI applications and agents at production scale.\n \nMistral Large 3 is a state-of-the-art, open-weight, general-purpose multimodal model with a granular Mixture-of-Experts architecture featuring 41B active parameters and 675B total parameters, designed for reliability and long-context comprehension. The Ministral 3 family—consisting of 14B, 8B, and 3B models—offers competitive checkpoints across language, vision, and instruct variants, enabling developers to select the right scale for customization and deployment. Amazon Bedrock is the first platform to offer these cutting-edge models, giving customers early access to Mistral AI's latest innovations. Mistral Large 3 excels at production-grade assistants, retrieval-augmented systems, and complex enterprise workflows with support for a 256K context window and powerful agentic capabilities. The Ministral 3 family complements this with flexible deployment options: Ministral 3 14B delivers advanced multimodal capabilities for local deployment, Ministral 3 8B provides best-in-class text and vision capabilities for edge deployment and single-GPU operation, and Ministral 3 3B offers robust capabilities in a compact package for low-resource environments. Together, these models span the full spectrum from frontier intelligence to efficient edge computing.\n \nThese models are now available in Amazon Bedrock. For the full list of available AWS Regions, refer to the documentation.\n \nTo get started with these models in Amazon Bedrock, visit the Amazon Bedrock Mistral AI page",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/mistral-large-3-ministral-3-family-available-amazon-bedrock",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "lex",
        "early-access",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-4f603af09d02",
      "title": "Introducing AWS AI Factories",
      "description": "AWS AI Factories are now available, providing rapidly deployable, high-performance AWS AI infrastructure in your own data centers. By combining the latest AWS Trainium accelerators and NVIDIA GPUs, specialized low-latency networking, high-performance storage, and AWS AI services, AI Factories accelerate your AI buildouts by months or years compared to building independently. Leveraging nearly two decades of AWS cloud leadership expertise, AWS AI Factories eliminate the complexity of procurement, setup, and optimization that typically delays AI initiatives.\n \nWith integrated AWS AI services like Amazon Bedrock and Amazon SageMaker, you gain immediate access to leading foundation models without negotiating separate contracts with individual model providers.  AWS AI Factories operate as dedicated environments built exclusively for you or your designated trusted community, ensuring complete separation and operating independence while integrating with the broader set of AWS services. This approach helps governments and enterprises meet digital sovereignty requirements while benefiting from the unparalleled security, reliability, and capabilities of the AWS Cloud. You provide the data center space and power capacity you've already acquired, while AWS deploys and manages the infrastructure. \n \nAWS AI Factories deliver advanced AI technologies to enterprises across all industries and government organizations seeking secure, isolated environments with strict data residency requirements. These dedicated environments provide access to the same advanced technologies available in public cloud Regions, allowing you to build AI-powered applications as well as train and deploy large language models using your own proprietary data. Rather than spending years building capacity independently, AWS accelerates deployment timelines so you can focus on innovation instead of infrastructure complexity. \n \nContact your AWS account team to learn more about deploying AWS AI Factories in your data center and accelerating your AI initiatives with AWS proven expertise in building and maintaining dedicated AI infrastructure at scale.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/aws-ai-factories",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "sagemaker",
        "lex",
        "trainium",
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "sagemaker",
        "lex",
        "trainium",
        "organizations",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-4d58f9580777",
      "title": "Announcing Amazon Nova 2 foundation models now available in Amazon Bedrock",
      "description": "Today AWS announces Amazon Nova 2, our next generation of general models that deliver reasoning capabilities with industry-leading price performance. The new models available today in Amazon Bedrock are:\n \n• Amazon Nova 2 Lite, a fast, cost-effective reasoning model for everyday workloads.\n \n• Amazon Nova 2 Pro (Preview), our most intelligent model for highly complex, multistep tasks.\n \nAmazon Nova 2 Lite and Amazon Nova 2 Pro (Preview) offer significant advancements over our previous generation models. These models support extended thinking with step-by-step reasoning and task decomposition and include three thinking intensity levels—low, medium, and high—giving developers control over the balance of speed, intelligence, and cost. The models also offer built-in tools such as code interpreter and web grounding, support remote MCP tools, and provide a one-million-token context window for richer interactions.\n \nNova 2 Lite can be used for a broad range of your everyday tasks. It offers the best combination of price, performance, and speed. Early customers are using Nova 2 Lite for customer service chatbots, document processing, and business process automation. Nova 2 Lite can be customized using supervised fine-tuning (SFT) on Amazon Bedrock and Amazon SageMaker, and full fine-tuning is available on Amazon SageMaker. Amazon Nova 2 Pro (Preview) can be used for highly complex agentic tasks such as multi-document analysis, video reasoning, and software migrations.\n \nAmazon Nova 2 Lite and Nova 2 Pro (Preview) is now available in Amazon Bedrock via global cross region inference in multiple locations. Nova 2 Pro is in preview with early access available to all Amazon Nova Forge customers. If interested, reach out to your AWS account team regarding access.\n \nLearn more at the AWS News Blog, Amazon Nova models product page, and Amazon Nova user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/nova-2-foundation-models-amazon-bedrock/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "sagemaker",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "sagemaker",
        "lex",
        "preview",
        "early-access",
        "ga",
        "now-available",
        "support",
        "new-model"
      ]
    },
    {
      "id": "aws-news-407048c12183",
      "title": "Amazon Bedrock adds 18 fully managed open weight models, the largest expansion of new models to date",
      "description": "Amazon Bedrock is a platform for building generative AI applications and agents at production scale. Amazon Bedrock provides access to a broad selection of fully managed models from leading AI companies through a unified API, enabling you to evaluate, switch, and adopt new models without rewriting applications or changing infrastructure. Today, Amazon Bedrock is adding 18 fully managed open weight models to its model offering, the largest expansion of new models to date.\n \nYou can now access the following models in Amazon Bedrock:\n  \nGoogle: Gemma 3 4B, Gemma 3 12B, Gemma 3 27B\n \nMiniMax AI: MiniMax M2\n \nMistral AI: Mistral Large 3, Ministral 3 3B, Ministral 3 8B, Ministral 3 14B, Magistral Small 1.2, Voxtral Mini 1.0, Voxtral Small 1.0\n \nMoonshot AI: Kimi K2 Thinking\n \nNVIDIA: NVIDIA Nemotron Nano 2 9B, NVIDIA Nemotron Nano 2 VL 12B\n \nOpenAI: gpt-oss-safeguard-20b, gpt-oss-safeguard-120b\n \nQwen: Qwen3-Next-80B-A3B, Qwen3-VL-235B-A22B\n \nFor the full list of available AWS Regions, refer to the documentation.\n \nTo learn more about all the models that Amazon Bedrock offers, view the Amazon Bedrock model choice page. To get started using these models in Amazon Bedrock, read the launch blog and visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-bedrock-fully-managed-open-weight-models",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "launch",
        "expansion",
        "new-model"
      ]
    },
    {
      "id": "aws-news-2da9fd465b9d",
      "title": "Build agents to automate production UI workflows with Amazon Nova Act (GA)",
      "description": "We are excited to announce the general availability of Amazon Nova Act, a new AWS service for developers to build and manage fleets of highly reliable agents for automating production UI workflows. Nova Act is powered by a custom Nova 2 Lite model and provides high reliability with unmatched cost efficiency, fastest time-to-value, and ease of implementation at scale.\n  Nova Act can reliably complete repetitive UI workflows in the browser, execute APIs or tools (e.g. write to PDF), and escalate to a human supervisor when appropriate. Developers that need to automate repetitive processes across the enterprise can define workflows combining the flexibility of natural language with more deterministic Python code. Technical teams using Nova Act can start prototyping quickly on the online playground at nova.amazon.com/act, refine and debug their scripts using the Nova Act IDE extension, and deploy to AWS in just a few steps.\n  Nova Act is available today in AWS Region US East (N. Virginia).\n  Learn more about Nova Act.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/build-automate-production-ui-workflows-nova-act/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "lex",
        "ga"
      ]
    },
    {
      "id": "aws-news-5615baf94b9d",
      "title": "Introducing Amazon Nova 2 Omni in Preview",
      "description": "We are excited to announce Amazon Nova 2 Omni, an all-in-one model for multimodal reasoning and image generation. It is the industry’s first reasoning model that supports text, images, video, and speech inputs while generating both text and image outputs. It enables multimodal understanding, image generation and editing using natural language, and speech transcription.\n  Unlike traditional approaches that often force organizations to stitch together various specialized models, each supporting different input and output types, Nova 2 Omni eliminates the complexity of managing multiple AI models. This helps to accelerate application development while reducing complexity and costs, enabling developers to tackle diverse tasks from marketing content creation and customer support call transcription to video analysis and documentation with visual aids.\n  The model supports a 1M token context window, 200+ languages for text processing and 10 languages for speech input. It can generate and edits high-quality images using natural language, enabling character consistency, text rendering within image as well as object and background modification. Nova 2 Omni delivers superior speech understanding with native reasoning to transcribe, translate and summarize multi-speaker conversations. And with flexible reasoning controls for depth and budget, developers can ensure optimal performance, accuracy, and cost management across different use cases.\n  Nova 2 Omni is in preview with early access available to all Nova Forge customers. Please reach out to your AWS account team for access. To learn more about Amazon Nova 2 Omni read the user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-nova-2-omni-preview",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "translate",
        "lex",
        "transcribe",
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "translate",
        "lex",
        "transcribe",
        "organizations",
        "preview",
        "early-access",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-f4dd8e1f32da",
      "title": "Amazon Nova Forge: Build your own Frontier Models using Nova",
      "description": "We are excited to announce the general availability of Nova Forge, a new service to build your own frontier models using Nova.\n  With Nova Forge, you can start your model development on SageMaker AI from early Nova checkpoints across pre-training, mid-training, or post-training phases. You can blend proprietary data with Amazon Nova-curated data to train the model. You can also take advantage of model development features available exclusively on Nova Forge, including the ability to execute Reinforcement Fine Tuning (RFT) with reward functions in your environment and to implement custom safety guardrails using the built-in responsible AI toolkit. Nova Forge allows you to build models that deeply understand your organization’s proprietary knowledge and reflects your expertise, while preserving general capabilities like reasoning and minimizing risks like catastrophic forgetting. In addition, Nova Forge customers get early access to new Nova models, including Nova 2 Pro and Nova 2 Omni.\n  Nova Forge is available today in US East (N. Virginia) AWS Region and will be available in additional regions in the coming months. Learn more about Nova Forge on the AWS News Blog, the Amazon Nova Forge product page, or the Amazon Nova Forge user guide. You can get started with Nova Forge today from the Amazon Nova Forge console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-nova-forge-frontier-models-nova/",
      "pubDate": "2025-12-02T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "nova",
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "sagemaker",
        "early-access",
        "ga"
      ]
    },
    {
      "id": "aws-news-f3f2eea0b334",
      "title": "How Myriad Genetics achieved fast, accurate, and cost-efficient document processing using the AWS open-source Generative AI Intelligent Document Processing Accelerator",
      "description": "In this post, we explore how Myriad Genetics partnered with the AWS Generative AI Innovation Center to transform their healthcare document processing pipeline using Amazon Bedrock and Amazon Nova foundation models, achieving 98% classification accuracy while reducing costs by 77% and processing time by 80%. We detail the technical implementation using AWS's open-source GenAI Intelligent Document Processing Accelerator, the optimization strategies for document classification and key information extraction, and the measurable business impact on Myriad's prior authorization workflows.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-myriad-genetics-achieved-fast-accurate-and-cost-efficient-document-processing-using-the-aws-open-source-generative-ai-intelligent-document-processing-accelerator/",
      "pubDate": "2025-11-27T00:58:14.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova"
      ]
    },
    {
      "id": "aws-news-587d9a94208c",
      "title": "How CBRE powers unified property management search and digital assistant using Amazon Bedrock",
      "description": "In this post, CBRE and AWS demonstrate how they transformed property management by building a unified search and digital assistant using Amazon Bedrock, enabling professionals to access millions of documents and multiple databases through natural language queries. The solution combines Amazon Nova Pro for SQL generation and Claude Haiku for document interactions, achieving a 67% reduction in processing time while maintaining enterprise-grade security across more than eight million documents.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-cbre-powers-unified-property-management-search-and-digital-assistant-using-amazon-bedrock/",
      "pubDate": "2025-11-27T00:56:27.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova"
      ]
    },
    {
      "id": "aws-news-f2f20ca0e88d",
      "title": "Building AI-Powered Voice Applications: Amazon Nova Sonic Telephony Integration Guide",
      "description": "Available through the Amazon Bedrock bidirectional streaming API, Amazon Nova Sonic can connect to your business data and external tools and can be integrated directly with telephony systems. This post will introduce sample implementations for the most common telephony scenarios.",
      "link": "https://aws.amazon.com/blogs/machine-learning/building-ai-powered-voice-applications-amazon-nova-sonic-telephony-integration-guide/",
      "pubDate": "2025-11-26T21:21:54.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "integration"
      ]
    },
    {
      "id": "aws-news-b1018aefba54",
      "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers",
      "description": "In this post, we demonstrate how to deploy the DeepSeek-R1-Distill-Qwen-32B model using AWS DLCs for vLLMs on Amazon EKS, showcasing how these purpose-built containers simplify deployment of this powerful open source inference engine. This solution can help you solve the complex infrastructure challenges of deploying LLMs while maintaining performance and cost-efficiency.",
      "link": "https://aws.amazon.com/blogs/architecture/deploy-llms-on-amazon-eks-using-vllm-deep-learning-containers/",
      "pubDate": "2025-08-14T15:09:51.000Z",
      "source": "architectureBlog",
      "services": [
        "lex",
        "eks"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "lex",
        "eks"
      ]
    }
  ]
}