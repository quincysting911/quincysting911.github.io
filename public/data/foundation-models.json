{
  "lastUpdated": "2026-02-16T06:50:56.729Z",
  "category": "foundation-models",
  "totalItems": 15,
  "items": [
    {
      "id": "aws-news-86e8f3e987b4",
      "title": "Amazon Bedrock adds support for the latest open-weight models in Asia Pacific (Sydney)",
      "description": "Amazon Bedrock is a fully managed service that provides secure, enterprise-grade access to high-performing foundation models from leading AI companies, enabling you to build and scale generative AI applications. Today, Amazon Bedrock announced support for the latest open-weight models in Asia Pacific (Sydney) using the bedrock-mantle endpoint. These include models from industry-leading providers, including DeepSeek, Google, MiniMax, Mistral, Moonshot AI, MiniMax, Nvidia, and OpenAI. The bedrock-mantle endpoint is powered by Project Mantle, a new distributed inference engine for large-scale machine learning model serving on Amazon Bedrock. Project Mantle simplifies and expedites onboarding of new models onto Amazon Bedrock. It provides highly performant and reliable serverless inference with sophisticated quality of service controls, unlocks higher default customer quotas with automated capacity management and unified pools, and delivers out-of-the-box compatibility with OpenAI API specifications.\n \n\n To learn more and get started, visit the Amazon Bedrock console or the Amazon Bedrock service documentation. To get started with Amazon Bedrock OpenAI API-compatible service endpoints, visit the OpenAI API compatibility documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-bedrock-support-latest-open-weight-models-asia-pacific-sydney/",
      "pubDate": "2026-02-12T22:14:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "support",
        "new-model"
      ]
    },
    {
      "id": "aws-news-a0fbcc6e4099",
      "title": "Amazon Bedrock expands support for AWS PrivateLink",
      "description": "Amazon Bedrock is a fully managed service that provides secure, enterprise-grade access to high-performing foundation models from leading AI companies. It enables you to build and scale generative AI applications. Amazon Bedrock already supported AWS PrivateLink for the bedrock-runtime endpoint. Now, with this launch, you can also use AWS PrivateLink to privately access your applications using the bedrock-mantle endpoint. The bedrock-mantle endpoint is powered by Project Mantle, a new distributed inference engine for large-scale machine learning model serving on Amazon Bedrock. Project Mantle simplifies and expedites onboarding of new models onto Amazon Bedrock. It provides highly performant and reliable serverless inference with sophisticated quality of service controls, unlocks higher default customer quotas with automated capacity management and unified pools, and delivers out-of-the-box compatibility with OpenAI API specifications.\n \nAWS PrivateLink support for OpenAI API-compatible endpoints is available in US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Jakarta), Asia Pacific (Tokyo), Asia Pacific (Mumbai), Asia Pacific (Sydney), South America (São Paulo), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Milan), Europe (Stockholm), and South America (Sao Paulo) AWS Regions. To learn more and get started, visit the Amazon Bedrock console or the Amazon Bedrock service documentation. To get started with Amazon Bedrock OpenAI API-compatible service endpoints, visit the OpenAI API compatibility documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-bedrock-expands-aws-privatelink-support-openai-api-endpoints/",
      "pubDate": "2026-02-12T21:40:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "launch",
        "support",
        "new-model"
      ]
    },
    {
      "id": "aws-news-dc4b03f3c7f7",
      "title": "Amazon Bedrock increases default quotas for Anthropic’s Claude Sonnet 4.5 model in AWS GovCloud (US)",
      "description": "Amazon Bedrock has increased the default quotas for Anthropic’s Claude Sonnet 4.5 in AWS GovCloud (US-West) and AWS GovCloud (US-East) to 5,000,000 tokens per minute and 1,000 requests per minute, aligning with commercial AWS regions. This 25x increase enables customers to scale their AI workloads more effectively in regulated environments.\n Claude Sonnet 4.5 is Anthropic's latest Sonnet model, excelling at building complex agents, coding, and long-horizon tasks while maintaining optimal speed and cost-efficiency for high-volume use-cases.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-bedrock-s4.5-quota-aws-govcloud-us",
      "pubDate": "2026-02-12T19:52:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lex"
      ]
    },
    {
      "id": "aws-news-0d02aec495a3",
      "title": "How Amazon uses Amazon Nova models to automate operational readiness testing for new fulfillment centers",
      "description": "In this post, we discuss how Amazon Nova in Amazon Bedrock can be used to implement an AI-powered image recognition solution that automates the detection and validation of module components, significantly reducing manual verification efforts and improving accuracy.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-amazon-uses-amazon-nova-models-to-automate-operational-readiness-testing-for-new-fulfillment-centers/",
      "pubDate": "2026-02-10T18:34:09.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova"
      ]
    },
    {
      "id": "aws-news-8115f2327170",
      "title": "Building real-time voice assistants with Amazon Nova Sonic compared to cascading architectures",
      "description": "Amazon Nova Sonic delivers real-time, human-like voice conversations through the bidirectional streaming interface. In this post, you learn how Amazon Nova Sonic can solve some of the challenges faced by cascaded approaches, simplify building voice AI agents, and provide natural conversational capabilities. We also provide guidance on when to choose each approach to help you make informed decisions for your voice AI projects.",
      "link": "https://aws.amazon.com/blogs/machine-learning/building-real-time-voice-assistants-with-amazon-nova-sonic-compared-to-cascading-architectures/",
      "pubDate": "2026-02-10T18:29:05.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-86cfec91c00f",
      "title": "Agent-to-agent collaboration: Using Amazon Nova 2 Lite and Amazon Nova Act for multi-agent systems",
      "description": "This post walks through how agent-to-agent collaboration on Amazon Bedrock works in practice, using Amazon Nova 2 Lite for planning and Amazon Nova Act for browser interaction, to turn a fragile single-agent setup into a predictable multi-agent system.",
      "link": "https://aws.amazon.com/blogs/machine-learning/agent-to-agent-collaboration-using-amazon-nova-2-lite-and-amazon-nova-act-for-multi-agent-systems/",
      "pubDate": "2026-02-09T16:00:28.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova"
      ]
    },
    {
      "id": "aws-news-07ee34afa737",
      "title": "Structured outputs on Amazon Bedrock: Schema-compliant AI responses",
      "description": "Today, we're announcing structured outputs on Amazon Bedrock—a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance. In this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanisms—JSON Schema output format and strict tool use—along with implementation details, best practices, and practical code examples.",
      "link": "https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/",
      "pubDate": "2026-02-06T20:12:14.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-ffce1b1c279a",
      "title": "Evaluate generative AI models with an Amazon Nova rubric-based LLM judge on Amazon SageMaker AI (Part 2)",
      "description": "In this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.",
      "link": "https://aws.amazon.com/blogs/machine-learning/evaluate-generative-ai-models-with-an-amazon-nova-rubric-based-llm-judge-on-amazon-sagemaker-ai-part-2/",
      "pubDate": "2026-02-06T16:29:45.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-4c444fe2b470",
      "title": "A practical guide to Amazon Nova Multimodal Embeddings",
      "description": "In this post, you will learn how to configure and use Amazon Nova Multimodal Embeddings for media asset search systems, product discovery experiences, and document retrieval applications.",
      "link": "https://aws.amazon.com/blogs/machine-learning/a-practical-guide-to-amazon-nova-multimodal-embeddings/",
      "pubDate": "2026-02-05T20:35:34.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-c520cba1d654",
      "title": "Claude Opus 4.6 now available in Amazon Bedrock",
      "description": "Starting today, Amazon Bedrock supports Claude Opus 4.6. According to Anthropic, Opus 4.6 is their most intelligent model and the world's best model for coding, enterprise agents, and professional work. Claude Opus 4.6 brings advanced capabilities to Amazon Bedrock customers, including industry-leading performance for agentic tasks, complex coding projects, and enterprise-grade workflows that require deep reasoning and reliability.\n  Claude Opus 4.6 excels across use cases that require sophisticated reasoning and multi-step orchestration. For agentic workflows, it manages complex tasks across dozens of tools with industry-leading reliability, proactively spinning up subagents and working with less oversight. Developers can leverage Opus 4.6’s coding capabilities for long-horizon projects, complex implementations, and large-scale codebases—handling the full lifecycle from requirements gathering to implementation and maintenance. Enterprise teams can use the model to power end-to-end workflows with professional polish, including financial analysis that surfaces insights requiring days of manual compilation, cybersecurity applications that catch subtle attack patterns, and computer use workflows that move data between applications. The model supports both 200K and 1M context tokens (preview), enabling processing of extensive documents and codebases.\n  Claude Opus 4.6 is now available in Amazon Bedrock. For the full list of available regions, refer to the documentation. To learn more and get started with the model in Amazon Bedrock, read the About Amazon blog and visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/2/claude-opus-4.6-available-amazon-bedrock/",
      "pubDate": "2026-02-05T09:59:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lex",
        "preview",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-bffd31a2575d",
      "title": "Structured outputs now available in Amazon Bedrock",
      "description": "Amazon Bedrock now supports structured outputs, a capability that provides consistent, machine-readable responses from foundation models that adhere to your defined JSON schemas. Instead of prompting for valid JSON and adding extra checks in your application, you can specify the format you want and receive responses that match it—making production workflows more predictable and resilient.\n  Structured outputs helps with common production tasks such as extracting key fields and powering workflows that use APIs or tools, where small formatting errors can break downstream systems. By ensuring schema compliance, it reduces the need for custom validation logic and lowers operational overhead through fewer failed requests and retries—so you can confidently deploy AI applications that require predictable, machine-readable outputs. You can use structured outputs in two ways: define a JSON schema that describes the response format you want, or use strict tool definitions to ensure a model’s tool calls match your specifications.\n  Structured outputs is generally available for Anthropic Claude 4.5 models and select open-weight models across the Converse, ConverseStream, InvokeModel, and InvokeModelWithResponseStream APIs in all commercial AWS Regions where Amazon Bedrock is supported. To learn more about structured outputs and the supported models, visit the Amazon Bedrock documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/structured-outputs-available-amazon-bedrock/",
      "pubDate": "2026-02-04T19:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "generally-available",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-583e5c06e6da",
      "title": "Cartesia Sonic 3 text-to-speech model is now available on Amazon SageMaker JumpStart",
      "description": "Cartesia’s Sonic 3 model is now available in Amazon SageMaker JumpStart, expanding the portfolio of foundation models available to AWS customers. Sonic 3 is Cartesia's latest state space model (SSM) for streaming text-to-speech (TTS), delivering high naturalness, accurate transcript following, and industry-leading latency with fine-grained control over volume, speed, and emotion.\n \nSonic 3 supports 42 languages and provides advanced controllability through API parameters and SSML tags for volume, speed, and emotion adjustments. The model includes natural laughter support, stable voices optimized for voice agents, and emotive voices for expressive characters. With sub-100ms latency, Sonic 3 enables real-time conversational AI that captures human speech nuances including emotions and tonal shifts.\n  With SageMaker JumpStart, customers can deploy Sonic 3 with just a few clicks to address their voice AI use cases. To get started with this model, navigate to the SageMaker JumpStart model catalog in the SageMaker Studio or use the SageMaker Python SDK to deploy the model to your AWS account. For more information about deploying and using foundation models in SageMaker JumpStart, see the Amazon SageMaker JumpStart documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/cartesia-sonic-3-on-sagemaker-jumpstart",
      "pubDate": "2026-02-04T19:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-7240408be297",
      "title": "DeepSeek OCR, MiniMax M2.1, and Qwen3-VL-8B-Instruct models are now available on SageMaker JumpStart",
      "description": "Today, AWS announced the availability of DeepSeek OCR, MiniMax M2.1, and Qwen3-VL-8B-Instruct in Amazon SageMaker JumpStart, expanding the portfolio of foundation models available to AWS customers. These three models bring specialized capabilities spanning document intelligence, multilingual coding, advanced multimodal reasoning, and vision-language understanding, enabling customers to build sophisticated AI applications across diverse use cases on AWS infrastructure.\n  These models address different enterprise AI challenges with specialized capabilities:\n DeepSeek OCR explores visual-text compression for document processing. It can extract structured information from forms, invoices, diagrams, and complex documents with dense text layouts.\n MiniMax M2.1 is optimized for coding, tool use, instruction following, and long-horizon planning. It automates multilingual software development and executes complex, multi-step office workflows, empowering developers to build autonomous applications.\n Qwen3-VL-8B-Instruct delivers ssuperior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\n With SageMaker JumpStart, customers can deploy any of these models with just a few clicks to address their specific AI use cases.\n  To get started with these models, navigate to the SageMaker JumpStart model catalog in the SageMaker console or use the SageMaker Python SDK to deploy the models to your AWS account. For more information about deploying and using foundation models in SageMaker JumpStart, see the Amazon SageMaker JumpStart documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/01/new-models-on-sagemaker-jumpstart",
      "pubDate": "2026-02-02T21:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "lex",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-5f799d8bb52c",
      "title": "Build Production-Ready Drug Discovery and Robotics Pipelines with NVIDIA NIMs on SageMaker JumpStart",
      "description": "Amazon SageMaker JumpStart now enables one-click deployment of four NVIDIA NIMs models purpose-built for biosciences and physical AI: ProteinMPNN, Nemotron-3.5B-Instruct, MSA Search NIM, and Cosmos Reason. NVIDIA NIM™ provides prebuilt, optimized inference microservices for rapidly deploying the latest AI models on any NVIDIA-accelerated infrastructure. These models bring advanced capabilities spanning protein design, reasoning with configurable outputs, and physical world understanding, enabling customers to accelerate biosciences research, drug discovery, and embodied AI applications on AWS infrastructure.\n \nProteinMPNN enables fast and efficient protein sequence optimization guided by structural data. This NIM generates high-quality sequences with enhanced binding affinity and stability, validated through experimental results. Designed for scalability and flexibility, ProteinMPNN integrates seamlessly into protein engineering workflows, transforming applications like enzyme design and therapeutic development.\n \nMSA Search NIM supports GPU-accelerated Multiple Sequence Alignment (MSA) of a query amino acid sequence against a set of protein sequence databases. These databases are searched for similar sequences to the query and then the collection of sequences are aligned to establish similar regions even when the proteins have different lengths and motifs.\n \nNemotron-3.5B-Instruct delivers high reasoning performance, native tool calling support, and extended context processing with 256k token context window. This model employs an efficient hybrid Mixture-of-Experts (MoE) architecture to ensure higher throughput than its predecessors for agentic and coding workloads, while maintaining the reasoning depth of a larger model. It is ideal for building multi-agent workflows, developer productivity tools, processes automation, and for scientific and mathematical reasoning analysis, amongst others.\n \nCosmos Reason is an open , customizable, reasoning vision language model (VLM) for physical AI and robotics. It enables robots and vision AI agents to reason like humans, using prior knowledge, physics understanding, and common sense to understand and act in the real world. This model understands space, time, and fundamental physics, and can serve as a planning model to reason what steps an embodied agent might take next.\n \nWith SageMaker JumpStart, customers can deploy any of these models with just a few clicks to address their specific AI use cases.\n \nTo get started with these models, navigate to the SageMaker JumpStart model catalog in the SageMaker console or use the SageMaker Python SDK to deploy the models to your AWS account. For more information about deploying and using foundation models in SageMaker JumpStart, see the Amazon SageMaker JumpStart documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/accelerate-biosciences-and-robotics-with-NVIDIA-NIMs-on-sagemaker-jumpstart",
      "pubDate": "2026-02-02T21:24:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "lex",
        "experimental",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-a76018245791",
      "title": "Create a customizable cross-company log lake, Part II: Build and add Amazon Bedrock",
      "description": "In this post, you learn how to build Log Lake, a customizable cross-company data lake for compliance-related use cases that combines AWS CloudTrail and Amazon CloudWatch logs. You'll discover how to set up separate tables for writing and reading, implement event-driven partition management using AWS Lambda, and transform raw JSON files into read-optimized Apache ORC format using AWS Glue jobs. Additionally, you'll see how to extend Log Lake by adding Amazon Bedrock model invocation logs to enable human review of agent actions with elevated permissions, and how to use an AI agent to query your log data without writing SQL.",
      "link": "https://aws.amazon.com/blogs/big-data/create-a-customizable-cross-company-log-lake-part-ii-build-and-add-amazon-bedrock/",
      "pubDate": "2026-01-27T17:46:45.000Z",
      "source": "bigDataBlog",
      "services": [
        "bedrock",
        "lambda",
        "glue",
        "cloudwatch"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lambda",
        "glue",
        "cloudwatch"
      ]
    }
  ]
}