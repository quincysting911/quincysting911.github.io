{
  "lastUpdated": "2025-11-21T06:16:18.487Z",
  "category": "foundation-models",
  "totalItems": 7,
  "items": [
    {
      "id": "aws-news-3e047f65ab4b",
      "title": "Amazon Bedrock is now available in additional Regions",
      "description": "Beginning today, customers can use Amazon Bedrock in the Africa (Cape Town), Canada West (Calgary), Mexico (Central), and Middle East (Bahrain) regions to easily build and scale generative AI applications using a variety of foundation models (FMs) as well as powerful tools to build generative AI applications.\n  Amazon Bedrock is a comprehensive and secure service for building generative AI applications and agents. Amazon Bedrock connects you to leading foundation models (FMs) and services to deploy and operate agents, enabling you to quickly move from experimentation to real-world deployment.\n  To get started, visit the Amazon Bedrock page and see the Amazon Bedrock documentation for more details.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-bedrock-available-in-additional-regions/",
      "pubDate": "2025-11-19T19:59:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-68402400a8cb",
      "title": "Build an agentic solution with Amazon Nova, Snowflake, and LangGraph",
      "description": "In this post, we cover how you can use tools from Snowflake AI Data Cloud and Amazon Web Services (AWS) to build generative AI solutions that organizations can use to make data-driven decisions, increase operational efficiency, and ultimately gain a competitive edge.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-an-agentic-solution-with-amazon-nova-snowflake-and-langgraph/",
      "pubDate": "2025-11-19T16:16:49.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "organizations",
        "ga"
      ]
    },
    {
      "id": "aws-news-71a8274aea0e",
      "title": "Amazon Bedrock Custom Model Import now supports OpenAI GPT OSS models",
      "description": "Amazon Bedrock Custom Model Import now supports Open AI GPT OSS models. You can import custom weights for gpt-oss-120b and gpt-oss-20b models. This enables you to bring your own customized GPT OSS models into Amazon Bedrock and deploy them in a fully managed, serverless environment—without having to manage infrastructure or model serving.\n  GPT OSS models are text-to-text models designed for reasoning, agentic, and developer tasks. The larger gpt-oss-120b model is optimized for production, general purpose, and high reasoning use cases, while the smaller gpt-oss-20b model is best suited for lower latency, or specialized used cases such as data processing or domain-specific summarization.\n  Amazon Bedrock Custom Model Import for GPT OSS models is generally available in the US-East (N. Virginia) AWS Region. You can get started by importing your custom GPT OSS models in the custom models section of the Amazon Bedrock console. To learn more about OpenAI models in Amazon Bedrock visit the product page. To see what all architectures are supported visit the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/bedrock-model-import-openai-gpt-oss-models/",
      "pubDate": "2025-11-19T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "generally-available",
        "support"
      ]
    },
    {
      "id": "aws-news-f786ad798688",
      "title": "Amazon EC2 P6-B300 instances with NVIDIA Blackwell Ultra GPUs are now available",
      "description": "Today, AWS announces the general availability of Amazon Elastic Compute Cloud (Amazon EC2) P6-B300 instances, accelerated by NVIDIA Blackwell Ultra B300 GPUs. Amazon EC2 P6-B300 instances provide 8x NVIDIA Blackwell Ultra GPUs with 2.1 TB high bandwidth GPU memory, 6.4 Tbps EFA networking, 300 Gbps dedicated ENA throughput, and 4 TB of system memory. \n \nP6-B300 instances deliver 2x networking bandwidth, 1.5x GPU memory size, and 1.5x GPU TFLOPS (at FP4, without sparsity) compared to P6-B200 instances, making them well suited to train and deploy large trillion-parameter foundation models (FMs) and large language models (LLMs) with sophisticated techniques. The higher networking and larger memory deliver faster training times and more token throughput for AI workloads. \n \nP6-B300 instances are now available in the p6-b300.48xlarge size through Amazon EC2 Capacity Blocks for ML and Savings Plans in the following AWS Region: US West (Oregon). For on-demand reservation of P6-B300 instances, please reach out to your account manager.\n \nTo learn more about P6-B300 instances, visit Amazon EC2 P6 instances.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-ec2-p6-b300-instances-nvidia-blackwell-ultra-gpus-available",
      "pubDate": "2025-11-18T19:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "ec2"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "ec2",
        "now-available"
      ]
    },
    {
      "id": "aws-news-1d21975f882d",
      "title": "Amazon Bedrock introduces Priority and Flex inference service tiers",
      "description": "Today, Amazon Bedrock introduces two new inference service tiers to optimize costs and performance for different AI workloads. The new Flex tier offers cost-effective pricing for non-time-critical applications like model evaluations and content summarization while the Priority tier provides premium performance and preferential processing for mission-critical applications. For most models that support Priority Tier, customers can realize up to 25% better output tokens per second (OTPS) latency compared to standard tier. These join the existing Standard tier for everyday AI applications with reliable performance.\n \nThese service tiers address key challenges that organizations face when deploying AI at scale. The Flex tier is designed for non-interactive workloads that can tolerate longer latencies, making it ideal for model evaluations, content summarization, labeling and annotation, and multistep agentic workflow, and it’s priced at a discount relative to the Standard tier. During periods of high demand, Flex requests receive lower priority relative to the Standard tier. The Priority tier is an ideal fit for mission critical applications, real-time end-user interactions, and interactive experiences where consistent, fast responses are essential. During periods of high demand, Priority requests receive processing priority, at a premium price, over other service tiers. These new service tiers are available today for a range of leading foundation models, including OpenAI (gpt-oss-20b, gpt-oss-120b), DeepSeek (DeepSeek V3.1), Qwen3 (Coder-480B-A35B-Instruct, Coder-30B-A3B-Instruct, 32B dense, Qwen3-235B-A22B-2507), and Amazon Nova (Nova Pro and Nova Premier). With these new options, Amazon Bedrock helps customers gain greater control over balancing cost efficiency with performance requirements, enabling them to scale AI workloads economically while ensuring optimal user experiences for their most critical applications.\n \nFor more information about the AWS Regions where Amazon Bedrock Priority and Flex inference service tiers are available, see the AWS Regions table\n \nLearn more about service tiers in our News Blog and documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-bedrock-priority-flex-inference-service-tiers",
      "pubDate": "2025-11-18T18:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "nova",
        "lex",
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "lex",
        "organizations",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-33f9af5cf730",
      "title": "Make your web apps hands-free with Amazon Nova Sonic",
      "description": "Graphical user interfaces have carried the torch for decades, but today’s users increasingly expect to talk to their applications. In this post we show how we added a true voice-first experience to a reference application—the Smart Todo App—turning routine task management into a fluid, hands-free conversation.",
      "link": "https://aws.amazon.com/blogs/machine-learning/make-your-web-apps-hands-free-with-amazon-nova-sonic/",
      "pubDate": "2025-11-14T18:18:54.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-b1018aefba54",
      "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers",
      "description": "In this post, we demonstrate how to deploy the DeepSeek-R1-Distill-Qwen-32B model using AWS DLCs for vLLMs on Amazon EKS, showcasing how these purpose-built containers simplify deployment of this powerful open source inference engine. This solution can help you solve the complex infrastructure challenges of deploying LLMs while maintaining performance and cost-efficiency.",
      "link": "https://aws.amazon.com/blogs/architecture/deploy-llms-on-amazon-eks-using-vllm-deep-learning-containers/",
      "pubDate": "2025-08-14T15:09:51.000Z",
      "source": "architectureBlog",
      "services": [
        "lex",
        "eks"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "lex",
        "eks"
      ]
    }
  ]
}