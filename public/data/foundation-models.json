{
  "lastUpdated": "2026-02-08T06:36:03.459Z",
  "category": "foundation-models",
  "totalItems": 15,
  "items": [
    {
      "id": "aws-news-07ee34afa737",
      "title": "Structured outputs on Amazon Bedrock: Schema-compliant AI responses",
      "description": "Today, we're announcing structured outputs on Amazon Bedrock—a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance. In this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanisms—JSON Schema output format and strict tool use—along with implementation details, best practices, and practical code examples.",
      "link": "https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/",
      "pubDate": "2026-02-06T20:12:14.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock"
      ]
    },
    {
      "id": "aws-news-ffce1b1c279a",
      "title": "Evaluate generative AI models with an Amazon Nova rubric-based LLM judge on Amazon SageMaker AI (Part 2)",
      "description": "In this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.",
      "link": "https://aws.amazon.com/blogs/machine-learning/evaluate-generative-ai-models-with-an-amazon-nova-rubric-based-llm-judge-on-amazon-sagemaker-ai-part-2/",
      "pubDate": "2026-02-06T16:29:45.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-4c444fe2b470",
      "title": "A practical guide to Amazon Nova Multimodal Embeddings",
      "description": "In this post, you will learn how to configure and use Amazon Nova Multimodal Embeddings for media asset search systems, product discovery experiences, and document retrieval applications.",
      "link": "https://aws.amazon.com/blogs/machine-learning/a-practical-guide-to-amazon-nova-multimodal-embeddings/",
      "pubDate": "2026-02-05T20:35:34.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-c520cba1d654",
      "title": "Claude Opus 4.6 now available in Amazon Bedrock",
      "description": "Starting today, Amazon Bedrock supports Claude Opus 4.6. According to Anthropic, Opus 4.6 is their most intelligent model and the world's best model for coding, enterprise agents, and professional work. Claude Opus 4.6 brings advanced capabilities to Amazon Bedrock customers, including industry-leading performance for agentic tasks, complex coding projects, and enterprise-grade workflows that require deep reasoning and reliability.\n  Claude Opus 4.6 excels across use cases that require sophisticated reasoning and multi-step orchestration. For agentic workflows, it manages complex tasks across dozens of tools with industry-leading reliability, proactively spinning up subagents and working with less oversight. Developers can leverage Opus 4.6’s coding capabilities for long-horizon projects, complex implementations, and large-scale codebases—handling the full lifecycle from requirements gathering to implementation and maintenance. Enterprise teams can use the model to power end-to-end workflows with professional polish, including financial analysis that surfaces insights requiring days of manual compilation, cybersecurity applications that catch subtle attack patterns, and computer use workflows that move data between applications. The model supports both 200K and 1M context tokens (preview), enabling processing of extensive documents and codebases.\n  Claude Opus 4.6 is now available in Amazon Bedrock. For the full list of available regions, refer to the documentation. To learn more and get started with the model in Amazon Bedrock, read the About Amazon blog and visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/2/claude-opus-4.6-available-amazon-bedrock/",
      "pubDate": "2026-02-05T09:59:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lex",
        "preview",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-bffd31a2575d",
      "title": "Structured outputs now available in Amazon Bedrock",
      "description": "Amazon Bedrock now supports structured outputs, a capability that provides consistent, machine-readable responses from foundation models that adhere to your defined JSON schemas. Instead of prompting for valid JSON and adding extra checks in your application, you can specify the format you want and receive responses that match it—making production workflows more predictable and resilient.\n  Structured outputs helps with common production tasks such as extracting key fields and powering workflows that use APIs or tools, where small formatting errors can break downstream systems. By ensuring schema compliance, it reduces the need for custom validation logic and lowers operational overhead through fewer failed requests and retries—so you can confidently deploy AI applications that require predictable, machine-readable outputs. You can use structured outputs in two ways: define a JSON schema that describes the response format you want, or use strict tool definitions to ensure a model’s tool calls match your specifications.\n  Structured outputs is generally available for Anthropic Claude 4.5 models and select open-weight models across the Converse, ConverseStream, InvokeModel, and InvokeModelWithResponseStream APIs in all commercial AWS Regions where Amazon Bedrock is supported. To learn more about structured outputs and the supported models, visit the Amazon Bedrock documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/structured-outputs-available-amazon-bedrock/",
      "pubDate": "2026-02-04T19:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "generally-available",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-583e5c06e6da",
      "title": "Cartesia Sonic 3 text-to-speech model is now available on Amazon SageMaker JumpStart",
      "description": "Cartesia’s Sonic 3 model is now available in Amazon SageMaker JumpStart, expanding the portfolio of foundation models available to AWS customers. Sonic 3 is Cartesia's latest state space model (SSM) for streaming text-to-speech (TTS), delivering high naturalness, accurate transcript following, and industry-leading latency with fine-grained control over volume, speed, and emotion.\n \nSonic 3 supports 42 languages and provides advanced controllability through API parameters and SSML tags for volume, speed, and emotion adjustments. The model includes natural laughter support, stable voices optimized for voice agents, and emotive voices for expressive characters. With sub-100ms latency, Sonic 3 enables real-time conversational AI that captures human speech nuances including emotions and tonal shifts.\n  With SageMaker JumpStart, customers can deploy Sonic 3 with just a few clicks to address their voice AI use cases. To get started with this model, navigate to the SageMaker JumpStart model catalog in the SageMaker Studio or use the SageMaker Python SDK to deploy the model to your AWS account. For more information about deploying and using foundation models in SageMaker JumpStart, see the Amazon SageMaker JumpStart documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/cartesia-sonic-3-on-sagemaker-jumpstart",
      "pubDate": "2026-02-04T19:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "ga",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-be7e04d5035f",
      "title": "Accelerating your marketing ideation with generative AI – Part 2: Generate custom marketing images from historical references",
      "description": "Building upon our earlier work of marketing campaign image generation using Amazon Nova foundation models, in this post, we demonstrate how to enhance image generation by learning from previous marketing campaigns. We explore how to integrate Amazon Bedrock, AWS Lambda, and Amazon OpenSearch Serverless to create an advanced image generation system that uses reference campaigns to maintain brand guidelines, deliver consistent content, and enhance the effectiveness and efficiency of new campaign creation.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerating-your-marketing-ideation-with-generative-ai-part-2-generate-custom-marketing-images-from-historical-references/",
      "pubDate": "2026-02-04T15:57:24.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "nova",
        "lambda",
        "opensearch"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "nova",
        "lambda",
        "opensearch"
      ]
    },
    {
      "id": "aws-news-7240408be297",
      "title": "DeepSeek OCR, MiniMax M2.1, and Qwen3-VL-8B-Instruct models are now available on SageMaker JumpStart",
      "description": "Today, AWS announced the availability of DeepSeek OCR, MiniMax M2.1, and Qwen3-VL-8B-Instruct in Amazon SageMaker JumpStart, expanding the portfolio of foundation models available to AWS customers. These three models bring specialized capabilities spanning document intelligence, multilingual coding, advanced multimodal reasoning, and vision-language understanding, enabling customers to build sophisticated AI applications across diverse use cases on AWS infrastructure.\n  These models address different enterprise AI challenges with specialized capabilities:\n DeepSeek OCR explores visual-text compression for document processing. It can extract structured information from forms, invoices, diagrams, and complex documents with dense text layouts.\n MiniMax M2.1 is optimized for coding, tool use, instruction following, and long-horizon planning. It automates multilingual software development and executes complex, multi-step office workflows, empowering developers to build autonomous applications.\n Qwen3-VL-8B-Instruct delivers ssuperior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\n With SageMaker JumpStart, customers can deploy any of these models with just a few clicks to address their specific AI use cases.\n  To get started with these models, navigate to the SageMaker JumpStart model catalog in the SageMaker console or use the SageMaker Python SDK to deploy the models to your AWS account. For more information about deploying and using foundation models in SageMaker JumpStart, see the Amazon SageMaker JumpStart documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/01/new-models-on-sagemaker-jumpstart",
      "pubDate": "2026-02-02T21:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "lex",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-5f799d8bb52c",
      "title": "Build Production-Ready Drug Discovery and Robotics Pipelines with NVIDIA NIMs on SageMaker JumpStart",
      "description": "Amazon SageMaker JumpStart now enables one-click deployment of four NVIDIA NIMs models purpose-built for biosciences and physical AI: ProteinMPNN, Nemotron-3.5B-Instruct, MSA Search NIM, and Cosmos Reason. NVIDIA NIM™ provides prebuilt, optimized inference microservices for rapidly deploying the latest AI models on any NVIDIA-accelerated infrastructure. These models bring advanced capabilities spanning protein design, reasoning with configurable outputs, and physical world understanding, enabling customers to accelerate biosciences research, drug discovery, and embodied AI applications on AWS infrastructure.\n \nProteinMPNN enables fast and efficient protein sequence optimization guided by structural data. This NIM generates high-quality sequences with enhanced binding affinity and stability, validated through experimental results. Designed for scalability and flexibility, ProteinMPNN integrates seamlessly into protein engineering workflows, transforming applications like enzyme design and therapeutic development.\n \nMSA Search NIM supports GPU-accelerated Multiple Sequence Alignment (MSA) of a query amino acid sequence against a set of protein sequence databases. These databases are searched for similar sequences to the query and then the collection of sequences are aligned to establish similar regions even when the proteins have different lengths and motifs.\n \nNemotron-3.5B-Instruct delivers high reasoning performance, native tool calling support, and extended context processing with 256k token context window. This model employs an efficient hybrid Mixture-of-Experts (MoE) architecture to ensure higher throughput than its predecessors for agentic and coding workloads, while maintaining the reasoning depth of a larger model. It is ideal for building multi-agent workflows, developer productivity tools, processes automation, and for scientific and mathematical reasoning analysis, amongst others.\n \nCosmos Reason is an open , customizable, reasoning vision language model (VLM) for physical AI and robotics. It enables robots and vision AI agents to reason like humans, using prior knowledge, physics understanding, and common sense to understand and act in the real world. This model understands space, time, and fundamental physics, and can serve as a planning model to reason what steps an embodied agent might take next.\n \nWith SageMaker JumpStart, customers can deploy any of these models with just a few clicks to address their specific AI use cases.\n \nTo get started with these models, navigate to the SageMaker JumpStart model catalog in the SageMaker console or use the SageMaker Python SDK to deploy the models to your AWS account. For more information about deploying and using foundation models in SageMaker JumpStart, see the Amazon SageMaker JumpStart documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/02/accelerate-biosciences-and-robotics-with-NVIDIA-NIMs-on-sagemaker-jumpstart",
      "pubDate": "2026-02-02T21:24:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "lex",
        "experimental",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-99b1a7d7c5d6",
      "title": "Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI",
      "description": "Evaluating the performance of large language models (LLMs) goes beyond statistical metrics like perplexity or bilingual evaluation understudy (BLEU) scores. For most real-world generative AI scenarios, it’s crucial to understand whether a model is producing better outputs than a baseline or an earlier iteration. This is especially important for applications such as summarization, content generation, […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/evaluating-generative-ai-models-with-amazon-nova-llm-as-a-judge-on-amazon-sagemaker-ai/",
      "pubDate": "2026-01-30T21:07:34.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "sagemaker",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "sagemaker",
        "lex"
      ]
    },
    {
      "id": "aws-news-050a26e65371",
      "title": "Scale AI in South Africa using Amazon Bedrock global cross-Region inference with Anthropic Claude 4.5 models",
      "description": "In this post, we walk through how global cross-Region inference routes requests and where your data resides, then show you how to configure the required AWS Identity and Access Management (IAM) permissions and invoke Claude 4.5 models using the global inference profile Amazon Resource Name (ARN). We also cover how to request quota increases for your workload. By the end, you'll have a working implementation of global cross-Region inference in af-south-1.",
      "link": "https://aws.amazon.com/blogs/machine-learning/scale-ai-in-south-africa-using-amazon-bedrock-global-cross-region-inference-with-anthropic-claude-4-5-models/",
      "pubDate": "2026-01-30T17:12:02.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "iam"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "iam"
      ]
    },
    {
      "id": "aws-news-a76018245791",
      "title": "Create a customizable cross-company log lake, Part II: Build and add Amazon Bedrock",
      "description": "In this post, you learn how to build Log Lake, a customizable cross-company data lake for compliance-related use cases that combines AWS CloudTrail and Amazon CloudWatch logs. You'll discover how to set up separate tables for writing and reading, implement event-driven partition management using AWS Lambda, and transform raw JSON files into read-optimized Apache ORC format using AWS Glue jobs. Additionally, you'll see how to extend Log Lake by adding Amazon Bedrock model invocation logs to enable human review of agent actions with elevated permissions, and how to use an AI agent to query your log data without writing SQL.",
      "link": "https://aws.amazon.com/blogs/big-data/create-a-customizable-cross-company-log-lake-part-ii-build-and-add-amazon-bedrock/",
      "pubDate": "2026-01-27T17:46:45.000Z",
      "source": "bigDataBlog",
      "services": [
        "bedrock",
        "lambda",
        "glue",
        "cloudwatch"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lambda",
        "glue",
        "cloudwatch"
      ]
    },
    {
      "id": "aws-news-b2b47008b4a3",
      "title": "Amazon Bedrock now supports 1-hour duration for prompt caching",
      "description": "Amazon Bedrock now supports a 1-hour time-to-live (TTL) option for prompt caching for select Anthropic Claude models. With this update, you can extend the persistence of cached prompt prefixes from the default 5 minutes to 1 hour, improving cost efficiency and performance for long-running agentic workflows and multi-turn conversations.\n  Previously, cached content remained active for a fixed 5-minute window and refreshed when reused. With the new 1-hour TTL option, you can maintain context for users who interact less frequently, or for complex agents that require more time between steps—such as tool use, retrieval, and orchestration. The 1-hour TTL is also useful for longer sessions and batch processing where you want cached content to persist across extended periods.\n  1-hour TTL prompt caching is generally available for Anthropic’s Claude Sonnet 4.5, Claude Haiku 4.5, and Claude Opus 4.5 in all commercial AWS Regions and AWS GovCloud (US) Regions where these models are available. The 1-hour cache is billed at a different rate than the standard 5-minute cache. To learn more, refer to the Amazon Bedrock documentation and Amazon Bedrock Pricing page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-bedrock-one-hour-duration-prompt-caching/",
      "pubDate": "2026-01-26T22:30:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lex",
        "generally-available",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-3489e5c40783",
      "title": "Amazon SageMaker HyperPod introduces enhanced lifecycle scripts debugging",
      "description": "Amazon SageMaker HyperPod now provides enhanced troubleshooting capabilities for lifecycle scripts, making it easier to identify and resolve issues during cluster node provisioning. SageMaker HyperPod helps you provision resilient clusters for running AI/ML workloads and developing state-of-the-art models such as large language models (LLMs), diffusion models, and foundation models (FMs).\n  When lifecycle scripts encounter issues during cluster creation or node operations, you now receive detailed error messages that include the specific CloudWatch log group and log stream names where you can find execution logs for lifecycle scripts. You can view these error messages by running the DescribeCluster API or by viewing the cluster details page in the SageMaker console. The console also provides a \"View lifecycle script logs\" button that navigates directly to the relevant CloudWatch log stream, making it easier to locate logs. Additionally, CloudWatch logs for lifecycle scripts now include specific markers to help you track lifecycle script execution progress, including indicators for when the lifecycle script log begins, when scripts are being downloaded, when downloads complete, and when scripts succeed or fail. These markers help you quickly identify where issues occurred during the provisioning process. These enhancements reduce the time required to diagnose and fix lifecycle script failures, helping you get your HyperPod clusters up and running faster.\n  This feature is available in all AWS Regions where Amazon SageMaker HyperPod is supported. To learn more, see SageMaker HyperPod cluster management in the Amazon SageMaker Developer Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-sagemaker-hyperpod-lcs-enhanced-debug/",
      "pubDate": "2026-01-21T19:15:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "hyperpod",
        "cloudwatch"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "cloudwatch",
        "ga",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-456e02260dac",
      "title": "Amazon Bedrock Reserved Tier available now for Claude Sonnet 4.5 in AWS GovCloud (US-West)",
      "description": "Today, Amazon Bedrock introduces the expansion of the Reserved service tier designed for workloads requiring predictable performance and guaranteed tokens-per-minute capacity. The Reserved tier provides the ability to reserve prioritized compute capacity, keeping service levels predictable for your mission critical applications. It also includes the flexibility to allocate different input and output tokens-per-minute capacities to match the exact requirements of your workload and control cost. This is particularly valuable because many workloads have asymmetric token usage patterns. For instance, summarization tasks consume many input tokens but generate fewer output tokens, while content generation applications require less input and more output capacity. When your application needs more tokens-per-minute capacity than what you reserved , the service automatically overflows to the pay-as-you-go Standard tier, ensuring uninterrupted operations. The Reserved tier is available today for Anthropic Claude Sonnet 4.5 in AWS GovCloud (US-West). Customers can reserve capacity for 1 month or 3 month duration. Customers pay a fixed price per 1K tokens-per-minute and are billed monthly. Amazon Bedrock Reserved Tier is available for customers in AWS GovCloud (US-West) via GOV-CRIS cross-region profile.\n  With the expansion of the Reserved service tier, Amazon Bedrock continues to provide more choice to customers, helping them develop, scale, and deploy applications and agents that improve productivity and customer experiences while balancing performance and cost requirements.\n  For more information about the AWS Regions where Amazon Bedrock Reserved tier is available, refer to the Documentation. To get access to the Reserved tier, please contact your AWS account team.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-bedrock-reserved-tier-for-claude-sonnet-in-govcloud/",
      "pubDate": "2026-01-21T17:02:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "lex",
        "expansion"
      ]
    }
  ]
}