{
  "lastUpdated": "2026-01-14T06:18:24.035Z",
  "category": "foundation-models",
  "totalItems": 8,
  "items": [
    {
      "id": "aws-news-7a070b83b1b0",
      "title": "Amazon SageMaker HyperPod now validates service quotas before creating clusters on console",
      "description": "Amazon SageMaker HyperPod console now validates service quotas for your AWS account before initiating cluster creation, enabling you to confirm sufficient quota availability before provisioning begins. SageMaker HyperPod helps you provision resilient clusters for running AI/ML workloads and developing state-of-the-art models such as large language models (LLMs), diffusion models, and foundation models (FMs).\n  When creating large-scale AI/ML clusters, you need to ensure your account has sufficient quotas for instances, storage, and networking resources, but quota validation previously required manual checks across multiple AWS services, often resulting in failed cluster creation attempts and wasted time if you miss requesting quota limit increases. The new quota validation capability in the SageMaker HyperPod console automatically checks your account-level quotas against your cluster configuration, including instance type limits, EBS volume sizes, and VPC-related quotas when creating new resources. The validation displays a clear table showing expected utilization, applied quota values, and compliance status for each quota. When quotas may be exceeded, you receive a warning alert with direct links to the Service Quotas console to request increases.\n  This feature is available in all AWS Regions where Amazon SageMaker HyperPod is supported. For a complete list of service quota validation checks performed, refer to the Amazon SageMaker HyperPod User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-sagemaker-hyperpod-validates-service-quotas/",
      "pubDate": "2026-01-12T18:10:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "hyperpod"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-12eadb812c54",
      "title": "How Omada Health scaled patient care by fine-tuning Llama models on Amazon SageMaker AI",
      "description": "This post is co-written with Sunaina Kavi, AI/ML Product Manager at Omada Health. Omada Health, a longtime innovator in virtual healthcare delivery, launched a new nutrition experience in 2025, featuring OmadaSpark, an AI agent trained with robust clinical input that delivers real-time motivational interviewing and nutrition education. It was built on AWS. OmadaSpark was designed […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-omada-health-scaled-patient-care-by-fine-tuning-llama-models-on-amazon-sagemaker-ai/",
      "pubDate": "2026-01-12T16:56:12.000Z",
      "source": "mlBlog",
      "services": [
        "nova",
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova",
        "sagemaker",
        "launch"
      ]
    },
    {
      "id": "aws-news-9aa4ee62ff93",
      "title": "Crossmodal search with Amazon Nova Multimodal Embeddings",
      "description": "In this post, we explore how Amazon Nova Multimodal Embeddings addresses the challenges of crossmodal search through a practical ecommerce use case. We examine the technical limitations of traditional approaches and demonstrate how Amazon Nova Multimodal Embeddings enables retrieval across text, images, and other modalities. You learn how to implement a crossmodal search system by generating embeddings, handling queries, and measuring performance. We provide working code examples and share how to add these capabilities to your applications.",
      "link": "https://aws.amazon.com/blogs/machine-learning/crossmodal-search-with-amazon-nova-multimodal-embeddings/",
      "pubDate": "2026-01-10T00:06:16.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-304967fedc03",
      "title": "Agentic QA automation using Amazon Bedrock AgentCore Browser and Amazon Nova Act",
      "description": "In this post, we explore how agentic QA automation addresses these challenges and walk through a practical example using Amazon Bedrock AgentCore Browser and Amazon Nova Act to automate testing for a sample retail application.",
      "link": "https://aws.amazon.com/blogs/machine-learning/agentic-qa-automation-using-amazon-bedrock-agentcore-browser-and-amazon-nova-act/",
      "pubDate": "2025-12-24T17:20:16.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "nova"
      ]
    },
    {
      "id": "aws-news-c58ed5025d6d",
      "title": "AWS AI League: Model customization and agentic showdown",
      "description": "In this post, we explore the new AWS AI League challenges and how they are transforming how organizations approach AI development. The grand finale at AWS re:Invent 2025 was an exciting showcase of their ingenuity and skills.",
      "link": "https://aws.amazon.com/blogs/machine-learning/aws-ai-league-model-customization-and-agentic-showdown/",
      "pubDate": "2025-12-23T17:36:07.000Z",
      "source": "mlBlog",
      "services": [
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "organizations",
        "ga"
      ]
    },
    {
      "id": "aws-news-ec7c2bf10f19",
      "title": "Accelerate Enterprise AI Development using Weights & Biases and Amazon Bedrock AgentCore",
      "description": "In this post, we demonstrate how to use Foundation Models (FMs) from Amazon Bedrock and the newly launched Amazon Bedrock AgentCore alongside W&B Weave to help build, evaluate, and monitor enterprise AI solutions. We cover the complete development lifecycle from tracking individual FM calls to monitoring complex agent workflows in production.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-enterprise-ai-development-using-weights-biases-weave-and-amazon-bedrock-agentcore/",
      "pubDate": "2025-12-23T17:32:23.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex",
        "launch"
      ]
    },
    {
      "id": "aws-news-d9db993d9387",
      "title": "MiniMax-M2 is now available on Amazon SageMaker JumpStart",
      "description": "MiniMax-M2 is now available on Amazon SageMaker JumpStart, providing customers with immediate access to deploy this efficient open-source model in minutes. With SageMaker JumpStart, you can quickly discover, evaluate, and deploy MiniMax-M2 using either SageMaker Studio's intuitive interface or the SageMaker Python SDK for programmatic deployment.\n \nMiniMax-M2 redefines efficiency for agents. It's a compact, fast, and cost-effective MoE model (230 billion total parameters with 10 billion active parameters) built for elite performance in coding and agentic tasks, all while maintaining powerful general intelligence.\n \nTo learn more about deploying foundation models with SageMaker JumpStart, deployment options with the SDK, and best practices for implementation, refer to our documentation.\n \nMiniMax-M2 is available in US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Mumbai), Asia Pacific (Sydney), Asia Pacific (Jakarta), Canada (Central), Europe (Frankfurt), Europe (Stockholm), Europe (Ireland), Europe (London), Europe (Paris), South America (São Paulo).",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/minimax-m2-on-sagemaker-jumpstart",
      "pubDate": "2025-12-23T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-b1018aefba54",
      "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers",
      "description": "In this post, we demonstrate how to deploy the DeepSeek-R1-Distill-Qwen-32B model using AWS DLCs for vLLMs on Amazon EKS, showcasing how these purpose-built containers simplify deployment of this powerful open source inference engine. This solution can help you solve the complex infrastructure challenges of deploying LLMs while maintaining performance and cost-efficiency.",
      "link": "https://aws.amazon.com/blogs/architecture/deploy-llms-on-amazon-eks-using-vllm-deep-learning-containers/",
      "pubDate": "2025-08-14T15:09:51.000Z",
      "source": "architectureBlog",
      "services": [
        "lex",
        "eks"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "lex",
        "eks"
      ]
    }
  ]
}