{
  "lastUpdated": "2025-12-31T06:18:08.969Z",
  "category": "foundation-models",
  "totalItems": 9,
  "items": [
    {
      "id": "aws-news-304967fedc03",
      "title": "Agentic QA automation using Amazon Bedrock AgentCore Browser and Amazon Nova Act",
      "description": "In this post, we explore how agentic QA automation addresses these challenges and walk through a practical example using Amazon Bedrock AgentCore Browser and Amazon Nova Act to automate testing for a sample retail application.",
      "link": "https://aws.amazon.com/blogs/machine-learning/agentic-qa-automation-using-amazon-bedrock-agentcore-browser-and-amazon-nova-act/",
      "pubDate": "2025-12-24T17:20:16.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "nova"
      ]
    },
    {
      "id": "aws-news-c58ed5025d6d",
      "title": "AWS AI League: Model customization and agentic showdown",
      "description": "In this post, we explore the new AWS AI League challenges and how they are transforming how organizations approach AI development. The grand finale at AWS re:Invent 2025 was an exciting showcase of their ingenuity and skills.",
      "link": "https://aws.amazon.com/blogs/machine-learning/aws-ai-league-model-customization-and-agentic-showdown/",
      "pubDate": "2025-12-23T17:36:07.000Z",
      "source": "mlBlog",
      "services": [
        "organizations"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "organizations",
        "ga"
      ]
    },
    {
      "id": "aws-news-ec7c2bf10f19",
      "title": "Accelerate Enterprise AI Development using Weights & Biases and Amazon Bedrock AgentCore",
      "description": "In this post, we demonstrate how to use Foundation Models (FMs) from Amazon Bedrock and the newly launched Amazon Bedrock AgentCore alongside W&B Weave to help build, evaluate, and monitor enterprise AI solutions. We cover the complete development lifecycle from tracking individual FM calls to monitoring complex agent workflows in production.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-enterprise-ai-development-using-weights-biases-weave-and-amazon-bedrock-agentcore/",
      "pubDate": "2025-12-23T17:32:23.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex",
        "launch"
      ]
    },
    {
      "id": "aws-news-6465bc8f0423",
      "title": "Accelerating your marketing ideation with generative AI – Part 1: From idea to generation with the Amazon Nova foundation models",
      "description": "In this post, the first of a series of three, we focus on how you can use Amazon Nova to streamline, simplify, and accelerate marketing campaign creation through generative AI. We show how Bancolombia, one of Colombia’s largest banks, is experimenting with the Amazon Nova models to generate visuals for their marketing campaigns.",
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerating-your-marketing-ideation-with-generative-ai-part-1-from-idea-to-generation-with-the-amazon-nova-foundation-models/",
      "pubDate": "2025-12-23T17:06:31.000Z",
      "source": "mlBlog",
      "services": [
        "nova"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "nova"
      ]
    },
    {
      "id": "aws-news-d9db993d9387",
      "title": "MiniMax-M2 is now available on Amazon SageMaker JumpStart",
      "description": "MiniMax-M2 is now available on Amazon SageMaker JumpStart, providing customers with immediate access to deploy this efficient open-source model in minutes. With SageMaker JumpStart, you can quickly discover, evaluate, and deploy MiniMax-M2 using either SageMaker Studio's intuitive interface or the SageMaker Python SDK for programmatic deployment.\n \nMiniMax-M2 redefines efficiency for agents. It's a compact, fast, and cost-effective MoE model (230 billion total parameters with 10 billion active parameters) built for elite performance in coding and agentic tasks, all while maintaining powerful general intelligence.\n \nTo learn more about deploying foundation models with SageMaker JumpStart, deployment options with the SDK, and best practices for implementation, refer to our documentation.\n \nMiniMax-M2 is available in US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Mumbai), Asia Pacific (Sydney), Asia Pacific (Jakarta), Canada (Central), Europe (Frankfurt), Europe (Stockholm), Europe (Ireland), Europe (London), Europe (Paris), South America (São Paulo).",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/minimax-m2-on-sagemaker-jumpstart",
      "pubDate": "2025-12-23T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-fd4ae9e56697",
      "title": "Deploy Mistral AI’s Voxtral on Amazon SageMaker AI",
      "description": "In this post, we demonstrate hosting Voxtral models on Amazon SageMaker AI endpoints using vLLM and the Bring Your Own Container (BYOC) approach. vLLM is a high-performance library for serving large language models (LLMs) that features paged attention for improved memory management and tensor parallelism for distributing models across multiple GPUs.",
      "link": "https://aws.amazon.com/blogs/machine-learning/deploy-mistral-ais-voxtral-on-amazon-sagemaker-ai/",
      "pubDate": "2025-12-22T18:32:19.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-4bd647ddd1d8",
      "title": "Build a multimodal generative AI assistant for root cause diagnosis in predictive maintenance using Amazon Bedrock",
      "description": "In this post, we demonstrate how to implement a predictive maintenance solution using Foundation Models (FMs) on Amazon Bedrock, with a case study of Amazon's manufacturing equipment within their fulfillment centers. The solution is highly adaptable and can be customized for other industries, including oil and gas, logistics, manufacturing, and healthcare.",
      "link": "https://aws.amazon.com/blogs/machine-learning/build-a-multimodal-generative-ai-assistant-for-root-cause-diagnosis-in-predictive-maintenance-using-amazon-bedrock/",
      "pubDate": "2025-12-22T18:21:28.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "ga"
      ]
    },
    {
      "id": "aws-news-adc0a5875ccc",
      "title": "Bi-directional streaming for real-time agent interactions now available in Amazon Bedrock AgentCore Runtime",
      "description": "In this post, you will learn about bi-directional streaming on AgentCore Runtime and the prerequisites to create a WebSocket implementation. You will also learn how to use Strands Agents to implement a bi-directional streaming solution for voice agents.",
      "link": "https://aws.amazon.com/blogs/machine-learning/bi-directional-streaming-for-real-time-agent-interactions-now-available-in-amazon-bedrock-agentcore-runtime/",
      "pubDate": "2025-12-18T17:00:21.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "now-available"
      ]
    },
    {
      "id": "aws-news-b1018aefba54",
      "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers",
      "description": "In this post, we demonstrate how to deploy the DeepSeek-R1-Distill-Qwen-32B model using AWS DLCs for vLLMs on Amazon EKS, showcasing how these purpose-built containers simplify deployment of this powerful open source inference engine. This solution can help you solve the complex infrastructure challenges of deploying LLMs while maintaining performance and cost-efficiency.",
      "link": "https://aws.amazon.com/blogs/architecture/deploy-llms-on-amazon-eks-using-vllm-deep-learning-containers/",
      "pubDate": "2025-08-14T15:09:51.000Z",
      "source": "architectureBlog",
      "services": [
        "lex",
        "eks"
      ],
      "categories": [
        "foundation-models"
      ],
      "tags": [
        "lex",
        "eks"
      ]
    }
  ]
}