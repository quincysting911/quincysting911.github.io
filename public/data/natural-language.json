{
  "lastUpdated": "2025-09-28T13:06:00.700Z",
  "category": "natural-language",
  "totalItems": 32,
  "items": [
    {
      "id": "aws-news-13a535dc87a5",
      "title": "Amazon EBS increases the maximum size and provisioned performance of General Purpose (gp3) volumes",
      "description": "Amazon Elastic Block Store (Amazon EBS) now supports higher volume-level limits for its General Purpose (gp3) volumes. With this update, gp3 volumes can scale up to 64 TiB in size (4X the previous 16 TiB limit), up to 80,000 IOPS (5X the previous 16,000 IOPS limit), and up to 2,000 MiB/s throughput (2X the previous 1,000 MiB/s limit).\n  These expanded limits help reduce operational complexity for storage-intensive workloads by enabling gp3 volumes with larger capacity and higher performance. You can consolidate multiple striped volumes into a single gp3 volume, streamline architectures, and lower management overhead. The increased limits particularly benefit customers running containerized workloads with limited support for striping multiple volumes, applications that rely on single-volume architectures, and growing workloads approaching current gp3 limits. The pricing model remains unchanged: you pay for storage plus any additional IOPS and throughput provisioned beyond the baseline performance.\n  The new gp3 limits are available in all AWS Commercial Regions and AWS GovCloud (US) Regions where gp3 volumes are available. To get started and learn more, please visit the Amazon EBS user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ebs-size-provisioned-performance-gp3-volumes/",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "lex",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-82a70dbffaef",
      "title": "AWS Compute Optimizer now supports 99 new Amazon EC2 instance types",
      "description": "AWS Compute Optimizer now supports 99 additional Amazon Elastic Compute Cloud (Amazon EC2) instance types. These enhancements help you identify additional savings opportunities across your EC2 instances without specialized knowledge or manual analysis.\n  Compute Optimizer has expanded support to include the latest generation Compute Optimized (C8gn, C8gd), General Purpose (M8i, M8i-flex, M8gd), Memory Optimized (R8i, R8i-flex, R8gd), and Storage Optimized (I8ge) instance types. This expansion enables Compute Optimizer to help you take advantage of the price-to-performance improvements offered by the newest instance types.\n  This new feature is available in all AWS Regions where Compute Optimizer is available except the AWS GovCloud (US) and the China Regions. For more information about Compute Optimizer, visit our product page and documentation. You can start using Compute Optimizer through the AWS Management Console, AWS CLI, or AWS SDK.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-compute-optimizer-99-new-amazon-ec2-instance-types/",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "lex",
        "new-feature",
        "improvement",
        "enhancement",
        "support",
        "expansion"
      ]
    },
    {
      "id": "aws-news-47da908bba5a",
      "title": "Amazon RDS for Db2 now offers Reserved Instances",
      "description": "Amazon Relational Database Service (RDS) for Db2 now offers Reserved Instances with up to 47% cost savings compared to On-Demand prices. The option to use Reserved Instances is available for all supported instance types.\n  Amazon RDS for Db2 Reserved Instances provide size flexibility for both Bring Your Own License (BYOL) and Db2 license purchased through AWS Marketplace. With Reserved Instances size flexibility, the discounted rate for Reserved Instances automatically applies to usage of any size in the same instance family. For example, if you purchase a db.r7i.2xlarge Reserved Instance in US East (N. Virginia), the discounted rate of this Reserved Instance can automatically apply to 2 db.r7i.xlarge instances. For information on RDS Reserved Instances, refer to Reserved DB instances for Amazon RDS.\n  You can purchase Reserved Instances through the AWS Management Console, AWS CLI, or AWS SDK. For detailed pricing information and purchase options, refer to Amazon RDS for Db2 Pricing.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-rds-db2-offers-reserved-instances",
      "pubDate": "2025-09-26T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "support"
      ]
    },
    {
      "id": "aws-news-1510ce03e38b",
      "title": "How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock",
      "description": "In this post, we explore how we built a multi-agent conversational AI system using Amazon Bedrock that delivers knowledge-grounded property investment advice. We explore the agent architecture, model selection strategy, and comprehensive continuous evaluation system that facilitates quality conversations while facilitating rapid iteration and improvement.",
      "link": "https://aws.amazon.com/blogs/machine-learning/how-prophero-built-an-intelligent-property-investment-advisor-with-continuous-evaluation-using-amazon-bedrock/",
      "pubDate": "2025-09-25T19:25:23.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock"
      ],
      "categories": [
        "generative-ai",
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "bedrock",
        "improvement"
      ]
    },
    {
      "id": "aws-news-6ff42c19e362",
      "title": "AWS Network Firewall enhances application layer traffic controls",
      "description": "AWS Network Firewall, a managed service that makes it easy to deploy essential network protections for your Amazon VPCs, now provides enhanced default rules to handle TLS client hellos, and HTTP requests split across multiple packets. This update introduces new application layer drop and alert established default stateful actions, enabling customers to maintain security controls while supporting modern TLS implementations and large HTTP requests.\n  These enhancements help customers implement robust security policies without writing complex custom rules. Security teams can now effectively inspect and filter traffic where key information is segmented across multiple packets, while maintaining visibility through detailed logging options, making it easier to secure applications using modern protocols and encryption standards.\n  This capability is available in all AWS Regions where AWS Network Firewall is supported.\n  To learn more, refer to AWS Network Firewall service documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-network-firewall-enhances-application-layer-traffic-controls",
      "pubDate": "2025-09-25T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "update",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-9a877ca57412",
      "title": "Research and Engineering Studio on AWS 2025.09 is now available",
      "description": "Today we’re announcing Research and Engineering Studio (RES) on AWS 2025.09, which brings support for fractional GPUs, simplified AMI management, and enhanced deployment flexibility. This release also expands regional availability to include four additional AWS commercial Regions.\n  Research and Engineering Studio on AWS is an open source solution that provides a web-based portal for administrators to create and manage secure cloud-based research and engineering environments. RES enables scientists and engineers to access powerful Windows and Linux virtual desktops with pre-installed applications and shared resources, without requiring cloud expertise.\n  Version 2025.09 adds support for Amazon EC2 g6f instances, enabling GPU fractionalization for more efficient resource utilization in graphics-intensive workloads. The release also introduces Systems Manager Parameter Alias support for AMI IDs, simplifying the management of project-specific images, and enables integration with existing Amazon Cognito user pools for streamlined authentication setup during deployment. Administrators can now also customize CIDR ranges in the AWS CloudFormation external resources template for better network planning and integration with existing resources.\n  This release expands regional availability to include Asia Pacific (Osaka), Asia Pacific (Jakarta), Middle East (UAE), and South America (São Paulo). To learn more about RES 2025.09, including detailed release notes and deployment instructions, visit the Research and Engineering Studio documentation or check out the RES GitHub repository.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/research-engineering-studio-aws-2025-09-now-available",
      "pubDate": "2025-09-25T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "now-available",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-f76fdca33bd0",
      "title": "Running deep research AI agents on Amazon Bedrock AgentCore",
      "description": "AI agents are evolving beyond basic single-task helpers into more powerful systems that can plan, critique, and collaborate with other agents to solve complex problems. Deep Agents—a recently introduced framework built on LangGraph—bring these capabilities to life, enabling multi-agent workflows that mirror real-world team dynamics. The challenge, however, is not just building such agents but […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/running-deep-research-ai-agents-on-amazon-bedrock-agentcore/",
      "pubDate": "2025-09-23T20:35:23.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "agentcore",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "agentcore",
        "lex"
      ]
    },
    {
      "id": "aws-news-33355eceeb89",
      "title": "Use Apache Airflow workflows to orchestrate data processing on Amazon SageMaker Unified Studio",
      "description": "Orchestrating machine learning pipelines is complex, especially when data processing, training, and deployment span multiple services and tools. In this post, we walk through a hands-on, end-to-end example of developing, testing, and running a machine learning (ML) pipeline using workflow capabilities in Amazon SageMaker, accessed through the Amazon SageMaker Unified Studio experience. These workflows are powered by Amazon Managed Workflows for Apache Airflow.",
      "link": "https://aws.amazon.com/blogs/big-data/use-apache-airflow-workflows-to-orchestrate-data-processing-on-amazon-sagemaker-unified-studio/",
      "pubDate": "2025-09-22T16:56:56.000Z",
      "source": "bigDataBlog",
      "services": [
        "sagemaker",
        "unified studio",
        "lex"
      ],
      "categories": [
        "machine-learning",
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "sagemaker",
        "unified studio",
        "lex"
      ]
    },
    {
      "id": "aws-news-77e33a4e16c4",
      "title": "Amazon Connect flow designer now supports analytics mode",
      "description": "Amazon Connect now offers new enhanced analytics in the drag-and-drop flow designer that help you make data-driven decisions when building and optimizing your flows. Amazon Connect flows allow you to create end-to-end self-service and automated customer experiences such as interactive voice response (IVR), step-by-step guides, and back office processes and tasks. With this launch, you can now view aggregate metrics on how customers move through each step in the flow including where they run into errors or abandon the experience. For example, you can see how many conversational AI interactions result in transfers to agent queues or when customers end up in the wrong queue because an error in the flow configuration. These new capabilities help you identify behavioral patterns and evaluate root causes, allowing you to deliver better outcomes for customers.\n  This new capability is included with Amazon Connect (with unlimited AI) pricing. To learn more about this feature, see the Amazon Connect Administrator Guide. This feature is available in all AWS regions that offers Amazon Connect. To learn more about Amazon Connect, the AWS cloud-based contact center, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-connect-flow-designer-analytics-mode/",
      "pubDate": "2025-09-22T16:00:00.000Z",
      "source": "whatsNew",
      "services": [],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "launch",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-d073fb3f6dab",
      "title": "Announcing AWS Neuron SDK 2.26.0",
      "description": "Today, AWS announces the general availability of Neuron SDK 2.26.0, delivering improvements for deep learning workloads on AWS Inferentia and Trainium-based instances. This release introduces support for PyTorch 2.8 and JAX 0.6.2, along with enhanced inference capabilities on Trainium2 (Trn2) instances. These updates enable developers to leverage the latest frameworks while benefiting from improved model deployment flexibility and performance optimizations.\n  With Neuron SDK 2.26.0, customers can now deploy FLUX.1-dev image generation model, along with Llama 4 Scout and Maverick variants (beta) on Trn2 instances. The release introduces expert parallelism support (beta) for efficient distribution of Mixture-of-Experts (MoE) models across multiple NeuronCores, and adds new capabilities through new Neuron Kernel Interface (NKI) APIs. The updated Neuron Profiler provides improved capabilities, including system profile grouping for distributed workloads.\n  The new SDK version is available in all AWS Regions supporting Inferentia and Trainium instances, offering enhanced performance and monitoring capabilities for machine learning workloads.\n  To learn more and for a full list of new features and enhancements, see:\n  \n \n \nAWS Neuron 2.26.0 release notes\n \n \nTrn2 Instances\n \n \nTrn1 Instances\n \n \nInf2 Instances",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-neuron-2-26-announce/",
      "pubDate": "2025-09-19T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex",
        "trainium",
        "inferentia",
        "neuron"
      ],
      "categories": [
        "generative-ai",
        "machine-learning",
        "natural-language"
      ],
      "tags": [
        "lex",
        "trainium",
        "inferentia",
        "neuron",
        "beta",
        "new-feature",
        "update",
        "improvement",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-f560c17f7d21",
      "title": "AWS Organizations supports full IAM policy language for service control policies (SCPs)",
      "description": "AWS Organizations now offers full IAM policy language support for service control policies (SCPs), enabling you to write SCPs with the same flexibility as IAM managed policies. With this launch, SCPs now support use of conditions, individual resource ARNs, and the NotAction element with Allow statements. Additionally, you can now use wildcards at the beginning or middle of Action element strings and the NotResource element.\n  With these policy language enhancements, you can now create more concise and precise policies to implement sophisticated permissions guardrails across your organization. For example, you can restrict access to specific resources with condition statements. The enhanced functionality maintains backward compatibility with existing SCPs, so no changes to current policies are required.\n  This feature is now available in all AWS commercial and AWS GovCloud (US) Regions.\n  To learn more about the enhanced SCP capabilities, see service control policies in the AWS Organizations User Guide and AWS blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-organizations-iam-language-service-control-policies/",
      "pubDate": "2025-09-19T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language",
        "ai-safety"
      ],
      "tags": [
        "lex",
        "launch",
        "ga",
        "now-available",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-c753ba529f3a",
      "title": "Scale visual production using Stability AI Image Services in Amazon Bedrock",
      "description": "This post was written with Alex Gnibus of Stability AI. Stability AI Image Services are now available in Amazon Bedrock, offering ready-to-use media editing capabilities delivered through the Amazon Bedrock API. These image editing tools expand on the capabilities of Stability AI’s Stable Diffusion 3.5 models (SD3.5) and Stable Image Core and Ultra models, which […]",
      "link": "https://aws.amazon.com/blogs/machine-learning/scale-visual-production-using-stability-ai-image-services-in-amazon-bedrock/",
      "pubDate": "2025-09-18T21:25:49.000Z",
      "source": "mlBlog",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "foundation-models",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available"
      ]
    },
    {
      "id": "aws-news-b61969a232f9",
      "title": "Second-generation AWS Outposts racks now supported in the AWS Canada (Central) and US West (N. California) Regions",
      "description": "Second-generation AWS Outposts racks are now supported in the AWS Canada (Central) and US West (N. California) Regions. Outposts racks extend AWS infrastructure, AWS services, APIs, and tools to virtually any on-premises data center or colocation space for a truly consistent hybrid experience.\n \nOrganizations from startups to enterprises and the public sector in and outside of Canada and the US can now order their Outposts racks connected to these two new supported Regions, optimizing for their latency and data residency needs. Outposts allows customers to run workloads that need low-latency access to on-premises systems locally while connecting back to their home Region for application management. Customers can also use Outposts and AWS services to manage and process data that needs to remain on-premises to meet data residency requirements. This regional expansion provides additional flexibility in the AWS Regions that customers’ Outposts can connect to.\n \nTo learn more about second-generation Outposts racks, read this blog post and user guide. For the most updated list of countries and territories and the AWS Regions where second-generation Outposts racks are supported, check out the Outposts racks FAQs page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/second-generation-outposts-racks-canada-us-west-regions",
      "pubDate": "2025-09-18T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "ga",
        "update",
        "support",
        "expansion"
      ]
    },
    {
      "id": "aws-news-3150c633eaf5",
      "title": "Stability AI Image Services now available in Amazon Bedrock",
      "description": "Amazon Bedrock announces the availability of Stability AI Image Services, a comprehensive suite of 9 specialized image editing tools designed to accelerate professional creative workflows. Stability AI Image Services enable granular control over image editing with a range of tools designed to work with your creative process, allowing you to take a single concept from ideation to finished product with precision and flexibility.\n  Stability AI Image Services offers two categories of image editing capabilities: Edit tools: Remove Background, Erase Object, Search and Replace, Search and Recolor, and Inpaint let you make targeted modifications to specific parts of your images. Control tools: Structure, Sketch, Style Guide, and Style Transfer give you powerful ways to generate variations based on existing images or sketches.\n  Stability AI Image Services is now available in Amazon Bedrock through the API and is supported in US West (Oregon), US East (N. Virginia), and US East (Ohio). For more information on supported regions, visit the Amazon Bedrock Model Support by Regions guide. For more details about Stability AI Image Services and its capabilities, visit the Stability AI product page and Stability AI documentation page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/stability-ai-image-services-generally-available-amazon-bedrock",
      "pubDate": "2025-09-18T16:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "foundation-models",
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available",
        "support"
      ]
    },
    {
      "id": "aws-news-bcb543e57edc",
      "title": "Amazon Lex provides enhanced confirmation and currency built-in slots to 10 additional languages",
      "description": "Amazon Lex now provides support for confirmation and currency slot types in 10 additional languages: Portuguese, Catalan, French, Italian, German, Spanish, Mandarin, Cantonese, Japanese, and Korean. Built-in slots help you build more natural and efficient conversations by understanding synonyms of what you user says and resolving those inputs to a standard format. The confirmation slot helps understand various expressions of user acknowledgement and converts them into ‘Yes’, ‘No’, “Don’t know’‘, or ‘Maybe’. The currency slot helps identify currency and represents the input in a structured way. For example, when a user says “nope” or “absolutely not”, the confirmation slot resolves to ‘No’ or when the user says “1 dollar’, the currency slot resolves it to ”USD 1.00“. These built-in slots help you build more natural and efficient conversational experiences.\n  This feature is available in all commercial AWS Regions where Amazon Lex operates. To learn more about these features, visit Amazon Lex documentation or to learn how Amazon Connect and Amazon Lex deliver cloud-based conversational AI experiences for contact centers, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-lex-enhanced-confirmation-currency-slots-to-languages/",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "support"
      ]
    },
    {
      "id": "aws-news-0f3ee5cb5f1f",
      "title": "Qwen3 models are now available fully managed in Amazon Bedrock",
      "description": "Amazon Bedrock continues to expand model choice by adding four Qwen3 open weight foundation models, now available as fully managed, serverless offerings. The lineup includes: Qwen3-Coder-480B-A35B-Instruct, Qwen3-Coder-30B-A3B-Instruct, Qwen3-235B-A22B-Instruct-2507, and Qwen3-32B for efficient dense computation. These models feature both dense and Mixture-of-Experts (MoE) architectures, providing flexible options for various development needs.\n  These open weight models enable you to build powerful AI applications with advanced agentic capabilities, without managing any infrastructure. The two Qwen3-Coder models excel at agentic coding and complex software engineering tasks, offering state-of-the-art performance for function calling and tool use. The 235B model delivers efficient general reasoning and instruction following across diverse tasks, while the 32B dense model provides a more traditional architecture suitable for a wide range of computational tasks.\n  Qwen3 models (32B, Coder-30B) are available today in the US East (N. Virginia), US West (Oregon), Asia Paciﬁc (Mumbai, Tokyo), Europe (Ireland, London, Milan, Stockholm), and South America (São Paulo) AWS Regions. Qwen 235B is available today in theUS West (Oregon), Asia Paciﬁc (Mumbai, Tokyo), and Europe (London, Milan, Stockholm) AWS Regions. Qwen Coder-480B is available today in the US West (Oregon), Asia Paciﬁc (Mumbai, Tokyo), and Europe (London, Stockholm) AWS Regions. Check the full Region list for future updates. To learn more, read the blog, product page, Amazon Bedrock pricing, and documentation. To get started with Qwen in Amazon Bedrock, visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/qwen3-models-fully-managed-amazon-bedrock",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "foundation-models",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available",
        "update"
      ]
    },
    {
      "id": "aws-news-b2e219fc1dd9",
      "title": "DeepSeek-V3.1 model now available fully managed in Amazon Bedrock",
      "description": "DeepSeek-V3.1 is now available as a fully managed foundation model in Amazon Bedrock. This advanced open weight model allows you to switch between thinking mode for detailed step-by-step analysis and non-thinking mode for quicker responses. With comprehensive multilingual support, it delivers enhanced accuracy and reduced hallucinations compared to previous DeepSeek models, while maintaining visibility into its decision-making process.\n  You can use DeepSeek-V3.1's enterprise-grade capabilities across critical business functions, from state-of-the-art software development to complex mathematical reasoning and data analysis. The model excels at sophisticated problem-solving tasks, demonstrating strong performance in coding benchmarks and technical challenges. Its enhanced tool-calling capabilities and seamless workflow integration make it ideal for building AI agents and automating enterprise processes, while its transparent reasoning approach helps teams understand and trust its outputs.\n  \n DeepSeek-V3.1 is now available in the US West (Oregon), Asia Paciﬁc (Tokyo), Asia Paciﬁc (Mumbai), Europe (London), and Europe (Stockholm) AWS Regions. To learn more, read the blog, product page, Amazon Bedrock pricing, and documentation. To get started with DeepSeek in Amazon Bedrock, visit the Amazon Bedrock console.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/deepseek-v3-1-model-fully-managed-amazon-bedrock",
      "pubDate": "2025-09-18T14:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "bedrock",
        "lex"
      ],
      "categories": [
        "generative-ai",
        "foundation-models",
        "natural-language"
      ],
      "tags": [
        "bedrock",
        "lex",
        "now-available",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-7d8f6bbc8755",
      "title": "AWS Step Functions now supports IPv6 with dual-stack endpoints",
      "description": "AWS Step Functions adds supports for IPv6. You can now send IPV6 traffic to AWS Step Functions via new dual-stack IPv4 and IPv6 endpoints. AWS Step Functions is a visual workflow service that enables customers to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services. This enhancement addresses the growing need for IP addresses as the internet continues to expand, providing a larger address space than the traditional IPv4 format.\n  With IPv6 support, organizations modernizing their applications can now build serverless workflows without being constrained by limited IPv4 address space. The new dual-stack endpoints support both IPv4 and IPv6 protocols while maintaining backwards compatibility with existing IPv4 endpoints. Step Functions also supports IPv6 connectivity through PrivateLink interface Virtual Private Cloud (VPC) endpoints, enabling you to access the service privately without traversing the public internet. This enables organizations operating in IPv6 environments to natively integrate with Step Functions without requiring complex translation mechanisms between IPv6 and IPv4.\n  IPv6 support for AWS Step Functions is now generally available in US East (Ohio), US East (N. Virginia), US West (Oregon), US West (N. California) as well as AWS GovCloud (US-East), and AWS GovCloud (US-West) Regions, where AWS Step Functions is available.\n  To learn more about IPv6 support on AWS, visit the documentation page.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-step-functions-ipv6-dual-stack-endpoints/",
      "pubDate": "2025-09-18T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "generally-available",
        "ga",
        "enhancement",
        "support"
      ]
    },
    {
      "id": "aws-news-ab65551d4b6c",
      "title": "Amazon OpenSearch Serverless now supports Disk-Optimized Vectors",
      "description": "We are excited to announce the launch of disk-optimized vector support for Amazon OpenSearch Serverless, offering customers a cost-effective solution for vector search operations without compromising on accuracy and recall rates. This new feature enables organizations to implement high-quality vector search capabilities while significantly reducing operational costs.\n  With the introduction of Disk Optimized Vectors, customers can now choose between memory-optimized and disk-optimized vector storage options. The disk-optimized option delivers the same high accuracy and recall rates as memory-optimized vectors at lower cost. While this option may introduce slightly higher latency, it's ideal for use cases where sub-millisecond response times aren't critical such as semantic search applications, recommendation systems, and other AI-powered search scenarios.\n  Amazon OpenSearch Serverless, our fully managed deployment option, eliminates the complexities of infrastructure management for search and analytics workloads. The service automatically scales compute capacity, measured in OpenSearch Compute Units (OCUs), based on your workload demands.\n \nPlease refer to the AWS Regional Services List for more information about Amazon OpenSearch Service availability. To learn more about OpenSearch Serverless, see the documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/opensearch-serverless-disk-optimized-vectors",
      "pubDate": "2025-09-18T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language",
        "ai-services"
      ],
      "tags": [
        "lex",
        "launch",
        "ga",
        "new-feature",
        "support"
      ]
    },
    {
      "id": "aws-news-1506e98066da",
      "title": "Amazon SageMaker HyperPod now supports autoscaling using Karpenter",
      "description": "Amazon SageMaker HyperPod now supports managed node autoscaling using Karpenter, enabling customers to automatically scale their clusters to meet dynamic inference and training demands. Real-time inference workloads require automatic scaling to address unpredictable traffic patterns and maintain service level agreements, while optimizing costs. However, organizations often struggle with the operational overhead of installing, configuring, and maintaining complex autoscaling solutions. HyperPod-managed node autoscaling eliminates the undifferentiated heavy lifting of Karpenter setup and maintenance, while providing integrated resilience and fault tolerance capabilities.\n  Autoscaling on HyperPod with Karpenter enables customers to achieve just-in-time provisioning that rapidly adapts GPU compute for inference traffic spikes. Customers can scale to zero nodes during low-demand periods without maintaining dedicated controller infrastructure and benefit from workload-aware node selection that optimizes instance types and costs. For inference workloads, this provides automatic capacity scaling to handle production traffic bursts, cost reduction through intelligent node consolidation during idle periods, and seamless integration with event-driven pod autoscalers like KEDA. Training workloads also benefit from automatic resource optimization during model development cycles. You can enable autoscaling on HyperPod using the UpdateCluster API with AutoScaling mode set to \"Enable\" and AutoScalerType set to \"Karpenter\".\n  This feature is available in all AWS Regions where Amazon SageMaker HyperPod EKS clusters are supported. To learn more about autoscaling on SageMaker HyperPod with Karpenter, see the user guide and blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/sagemaker-hyperpod-autoscaling/",
      "pubDate": "2025-09-18T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "hyperpod",
        "lex"
      ],
      "categories": [
        "machine-learning",
        "natural-language"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "lex",
        "ga",
        "update",
        "integration",
        "support"
      ]
    },
    {
      "id": "aws-news-83fa28b776e6",
      "title": "Amazon Corretto 25 is now generally available",
      "description": "Amazon Corretto 25, a Long Term Support (LTS) version, is now generally available. Amazon Corretto is a no-cost, multi-platform, production-ready distribution of OpenJDK. You can download Corretto 25 for Linux, Windows, and macOS from our downloads page.\n  Amazon Corretto 25 new features include:\n  \n \n \nTwo features that were initially released as experimental in JDK 24 are now LTS production-ready in JDK 25:\n Compact Object Headers: designed to lower heap memory usage by shrinking object headers from 96-128 bits down to 64 bits.\n Generational Shenandoah GC: engineered to provide sustainable throughput and lower p99 pause times or similar pause times with a smaller heap and reduced CPU usage.\n \n \nAhead-of-Time (AOT) Caching: designed to improve cold-start and warm-up time by reusing pre-parsed pre-linked classes and compilation profiles between training and production runs.\n \n \nLanguage improvements: primitive types in patterns, flexible constructors, module‑wide imports, compact source files, scoped values for thread-local variables, stable values for immutable data, all designed to cut boilerplate, keep everyday code shorter and safer.\n \n \nObservability: JDK Flight Recorder gains CPU‑time sampling, cooperative sampling and method‑trace events for low‑overhead production profiling.\n \n \nStructured Concurrency: designed to provide coordinated task management, allowing related tasks fail or finish together.\n \n \nVector API: developed to provide computations that compile to optimal vector instructions on supported CPUs.\n \n \nVirtual Thread pinning improvements: reduces thread pinning in synchronized blocks for better scalability.\n \n \nA detailed description of these features can be found on the OpenJDK 25 Project page. Amazon Corretto 25 is distributed by Amazon under an open source license and will be supported through October 2032.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-corretto-25-generally-available",
      "pubDate": "2025-09-17T17:18:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "lex",
        "experimental",
        "generally-available",
        "ga",
        "new-feature",
        "improvement",
        "support"
      ]
    },
    {
      "id": "aws-news-02419c1b0a2f",
      "title": "AWS Budgets now supports custom time periods",
      "description": "Today AWS announced custom time periods for AWS Budgets, a new capability that lets you create budgets with flexible start and end dates. This enhancement allows you to define budget periods that align with your organization's specific needs, moving beyond traditional calendar-based periods like monthly, quarterly, or annual budgets.\n  Custom time periods help you accurately monitor costs for projects with specific duration and funding limits. For example, if you have a three-month development project starting mid-month, you can create a single budget for that exact time frame and receive alerts when spending approaches your thresholds. This eliminates the need to calculate and split your project budget across multiple calendar months or maintain separate spreadsheets to track time-bound initiatives.\n  Custom time periods in AWS Budgets is available today in all AWS commercial Regions, except the AWS GovCloud (US) Regions and the China Regions.\n  To learn more about custom time periods in AWS Budgets, see Managing your costs with AWS Budgets in the AWS Billing User Guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-budgets-custom-time-periods/",
      "pubDate": "2025-09-17T13:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "ga",
        "enhancement",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-f789b4eea2b5",
      "title": "Amazon Lex provides generative AI based enhanced natural language understanding in eight new languages",
      "description": "Amazon Lex now allows you to leverage large language models (LLMs) to improve the natural language understanding of your deterministic conversational AI bots in eight new languages: Chinese, Japanese, Korean, Portuguese, Catalan, French, Italian, and German. With this capability, your voice- and chat-bots can better handle complex utterances, maintain accuracy despite spelling errors, and extract key information from verbose inputs to fulfill the customer’s request. For example, a customer could say ‘Hi I want to book a flight for my wife, my two kids and myself’, and the LLM will properly identify to book flight tickets for four people.\n \nThis feature is available in 10 commercial AWS Regions where Amazon Connect is available: Europe (Ireland), Europe (Frankfurt), US East (N. Virginia), Asia Pacific (Seoul), Europe (London), Asia Pacific (Tokyo), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central). To learn more about this feature, visit Amazon Lex documentation or to learn how Amazon Connect and Amazon Lex deliver cloud-based conversational AI experiences for contact centers, please visit the Amazon Connect website.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-lex-generative-ai-natural-language-eight-languages/",
      "pubDate": "2025-09-16T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "generative-ai",
        "natural-language"
      ],
      "tags": [
        "lex",
        "ga"
      ]
    },
    {
      "id": "aws-news-36861798e0d8",
      "title": "Amazon AppStream 2.0 adds support for fractional GPU instances",
      "description": "Today, Amazon AppStream 2.0 announces support for Graphics G6 instances with fractionalized GPU sizes, which are built on the EC2 G6 family, designed to cater to graphics applications that need smaller GPU fractions.\n  Graphics G6 instances with fractionalized GPU sizes (G6f and Gr6f) allow users to utilize only the GPU resources they need, rather than provisioning full GPU instances. This approach helps enable better resource optimization through shared GPU capacity, offering flexibility to choose smaller GPU fractions (such as 1/2, 1/4, or 1/8) that align with specific workload requirements. Organizations can benefit from reduced costs by avoiding over-provisioning while maintaining access to GPU capabilities for applications that don't require full GPU power.\n  These new instance types are available in 10 AWS Regions, including US East (N. Virginia, Ohio), US West (Oregon), Canada (Central), Europe (Frankfurt, London), Asia Pacific (Tokyo, Mumbai, Sydney), and South America (Sao Paulo). AppStream 2.0 offers pay-as-you-go pricing, see Amazon AppStream 2.0 Pricing for more information.\n  To get started, select an AppStream 2.0 Graphics G6 instances with fractionalized GPU sizes when launching an image builder or creating a new fleet. You can launch Graphics G6 instances with fractionalized GPU sizes using either the AWS management console or the AWS SDK. To learn more, see AppStream 2.0 Instance Families.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-appstream-fractional-gpu/",
      "pubDate": "2025-09-16T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "launch",
        "ga",
        "support"
      ]
    },
    {
      "id": "aws-news-c8de345f941d",
      "title": "Amazon EC2 R8i and R8i-flex instances are now available in additional regions",
      "description": "Starting today, Amazon Elastic Compute Cloud (Amazon EC2) R8i and R8i-flex instances are available in the Asia Pacific (Malaysia, Singapore) and Europe (Frankfurt) regions. These instances are powered by custom Intel Xeon 6 processors, available only on AWS, delivering the highest performance and fastest memory bandwidth among comparable Intel processors in the cloud. The R8i and R8i-flex instances offer up to 15% better price-performance, and 2.5x more memory bandwidth compared to previous generation Intel-based instances. They deliver 20% better performance than R7i instances, with even higher gains for specific workloads. They are up to 30% faster for PostgreSQL databases, up to 60% faster for NGINX web applications, and up to 40% faster for AI deep learning recommendation models compared to R7i.\n  R8i-flex, our first memory-optimized Flex instances, are the easiest way to get price performance benefits for a majority of memory-intensive workloads. They offer the most common sizes, from large to 16xlarge, and are a great first choice for applications that don't fully utilize all compute resources.\n  R8i instances are a great choice for all memory-intensive workloads, especially for workloads that need the largest instance sizes or continuous high CPU usage. R8i instances offer 13 sizes including 2 bare metal and the new 96xlarge size for the largest applications. R8i instances are SAP-certified and deliver 142,100 aSAPS, the highest among all comparable machines in on-premises and cloud environments, delivering exceptional performance for mission-critical SAP workloads.\n  To get started, sign in to the AWS Management Console. Customers can purchase these instances via Savings Plans, On-Demand instances, and Spot instances. For more information about the new R8i and R8i-flex instances visit the AWS News blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/amazon-ec2-r8i-r8i-flex-additional-regions/",
      "pubDate": "2025-09-16T07:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "ga",
        "now-available"
      ]
    },
    {
      "id": "aws-news-175801ccccab",
      "title": "AWS Direct Connect support for 4-byte Autonomous System numbers for Virtual interfaces",
      "description": "AWS Direct Connect now supports 4-byte Autonomous System (AS) numbers for virtual interfaces. Direct Connect uses the standard Border Gateway Protocol to provide customers with private connectivity to the AWS global network. However, customers with complex, multi-tenant network topologies or who need to maintain consistent AS numbering across their entire network can run into challenges with the maximum limit of 65,536 possible 2-byte AS numbers. With 4-byte AS numbers, customers can now use the entire range supported by RFC 6793, up to 4,294,967,294.\n Support for 4-byte AS numbers is now available in all AWS regions globally and on all Direct Connect virtual interface types. To get started, visit the AWS Direct Connect Console or use the updated APIs to create virtual interfaces with the new 4-byte AS numbers. For more information, check out the AWS Direct Connect documentation.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/09/aws-direct-connect-4-byte-autonomous-system-numbers/",
      "pubDate": "2025-09-12T17:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex",
        "ga",
        "now-available",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-cb8062515dfa",
      "title": "Accelerating local serverless development with console to IDE and remote debugging for AWS Lambda",
      "description": "Delightful developer experience is an important part of building serverless applications efficiently, whether you’re creating an automation script or developing a complex enterprise application. While AWS Lambda has transformed modern application development in the cloud with its serverless computing model, developers spend significant time working in their local environments. They rely on familiar IDEs, debugging […]",
      "link": "https://aws.amazon.com/blogs/compute/accelerating-local-serverless-development-with-console-to-ide-and-remote-debugging-for-aws-lambda/",
      "pubDate": "2025-09-09T18:00:39.000Z",
      "source": "computeBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language"
      ],
      "tags": [
        "lex"
      ]
    },
    {
      "id": "aws-news-89b810455ece",
      "title": "New general-purpose Amazon EC2 M8i and M8i-flex instances are now available",
      "description": "M8i and M8i-flex instances powered by Intel Xeon processors offer up to 15% better price performance, 20% higher performance, and 2.5 times more memory throughput compared to previous generations.",
      "link": "https://aws.amazon.com/blogs/aws/new-general-purpose-amazon-ec2-m8i-and-m8i-flex-instances-are-now-available/",
      "pubDate": "2025-08-28T19:35:40.000Z",
      "source": "newsBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "news",
        "natural-language"
      ],
      "tags": [
        "lex",
        "now-available"
      ]
    },
    {
      "id": "aws-news-7e2f23dd38ac",
      "title": "Simplify multi-tenant encryption with a cost-conscious AWS KMS key strategy",
      "description": "In this post, we explore an efficient approach to managing encryption keys in a multi-tenant SaaS environment through centralization, addressing challenges like key proliferation, rising costs, and operational complexity across multiple AWS accounts and services. We demonstrate how implementing a centralized key management strategy using a single AWS KMS key per tenant can maintain security and compliance while reducing operational overhead as organizations scale.",
      "link": "https://aws.amazon.com/blogs/architecture/simplify-multi-tenant-encryption-with-a-cost-conscious-aws-kms-key-strategy/",
      "pubDate": "2025-08-21T21:54:51.000Z",
      "source": "architectureBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "lex",
        "ga"
      ]
    },
    {
      "id": "aws-news-ca52f29c313d",
      "title": "Best performance and fastest memory with the new Amazon EC2 R8i and R8i-flex instances",
      "description": "R8i and R8i-flex instances powered by Intel Xeon processors offer up to 15% better price performance, 20% higher performance, and 2.5 times more memory throughput compared to previous generations.",
      "link": "https://aws.amazon.com/blogs/aws/best-performance-and-fastest-memory-with-the-new-amazon-ec2-r8i-and-r8i-flex-instances/",
      "pubDate": "2025-08-19T19:16:24.000Z",
      "source": "newsBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "news",
        "natural-language"
      ],
      "tags": [
        "lex"
      ]
    },
    {
      "id": "aws-news-b1018aefba54",
      "title": "Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers",
      "description": "In this post, we demonstrate how to deploy the DeepSeek-R1-Distill-Qwen-32B model using AWS DLCs for vLLMs on Amazon EKS, showcasing how these purpose-built containers simplify deployment of this powerful open source inference engine. This solution can help you solve the complex infrastructure challenges of deploying LLMs while maintaining performance and cost-efficiency.",
      "link": "https://aws.amazon.com/blogs/architecture/deploy-llms-on-amazon-eks-using-vllm-deep-learning-containers/",
      "pubDate": "2025-08-14T15:09:51.000Z",
      "source": "architectureBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "foundation-models",
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "lex"
      ]
    },
    {
      "id": "aws-news-11d98a88cbe1",
      "title": "Implement monitoring for Amazon EKS with managed services",
      "description": "In this post, we show you how to implement comprehensive monitoring for Amazon Elastic Kubernetes Service (Amazon EKS) workloads using AWS managed services. This solution demonstrates building an EKS platform that combines flexible compute options with enterprise-grade observability using AWS native services and OpenTelemetry.",
      "link": "https://aws.amazon.com/blogs/architecture/implement-monitoring-for-amazon-eks-with-managed-services/",
      "pubDate": "2025-07-18T15:47:13.000Z",
      "source": "architectureBlog",
      "services": [
        "lex"
      ],
      "categories": [
        "natural-language",
        "industry-cases"
      ],
      "tags": [
        "lex"
      ]
    }
  ]
}