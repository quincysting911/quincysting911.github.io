{
  "lastUpdated": "2025-12-03T06:16:40.775Z",
  "category": "machine-learning",
  "totalItems": 16,
  "items": [
    {
      "id": "aws-news-e19574de6969",
      "title": "Amazon SageMaker AI announces serverless MLflow capability for faster AI development",
      "description": "Amazon SageMaker AI now offers a serverless MLflow capability that dynamically scales to support AI model development tasks. With MLflow, AI developers can begin tracking, comparing, and evaluating experiments without waiting for infrastructure setup.\n  As customers across industries accelerate AI development, they require capabilities to track experiments, observe behavior, and evaluate the performance of AI models, applications and agents. However, managing MLflow infrastructure requires administrators to continuously maintain and scale tracking servers, make complex capacity planning decisions, and deploy separate instances for data isolation. This infrastructure burden diverts resources away from core AI development and creates bottlenecks that impact team productivity and cost effectiveness.\n  With this update, MLflow now scales dynamically to deliver fast performance for demanding and unpredictable model development tasks, then scales down during idle time. Administrators can also enhance productivity by setting up cross-account access via Resource Access Manager (RAM) to simplify collaboration across organizational boundaries.\n  The serverless MLflow capability on Amazon SageMaker AI is offered at no additional charge and works natively with familiar Amazon SageMaker AI model development capabilities like SageMaker AI JumpStart, SageMaker Model Registry and SageMaker Pipelines. Customers can access the latest version of MLflow on Amazon SageMaker AI with automatic version updates.\n  Amazon SageMaker AI with MLflow is now available in select AWS Regions. To learn more, see the Amazon SageMaker AI user guide and the AWS News Blog.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/12/sagemaker-ai-serverless-mlflow-ai-development/",
      "pubDate": "2025-12-02T08:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "jumpstart",
        "lex"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "jumpstart",
        "lex",
        "ga",
        "now-available",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-c803298e7651",
      "title": "AWS Clean Rooms supports synthetic dataset generation training custom ML training",
      "description": "AWS Clean Rooms now enables you and your partners to generate privacy-enhancing synthetic datasets from your collective data to train regression and classification machine learning (ML) models.\n \nSynthetic dataset generation allows you and your partners to create training datasets with similar statistical properties to the original data, without the training code having access to real records. This new capability de-identifies subjects—such as people or entities about whom data has been collected—in the original data, mitigating the risk that a model will memorize information about individuals in the training data. This unlocks new ML model training use cases that were previously restricted by privacy concerns, such as campaign optimization, fraud detection, and medical research. For example, an airline with a proprietary algorithm wants to collaborate with a hotel brand to offer joint promotions to high-value customers, but neither organization wants to share sensitive consumer data. Using AWS Clean Rooms ML, they can generate a synthetic version of their collective dataset to train the model without exposing raw data—enabling more accurate promotions targeting while protecting customer privacy.\n \nFor more information about the AWS Regions where AWS Clean Rooms ML is available, see the AWS Regions table. To learn more, visit AWS Clean Rooms ML.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/aws-clean-rooms-synthetic-dataset-generation-custom-ml",
      "pubDate": "2025-11-30T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "rds"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "rds",
        "ga",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-e81021595cbc",
      "title": "AWS Glue now supports Apache Iceberg based materialized views",
      "description": "AWS Glue now supports materialized views, a new capability that makes it easier for data teams to transform data and accelerate query performance. Materialized views are managed tables in the AWS Glue Data Catalog that store precomputed query results in Apache Iceberg format and automatically keep them up to date as source data changes. This feature is designed to make it easy for data engineers and analytics teams to transform data through multiple stages, from raw data to final analytical tables while reducing engineering effort and operational overhead.\n  Customers can now create materialized views using standard Spark SQL syntax with a data refresh schedule. The service automatically handles the refresh schedule, change detection, incremental updates, and compute infrastructure management. Spark engines across Amazon Athena, Amazon EMR, and AWS Glue intelligently rewrite queries to use these materialized views, accelerating performance by up to 8x while reducing compute costs. You can use SQL query engines like Athena and Redshift to access the materialized views as Iceberg tables from SQL editors and Amazon SageMaker notebooks.\n  Materialized views in AWS Glue are available in Europe (Stockholm), Asia Pacific (Thailand), Asia Pacific (Mumbai), Europe (Paris), US East (Ohio),Europe (Ireland), Europe (Frankfurt), South America (Sao Paulo), Asia Pacific (Hong Kong), US East (N. Virginia), Asia Pacific (Seoul), Asia Pacific (Malaysia), Europe (London), Asia Pacific (Tokyo), US West (Oregon), US West (N. California), Asia Pacific (Singapore), Asia Pacific (Sydney), Canada (Central), and Europe (Spain). To learn more, visit Working with Materialized Views in the AWS Glue developer guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/aws-glue-apache-iceberg-based-materialized-views",
      "pubDate": "2025-11-30T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "emr",
        "redshift",
        "glue",
        "athena"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "emr",
        "redshift",
        "glue",
        "athena",
        "ga",
        "update",
        "support",
        "new-capability"
      ]
    },
    {
      "id": "aws-news-70a75d3e1cfa",
      "title": "Medidata’s journey to a modern lakehouse architecture on AWS",
      "description": "In this post, we show you how Medidata created a unified, scalable, real-time data platform that serves thousands of clinical trials worldwide with AWS services, Apache Iceberg, and a modern lakehouse architecture.",
      "link": "https://aws.amazon.com/blogs/big-data/medidatas-journey-to-a-modern-lakehouse-architecture-on-aws/",
      "pubDate": "2025-11-27T01:00:46.000Z",
      "source": "bigDataBlog",
      "services": [],
      "categories": [
        "machine-learning"
      ],
      "tags": []
    },
    {
      "id": "aws-news-e771ae5d8453",
      "title": "Managed Tiered KV Cache and Intelligent Routing for Amazon SageMaker HyperPod",
      "description": "In this post, we introduce Managed Tiered KV Cache and Intelligent Routing for Amazon SageMaker HyperPod, new capabilities that can reduce time to first token by up to 40% and lower compute costs by up to 25% for long context prompts and multi-turn conversations. These features automatically manage distributed KV caching infrastructure and intelligent request routing, making it easier to deploy production-scale LLM inference workloads with enterprise-grade performance while significantly reducing operational overhead.",
      "link": "https://aws.amazon.com/blogs/machine-learning/managed-tiered-kv-cache-and-intelligent-routing-for-amazon-sagemaker-hyperpod/",
      "pubDate": "2025-11-27T00:50:04.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker",
        "hyperpod"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "hyperpod"
      ]
    },
    {
      "id": "aws-news-3bab677a86c4",
      "title": "Amazon SageMaker HyperPod now supports custom Kubernetes labels and taints",
      "description": "Amazon SageMaker HyperPod now supports custom Kubernetes labels and taints, enabling customers to control pod scheduling and integrate seamlessly with existing Kubernetes infrastructure. Customers deploying AI workloads on HyperPod clusters orcehstrated with EKS need precise control over workload placement to prevent expensive GPU resources from being consumed by system pods and non-AI workloads, while ensuring compatibility with custom device plugins such as EFA and NVIDIA GPU operators. Previously, customers had to manually apply labels and taints using kubectl and reapply them after every node replacement, scaling, or patching operation, creating significant operational overhead.\n  This capability allows you to configure labels and taints at the instance group level through the CreateCluster and UpdateCluster APIs, providing a managed approach to defining and maintaining scheduling policies across the entire node lifecycle. Using the new KubernetesConfig parameter, you can specify up to 50 labels and 50 taints per instance group. Labels enable resource organization and pod targeting through node selectors, while taints repel pods without matching tolerations to protect specialized nodes. For example, you can apply NoSchedule taints to GPU instance groups to ensure only AI training jobs with explicit tolerations consume high-cost compute resources, or add custom labels that enable device plugin pods to schedule correctly. HyperPod automatically applies these configurations during node creation and maintains them across replacement, scaling, and patching operations, eliminating manual intervention and reducing operational overhead.\n  This feature is available in all AWS Regions where Amazon SageMaker HyperPod is available. To learn more about custom labels and taints, see the user guide.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/amazon-sagemaker-hyperpod-kubernetes/",
      "pubDate": "2025-11-26T18:45:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "hyperpod",
        "eks"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "eks",
        "ga",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-72a635483fb7",
      "title": "AWS announces support for Apache Iceberg V3 deletion vectors and row lineage",
      "description": "AWS now supports deletion vectors and row lineage as defined in the Apache Iceberg Version 3 (V3) specification. These new features are available with Apache Spark on Amazon EMR 7.12, AWS Glue, Amazon SageMaker notebooks, Amazon S3 Tables, and the AWS Glue Data Catalog.\n  These Iceberg V3 capabilities help customers build petabyte-scale data lakes with improved performance for data modifications and functionality to easily track changed records. Deletion vectors write optimized delete files that speed up data pipelines and reduce data compaction costs. Row lineage provides metadata fields on each record to track changes with a simple SQL query, eliminating the computational expense of finding small changes in large tables.\n  Get started creating V3 tables by setting the table property to 'format-version = 3' in the CREATE TABLE command in Spark or a SageMaker notebook. To upgrade existing tables, simply update the table property in metadata with the new format version. When you do this, AWS query engines that support V3 will automatically begin to use deletion vectors and row lineage.\n  Iceberg V3 deletion vectors and row lineage are now available in all AWS Regions where each respective service/feature—Amazon EMR, AWS Glue, SageMaker notebooks, S3 Tables, and AWS Glue Data Catalog—is supported. To learn more about AWS support for Iceberg V3, visit Apache Iceberg V3 on AWS, and read the blog post.",
      "link": "https://aws.amazon.com/about-aws/whats-new/2025/11/aws-apache-iceberg-v3-deletion-vectors-row-lineage",
      "pubDate": "2025-11-26T15:00:00.000Z",
      "source": "whatsNew",
      "services": [
        "sagemaker",
        "s3",
        "emr",
        "rds",
        "glue"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "s3",
        "emr",
        "rds",
        "glue",
        "now-available",
        "new-feature",
        "update",
        "support"
      ]
    },
    {
      "id": "aws-news-a7bab8231d01",
      "title": "Orchestrating data processing tasks with a serverless visual workflow in Amazon SageMaker Unified Studio",
      "description": "In this post, we show how to use the new visual workflow experience in SageMaker Unified Studio IAM-based domains to orchestrate an end-to-end machine learning workflow. The workflow ingests weather data, applies transformations, and generates predictions—all through a single, intuitive interface, without writing any orchestration code.",
      "link": "https://aws.amazon.com/blogs/big-data/orchestrating-data-processing-tasks-with-a-serverless-visual-workflow-in-amazon-sagemaker-unified-studio/",
      "pubDate": "2025-11-25T23:08:05.000Z",
      "source": "bigDataBlog",
      "services": [
        "sagemaker",
        "unified studio",
        "iam"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "unified studio",
        "iam"
      ]
    },
    {
      "id": "aws-news-ba4b2b5742c2",
      "title": "Train custom computer vision defect detection model using Amazon SageMaker",
      "description": "In this post, we demonstrate how to migrate computer vision workloads from Amazon Lookout for Vision to Amazon SageMaker AI by training custom defect detection models using pre-trained models available on AWS Marketplace. We provide step-by-step guidance on labeling datasets with SageMaker Ground Truth, training models with flexible hyperparameter configurations, and deploying them for real-time or batch inference—giving you greater control and flexibility for automated quality inspection use cases.",
      "link": "https://aws.amazon.com/blogs/machine-learning/train-custom-computer-vision-defect-detection-model-using-amazon-sagemaker/",
      "pubDate": "2025-11-25T22:44:22.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker",
        "lookout for vision",
        "lex"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "lookout for vision",
        "lex"
      ]
    },
    {
      "id": "aws-news-e78bda25ea6d",
      "title": "Introducing bidirectional streaming for real-time inference on Amazon SageMaker AI",
      "description": "We're introducing bidirectional streaming for Amazon SageMaker AI Inference, which transforms inference from a transactional exchange into a continuous conversation. This post shows you how to build and deploy a container with bidirectional streaming capability to a SageMaker AI endpoint. We also demonstrate how you can bring your own container or use our partner Deepgram's pre-built models and containers on SageMaker AI to enable bi-directional streaming feature for real-time inference.",
      "link": "https://aws.amazon.com/blogs/machine-learning/introducing-bidirectional-streaming-for-real-time-inference-on-amazon-sagemaker-ai/",
      "pubDate": "2025-11-25T19:09:59.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker"
      ]
    },
    {
      "id": "aws-news-309e6475f8de",
      "title": "Warner Bros. Discovery achieves 60% cost savings and faster ML inference with AWS Graviton",
      "description": "Warner Bros. Discovery (WBD) is a leading global media and entertainment company that creates and distributes the world’s most differentiated and complete portfolio of content and brands across television, film and streaming. In this post, we describe the scale of our offerings, artificial intelligence (AI)/machine learning (ML) inference infrastructure requirements for our real time recommender systems, and how we used AWS Graviton-based Amazon SageMaker AI instances for our ML inference workloads and achieved 60% cost savings and 7% to 60% latency improvements across different models.",
      "link": "https://aws.amazon.com/blogs/machine-learning/warner-bros-discovery-achieves-60-cost-savings-and-faster-ml-inference-with-aws-graviton/",
      "pubDate": "2025-11-25T17:26:48.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker",
        "graviton"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "graviton",
        "improvement"
      ]
    },
    {
      "id": "aws-news-7612920ee216",
      "title": "Physical AI in practice: Technical foundations that fuel human-machine interactions",
      "description": "In this post, we explore the complete development lifecycle of physical AI—from data collection and model training to edge deployment—and examine how these intelligent systems learn to understand, reason, and interact with the physical world through continuous feedback loops. We illustrate this workflow through Diligent Robotics' Moxi, a mobile manipulation robot that has completed over 1.2 million deliveries in hospitals, saving nearly 600,000 hours for clinical staff while transforming healthcare logistics and returning valuable time to patient care.",
      "link": "https://aws.amazon.com/blogs/machine-learning/physical-ai-in-practice-technical-foundations-that-fuel-human-machine-interactions/",
      "pubDate": "2025-11-25T17:00:25.000Z",
      "source": "mlBlog",
      "services": [],
      "categories": [
        "machine-learning"
      ],
      "tags": []
    },
    {
      "id": "aws-news-333d00155bd8",
      "title": "Power up your ML workflows with interactive IDEs on SageMaker HyperPod",
      "description": "Amazon SageMaker HyperPod clusters with Amazon Elastic Kubernetes Service (EKS) orchestration now support creating and managing interactive development environments such as JupyterLab and open source Visual Studio Code, streamlining the ML development lifecycle by providing managed environments for familiar tools to data scientists. This post shows how HyperPod administrators can configure Spaces for their clusters, and how data scientists can create and connect to these Spaces.",
      "link": "https://aws.amazon.com/blogs/machine-learning/power-up-your-ml-workflows-with-interactive-ides-on-sagemaker-hyperpod/",
      "pubDate": "2025-11-24T21:25:56.000Z",
      "source": "mlBlog",
      "services": [
        "sagemaker",
        "hyperpod",
        "eks"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "hyperpod",
        "eks",
        "support"
      ]
    },
    {
      "id": "aws-news-c2a79337519c",
      "title": "Enforce business glossary classification rules in Amazon SageMaker Catalog",
      "description": "Amazon SageMaker Catalog now supports metadata enforcement rules for glossary terms classification (tagging) at the asset level. With this capability, administrators can require that assets include specific business terms or classifications. Data producers must apply required glossary terms or classifications before an asset can be published. In this post, we show how to enforce business glossary classification rules in SageMaker Catalog.",
      "link": "https://aws.amazon.com/blogs/big-data/enforce-business-glossary-classification-rules-in-amazon-sagemaker-catalog/",
      "pubDate": "2025-11-20T18:39:41.000Z",
      "source": "bigDataBlog",
      "services": [
        "sagemaker"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "support"
      ]
    },
    {
      "id": "aws-news-e3fa9789b76a",
      "title": "Getting started with Amazon S3 Tables in Amazon SageMaker Unified Studio",
      "description": "In this post, you learn how to integrate SageMaker Unified Studio with S3 Tables and query your data using Amazon Athena, Amazon Redshift, or Apache Spark in EMR and AWS Glue.",
      "link": "https://aws.amazon.com/blogs/big-data/getting-started-with-amazon-s3-tables-in-amazon-sagemaker-unified-studio/",
      "pubDate": "2025-11-19T23:26:57.000Z",
      "source": "bigDataBlog",
      "services": [
        "sagemaker",
        "unified studio",
        "s3",
        "emr",
        "redshift",
        "glue",
        "athena"
      ],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "sagemaker",
        "unified studio",
        "s3",
        "emr",
        "redshift",
        "glue",
        "athena"
      ]
    },
    {
      "id": "aws-news-d4d5eb413522",
      "title": "How Yelp modernized its data infrastructure with a streaming lakehouse on AWS",
      "description": "This is a guest post by Umesh Dangat, Senior Principal Engineer for Distributed Services and Systems at Yelp, and Toby Cole, Principle Engineer for Data Processing at Yelp, in partnership with AWS. Yelp processes massive amounts of user data daily—over 300 million business reviews, 100,000 photo uploads, and countless check-ins. Maintaining sub-minute data freshness with […]",
      "link": "https://aws.amazon.com/blogs/big-data/how-yelp-modernized-its-data-infrastructure-with-a-streaming-lakehouse-on-aws/",
      "pubDate": "2025-11-13T18:07:22.000Z",
      "source": "bigDataBlog",
      "services": [],
      "categories": [
        "machine-learning"
      ],
      "tags": [
        "ga"
      ]
    }
  ]
}